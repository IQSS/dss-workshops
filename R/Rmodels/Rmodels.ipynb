{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": true,
    "results": false,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "options(max.print = 100)\n",
    "knitr::opts_chunk$set(eval=TRUE, results=TRUE, message=FALSE, warning=FALSE, error=FALSE, fig.path=\"R/Rmodels/figures/\")\n",
    "knitr::opts_knit$set(root.dir=\"R/Rmodels\") # base.dir=\"R/Rmodels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Regression Models\n",
    "\n",
    "**Topics**\n",
    "\n",
    "* Formula interface for model specification\n",
    "* Function methods for extracting quantities of interest from models\n",
    "* Contrasts to test specific hypotheses\n",
    "* Model comparisons\n",
    "* Predicted marginal effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Class Structure\n",
    "\n",
    "* Informal --- Ask questions at any time. Really!\n",
    "* Collaboration is encouraged - please spend a minute introducing yourself to your neighbors!\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This is an intermediate R course:\n",
    "\n",
    "* Assumes working knowledge of R\n",
    "* Relatively fast-paced\n",
    "* This is not a statistics course! We will teach you *how* to fit models in R,\n",
    "  but we assume you know the theory behind the models.\n",
    "\n",
    "### Goals\n",
    "\n",
    "We will learn about the R modeling ecosystem by fitting a variety of statistical models to\n",
    "different datasets. In particular, our goals are to learn about:\n",
    "\n",
    "1. Modeling workflow\n",
    "2. Visualizing and summarizing data before modeling\n",
    "3. Modeling continuous outcomes\n",
    "4. Modeling binary outcomes\n",
    "5. Modeling clustered data\n",
    "\n",
    "We will not spend much time *interpreting* the models we fit, since this is not a statistics workshop.\n",
    "But, we will walk you through how model results are organized and orientate you to where you can find\n",
    "typical quantities of interest.\n",
    "\n",
    "### Launch an R session\n",
    "\n",
    "Start RStudio and create a new project:\n",
    "\n",
    "* On Windows click the start button and search for RStudio. On Mac\n",
    "    RStudio will be in your applications folder.\n",
    "* In Rstudio go to `File -> New Project`.\n",
    "* Choose `Existing Directory` and browse to the workshop materials directory on your desktop.\n",
    "* Choose `File -> Open File` and select the file with the word \"BLANK\" in the name.\n",
    "\n",
    "### Packages\n",
    "\n",
    "You should have already installed the `tidyverse` and `rmarkdown`\n",
    "packages onto your computer before the workshop\n",
    "--- see [R Installation](./Rinstall.html).\n",
    "Now let's load these packages into the search path of our R session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(rmarkdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets install some packages that will help with modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"lme4\")\n",
    "library(lme4)  # for mixed models\n",
    "\n",
    "# install.packages(\"emmeans\")\n",
    "library(emmeans)  # for marginal effects\n",
    "\n",
    "# install.packages(\"effects\")\n",
    "library(effects)  # for predicted marginal means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling workflow\n",
    "\n",
    "Before we delve into the details of how to fit models in R, it's worth taking a step\n",
    "back and thinking more broadly about the components of the modeling process. These\n",
    "can roughly be divided into 3 stages:\n",
    "\n",
    "1. Pre-estimation\n",
    "2. Estimation\n",
    "3. Post-estimaton\n",
    "\n",
    "At each stage, the goal is to complete a different task (e.g., to clean data, fit a model, test a hypothesis),\n",
    "but the process is sequential --- we move through the stages in order (though often many times in one project!)\n",
    "\n",
    "![](R/Rmodels/images/R_model_pipeline.png)\n",
    "\n",
    "Throughout this workshop we will go through these stages several times as we fit different types of model.\n",
    "\n",
    "## R modeling ecosystem\n",
    "\n",
    "There are literally hundreds of R packages that provide model fitting functionality.\n",
    "We're going to focus on just two during this workshop --- `stats`, from Base R, and\n",
    "`lme4`. It's a good idea to look at [CRAN Task Views](https://cran.r-project.org/web/views/) \n",
    "when trying to find a modeling package for your needs, as they provide an extensive \n",
    "curated list. But, here's a more digestable table showing some of the most popular\n",
    " packages for particular types of model.\n",
    "\n",
    "| Models              | Packages                               |             \n",
    "|:--------------------|:---------------------------------------|\n",
    "| Generalized linear  | `stats`, `biglm`, `MASS`, `robustbase` | \n",
    "| Mixed effects       | `lme4`, `nlme`, `glmmTMB`, `MASS`      |                                     \n",
    "| Econometric         | `pglm`, `VGAM`, `pscl`, `survival`     | \n",
    "| Bayesian            | `brms`, `blme`, `MCMCglmm`, `rstan`    | \n",
    "| Machine learning    | `mlr`, `caret`, `h2o`, `tensorflow`    | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before fitting a model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**GOAL: To learn about the data by creating summaries and visualizations.**\n",
    "</div>\n",
    "\n",
    "One important part of the pre-estimation stage of model fitting, is gaining an understanding\n",
    "of the data we wish to model by creating plots and summaries. Let's do this now.\n",
    "\n",
    "### Load the data\n",
    "\n",
    "List the data files we're going to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list.files(\"dataSets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the `states` data first, which originally appeared in *Statistics with Stata* by Lawrence C. Hamilton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # read the states data\n",
    "  states_data <- read_rds(\"dataSets/states.rds\")\n",
    "\n",
    "  # look at the last few rows\n",
    "  tail(states_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Description                                        |\n",
    "|:---------|:---------------------------------------------------|\n",
    "| csat     | Mean composite SAT score                           |\n",
    "| expense  | Per pupil expenditures                             |\n",
    "| percent  | % HS graduates taking SAT                          |\n",
    "| income   | Median household income, $1,000                    |\n",
    "| region   | Geographic region: West, N. East, South, Midwest   |\n",
    "| house    | House '91 environ. voting, %                       |\n",
    "| senate   | Senate '91 environ. voting, %                      |\n",
    "| energy   | Per capita energy consumed, Btu                    |\n",
    "| metro    | Metropolitan area population, %                    |\n",
    "| waste    | Per capita solid waste, tons                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the data\n",
    "\n",
    "Start by examining the data to check for problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # summary of expense and csat columns, all rows\n",
    "  sts_ex_sat <- \n",
    "      states_data %>% \n",
    "      select(expense, csat)\n",
    "  \n",
    "  summary(sts_ex_sat)\n",
    "\n",
    "  # correlation between expense and csat\n",
    "  cor(sts_ex_sat, use = \"pairwise\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "Plot the data to look for multivariate outliers, non-linear relationships etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # scatter plot of expense vs csat\n",
    "  plot(sts_ex_sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, in a real project, you would want to spend more time investigating the data,\n",
    "but we'll now move on to modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with continuous outcomes\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**GOAL: To learn about the R modeling ecosystem by fitting ordinary least squares (OLS) models.** In particular:\n",
    "\n",
    "1. Formula representation of a model specification\n",
    "2. Model classes\n",
    "3. Function methods\n",
    "4. Model comparison\n",
    "</div>\n",
    "\n",
    "Once the data have been inspected and cleaned, we can start estimating models.\n",
    "The simplest models (but those with the most assumptions) are those for continuous and unbounded outcomes.\n",
    "Typically, for these outcomes, we'd use a model estimated using Ordinary Least Lquares (OLS),\n",
    "which in R can be fit with the `lm()` (linear model) function.\n",
    "\n",
    "To fit a model in R, we first have to convert our theoretical model into\n",
    "a `formula` --- a symbolic representation of the model in R syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "# formula for model specification\n",
    "outcome ~ pred1 + pred2 + pred3\n",
    "\n",
    "# NOTE the ~ is a tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following model predicts SAT scores based on per-pupil expenditures:\n",
    "\n",
    "<div class=\"alert alert-secondary\">\n",
    "$$\n",
    "csat_i = \\beta_01 + \\beta_1expense_i + \\epsilon_i\n",
    "$$\n",
    "</div>\n",
    "\n",
    "We can use `lm()` to fit this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Fit our regression model\n",
    "  sat_mod <- lm(csat ~ 1 + expense, # regression formula\n",
    "                data = states_data) # data \n",
    "                \n",
    "  # Summarize and print the results\n",
    "  summary(sat_mod) %>% coef() # show regression coefficients table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the association between expense & SAT scores *negative*?\n",
    "\n",
    "Many people find it surprising that the per-capita expenditure on students is negatively related to SAT scores. The beauty of multiple regression is that we can try to pull these apart. What would the association between expense and SAT scores be if there were no difference among the states in the percentage of students taking the SAT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  lm(csat ~ 1 + expense + percent, data = states_data) %>% \n",
    "  summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `lm` class & methods\n",
    "\n",
    "Okay, we fitted our model. Now what? Typically, the main goal in the **post-estimation stage** of analysis\n",
    "is to extract **quantities of interest** from our fitted model. These quantities could be things like:\n",
    "\n",
    "1. Testing whether one group is different on average from another group\n",
    "2. Generating average response values from the model for interesting combinations of predictor values\n",
    "3. Calculating interval estimates for particular coefficients\n",
    "\n",
    "But before we can do any of that, we need to know more about **what a fitted model actually is,**\n",
    "**what information it contains, and how we can extract from it information that we want to report**.\n",
    "\n",
    "Let's start by examining the model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  class(sat_mod)\n",
    "  str(sat_mod)\n",
    "  names(sat_mod)\n",
    "  methods(class = class(sat_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `function methods` to get more information about the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  summary(sat_mod)\n",
    "  summary(sat_mod) %>% coef()\n",
    "  methods(\"summary\")\n",
    "  confint(sat_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does R know which method to call for a given object? \n",
    "R uses `generic functions`, which provide access to `methods`. \n",
    "Method dispatch takes place based on the `class` of the\n",
    "first argument to the generic function. For example, for the generic \n",
    "function `summary()` and an object of class `lm`, the method \n",
    "dispatched will be `summary.lm()`. Function methods always take \n",
    "the form `generic.method()`:\n",
    "\n",
    "![](R/Rmodels/images/methods.png)\n",
    "\n",
    "It's always worth examining what function methods are available for the class of model you're fitting.\n",
    "Here's a summary table of some of the most often used methods. These are post-estimation tools you\n",
    "will want in your toolbox:\n",
    "\n",
    "| Function      | Package        | Output                                                  |\n",
    "|:--------------|:---------------|:--------------------------------------------------------|\n",
    "| `summary()`   | `stats` base R | standard errors, test statistics, p-values, GOF stats   |\n",
    "| `confint()`   | `stats` base R | confidence intervals                                    |\n",
    "| `anova()`     | `stats` base R | anova table (one model), model comparison (> one model) |\n",
    "| `coef()`      | `stats` base R | point estimates                                         |\n",
    "| `drop1()`     | `stats` base R | model comparison                                        |\n",
    "| `predict()`   | `stats` base R | predicted response values                               |\n",
    "| `fitted()`    | `stats` base R | predicted response values (for observed data)           |\n",
    "| `residuals()` | `stats` base R | residuals                                               |\n",
    "| `fixef()`     | `lme4`         | fixed effect point estimates (mixed models only)        |\n",
    "| `ranef()`     | `lme4`         | random effect point estimates (mixed models only)       |\n",
    "| `coef()`      | `lme4`         | empirical Bayes estimates (mixed models only)           |\n",
    "| `allEffects()`| `effects`      | predicted marginal means                                |\n",
    "| `emmeans()`   | `emmeans`      | predicted marginal means & marginal effects             |\n",
    "| `margins()`   | `margins`      | predicted marginal means & marginal effects             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS regression assumptions\n",
    "\n",
    "OLS regression relies on several assumptions, including:\n",
    "\n",
    "1. The model includes all relevant variables (i.e., no omitted variable bias).\n",
    "2. The model is linear in the parameters (i.e., the coefficients and error term).\n",
    "3. The error term has an expected value of zero.\n",
    "4. All right-hand-side variables are uncorrelated with the error term.\n",
    "5. No right-hand-side variables are a perfect linear function of other RHS variables.\n",
    "6. Observations of the error term are uncorrelated with each other.\n",
    "7. The error term has constant variance (i.e., homoscedasticity).\n",
    "8. (Optional - only needed for inference). The error term is normally distributed.\n",
    "\n",
    "Investigate assumptions #7 and #8 visually by plotting your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  par(mfrow = c(2, 2)) # splits the plotting window into 4 panels\n",
    "  plot(sat_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models\n",
    "\n",
    "Do congressional voting patterns predict SAT scores over and above expense? Fit two models and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # fit another model, adding house and senate as predictors\n",
    "  sat_voting_mod <- lm(csat ~ 1 + expense + house + senate,\n",
    "                        data = na.omit(states_data))\n",
    "\n",
    "  summary(sat_voting_mod) %>% coef()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we using `na.omit()`? Let's see what `na.omit()` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data\n",
    "dat <- data.frame(\n",
    "  x = 1:5,\n",
    "  y = c(3, 2, 1, NA, 5),\n",
    "  z = c(6, NA, 2, 7, 3))\n",
    "dat\n",
    "\n",
    "na.omit(dat) # listwise deletion of observations\n",
    "\n",
    "# also see\n",
    "# ?complete.cases\n",
    "dat[with(dat, complete.cases(x, y, z)), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare models, we must fit them to the same data. This is why we need `na.omit()`.\n",
    "Now let's update our first model using `na.omit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  sat_mod <- update(sat_mod, data = na.omit(states_data))\n",
    "\n",
    "  # compare using an F-test with the anova() function\n",
    "  anova(sat_mod, sat_voting_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Exercise 0\n",
    "\n",
    "**Ordinary least squares regression**\n",
    "\n",
    "Use the *states.rds* data set. Fit a model predicting energy consumed per capita (energy) from the percentage of residents living in metropolitan areas (`metro`). Be sure to\n",
    "\n",
    "1.  Examine/plot the data before fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "2.  Print and interpret the model `summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "3.  `plot()` the model to look for deviations from modeling assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "4. Select one or more additional predictors to add to your model and repeat steps 1-3. Is this model significantly better than the model with `metro` as the only predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><span style=\"color:red\"><b>Click for Exercise 0 Solution</b></span></summary>\n",
    "  <div class=\"alert alert-success\">\n",
    "\n",
    "Use the *states.rds* data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  states <- read_rds(\"dataSets/states.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a model predicting energy consumed per capita (energy) from the percentage of residents living in metropolitan areas (metro). Be sure to:\n",
    "\n",
    "1.  Examine/plot the data before fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  states_en_met <- subset(states, select = c(\"metro\", \"energy\"))\n",
    "  summary(states_en_met)\n",
    "  plot(states_en_met)\n",
    "  cor(states_en_met, use = \"pairwise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Print and interpret the model `summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  mod_en_met <- lm(energy ~ metro, data = states)\n",
    "  summary(mod_en_met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  `plot()` the model to look for deviations from modeling assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  par(mfrow = c(2, 2)) # splits the plotting window into 4 panels\n",
    "  plot(mod_en_met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Select one or more additional predictors to add to your model and repeat steps 1-3. Is this model significantly better than the model with *metro* as the only predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "  states_en_met_pop_wst <- subset(states, select = c(\"energy\", \"metro\", \"pop\", \"waste\"))\n",
    "  summary(states_en_met_pop_wst)\n",
    "  plot(states_en_met_pop_wst)\n",
    "  cor(states_en_met_pop_wst, use = \"pairwise\")\n",
    "\n",
    "  mod_en_met_pop_waste <- lm(energy ~ 1 + metro + pop + waste, data = states)\n",
    "  summary(mod_en_met_pop_waste)\n",
    "  anova(mod_en_met, mod_en_met_pop_waste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions & factors\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**GOAL: To learn how to specify interaction effects and fit models with categorical predictors.** In particular:\n",
    "\n",
    "1. Formula syntax for interaction effects\n",
    "2. Factor levels and labels\n",
    "3. Contrasts and pairwise comparisons\n",
    "</div>\n",
    "\n",
    "### Modeling interactions\n",
    "\n",
    "Interactions allow us assess the extent to which the association between one predictor and the outcome depends on a second predictor. For example: Does the association between expense and SAT scores depend on the median income in the state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Add the interaction to the model\n",
    "  sat_expense_by_percent <- lm(csat ~ 1 + expense + income + expense : income, data = states_data)\n",
    "\n",
    "  # same as above, but shorter syntax\n",
    "  sat_expense_by_percent <- lm(csat ~ 1 + expense * income, data = states_data) \n",
    "\n",
    "  # Show the regression coefficients table\n",
    "  summary(sat_expense_by_percent) %>% coef() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with categorical predictors\n",
    "\n",
    "Let's try to predict SAT scores from region, a categorical variable. \n",
    "Note that you must make sure R does not think your categorical variable is numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # make sure R knows region is categorical\n",
    "  str(states_data$region)\n",
    "  states_data$region <- factor(states_data$region)\n",
    "\n",
    "  # arguments to the factor() function\n",
    "  # factor(x, levels, labels)\n",
    "\n",
    "  levels(states_data$region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Add region to the model\n",
    "  sat_region <- lm(csat ~ 1 + region, data = states_data) \n",
    "\n",
    "  # Show the results\n",
    "  summary(sat_region) %>% coef() # show the regression coefficients table\n",
    "  anova(sat_region) # show ANOVA table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, make sure to tell R which variables are categorical by converting them to factors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting factor reference groups & contrasts\n",
    "\n",
    "**Contrasts** is the umbrella term used to describe the process of testing linear combinations of parameters from regression models.\n",
    "All statistical sofware use contrasts, but each sofware has different defaults and their own way of overriding these.\n",
    "\n",
    "The default contrasts in R are \"treatment\" contrasts (aka \"dummy coding\"), where each level within a factor\n",
    "is identified within a matrix of binary `0` / `1` variables, with the first level chosen as the reference category.\n",
    "They're called \"treatment\" contrasts, because of the typical use case where there is one control group (the reference group)\n",
    "and one or more treatment groups that are to be compared to the controls. It is easy to change the default contrasts to something\n",
    "other than treatment contrasts, though this is rarely needed. More often, we may want to change the reference group in\n",
    "treatment contrasts or get all sets of pairwise contrasts between factor levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "  # change the reference group\n",
    "  states_data$region <- relevel(states_data$region, ref = \"Midwest\")\n",
    "  m1 <- lm(csat ~ 1 + region, data = states_data)\n",
    "  summary(m1) %>% coef()\n",
    "\n",
    "  # get all pairwise contrasts between means\n",
    "  means <- emmeans(m1, specs = ~ region)\n",
    "  means\n",
    "  contrast(means, method = \"pairwise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "**Interactions & factors**\n",
    "\n",
    "Use the `states` data set.\n",
    "\n",
    "1.  Add on to the regression equation that you created in Exercise 1 by generating an interaction term and testing the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "2.  Try adding region to the model. Are there significant differences across the four regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><span style=\"color:red\"><b>Click for Exercise 1 Solution</b></span></summary>\n",
    "  <div class=\"alert alert-success\">\n",
    "\n",
    "Use the states data set.\n",
    "\n",
    "1.  Add on to the regression equation that you created in exercise 1 by generating an interaction term and testing the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  mod_en_metro_by_waste <- lm(energy ~ 1 + metro * waste, data = states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Try adding a region to the model. Are there significant differences across the four regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "  mod_en_region <- lm(energy ~ 1 + metro * waste + region, data = states)\n",
    "  anova(mod_en_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with binary outcomes\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**GOAL: To learn how to use the `glm()` function to model binary outcomes.** In particular:\n",
    "\n",
    "1. The `family` and `link` components of the `glm()` function call\n",
    "2. Transforming model coefficients into odds ratios\n",
    "3. Transforming model coefficients into predicted marginal means\n",
    "</div>\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "This far we have used the `lm()` function to fit our regression models. `lm()` is great, but limited --- in particular it only fits models for continuous dependent variables. For categorical dependent variables we can use the `glm()` function.\n",
    "\n",
    "For these models we will use a different dataset, drawn from the National Health Interview Survey. From the [CDC website](http://www.cdc.gov/nchs/nhis.htm):\n",
    "\n",
    "> The National Health Interview Survey (NHIS) has monitored the health of the nation since 1957. NHIS data on a broad range of health topics are collected through personal household interviews. For over 50 years, the U.S. Census Bureau has been the data collection agent for the National Health Interview Survey. Survey results have been instrumental in providing data to track health status, health care access, and progress toward achieving national health objectives.\n",
    "\n",
    "Load the National Health Interview Survey data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  NH11 <- read_rds(\"dataSets/NatHealth2011.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression example\n",
    "\n",
    "Motivation for a logistic regression model --- with a binary response:\n",
    "\n",
    "1. Errors will not be normally distributed\n",
    "2. Variance will not be homoskedastic\n",
    "3. Predictions should be constrained to be on the interval [0, 1]\n",
    "\n",
    "![](R/Rmodels/images/logistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anatomy of a generalized linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "  # OLS model using lm()\n",
    "  lm(outcome ~ 1 + pred1 + pred2, \n",
    "     data = mydata)\n",
    "\n",
    "  # OLS model using glm()\n",
    "  glm(outcome ~ 1 + pred1 + pred2, \n",
    "      data = mydata, \n",
    "      family = gaussian(link = \"identity\"))\n",
    " \n",
    "  # logistic model using glm()\n",
    "  glm(outcome ~ 1 + pred1 + pred2, \n",
    "      data = mydata, \n",
    "      family = binomial(link = \"logit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `family` argument sets the error distribution for the model, while the `link` function\n",
    "argument relates the predictors to the expected value of the outcome.\n",
    "\n",
    "Let's predict the probability of being diagnosed with hypertension based on `age`, `sex`, `sleep`, and `bmi`.\n",
    "Here's the model:\n",
    "\n",
    "<div class=\"alert alert-secondary\">\n",
    "$$\n",
    "logit(hypev_i) = \\beta_01 + \\beta_1agep_i + \\beta_2sex_i + \\beta_3sleep_i + \\beta_4bmi_i + \\epsilon_i \n",
    "$$\n",
    "</div>\n",
    "\n",
    "where $logit(\\cdot)$ is the link function, which is equivalent to the log odds of `hypev`:\n",
    "\n",
    "<div class=\"alert alert-secondary\">\n",
    "$$\n",
    "logit(hypev_i) = ln \\frac{p(hypev_i = 1)}{p(hypev_i = 0)}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "And here's how we fit this in R. First, let's clean up the hypertension outcome by making it binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  str(NH11$hypev) # check stucture of hypev\n",
    "  levels(NH11$hypev) # check levels of hypev\n",
    "\n",
    "  # collapse all missing values to NA\n",
    "  NH11$hypev <- factor(NH11$hypev, levels=c(\"2 No\", \"1 Yes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `glm()` to estimate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # run our regression model\n",
    "  hyp_out <- glm(hypev ~ 1 + age_p + sex + sleep + bmi,\n",
    "                 data = NH11, \n",
    "                 family = binomial(link = \"logit\"))\n",
    "\n",
    "  summary(hyp_out) %>% coef()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds ratios\n",
    "\n",
    "Generalized linear models use link functions to relate the average value of the response to the predictors,\n",
    "so raw coefficients are difficult to interpret. For example, the `age` coefficient of .06 in the previous\n",
    "model tells us that for every one unit increase in `age`, the log odds of hypertension diagnosis increases\n",
    "by 0.06. Since most of us are not used to thinking in log odds this is not too helpful!\n",
    "\n",
    "One solution is to transform the coefficients to make them easier to interpret.\n",
    "Here we transform them into odds ratios by exponentiating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # point estimates\n",
    "  coef(hyp_out) %>% exp()\n",
    "  \n",
    "  # confidence intervals\n",
    "  confint(hyp_out) %>% exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted marginal means\n",
    "\n",
    "Instead of reporting odds ratios, we may want to calculate predicted marginal means (sometimes called \"least squares means\").\n",
    "These are average values of the outcome at particular levels of the predictors. For ease of interpretation, we want these\n",
    "marginal means to be on the response scale (i.e., the probability scale). We can use the `effects` package to compute\n",
    "these quantities of interest for us (by default, the numerical output will be on the response scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  eff <- allEffects(hyp_out)\n",
    "  plot(eff, type = \"response\") # \"response\" refers to the probability scale\n",
    "\n",
    "  # generate a sequence at which to get predictions of the outcome\n",
    "  seq(20, 80, by = 5)\n",
    "\n",
    "  # override defaults\n",
    "  eff <- allEffects(hyp_out, xlevels = list(age_p = seq(20, 80, by = 5)))\n",
    "  eff_df <- as.data.frame(eff) # confidence intervals\n",
    "  eff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Exercise 2 \n",
    "\n",
    "**Logistic regression**\n",
    "\n",
    "Use the `NH11` data set that we loaded earlier.\n",
    "\n",
    "1.  Use `glm()` to conduct a logistic regression to predict ever worked (`everwrk`) using age (`age_p`) and marital status (`r_maritl`). Make sure you only keep the following two levels for `everwrk` (`1 Yes` and `2 No`). Hint: use the `factor()` function. Also, make sure to drop any `r_maritl` levels that do not contain observations. Hint: see `?droplevels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "2.  Predict the probability of working for each level of marital status. Hint: use `allEffects()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data are not perfectly clean and ready to be modeled. You will need to clean up at least some of the variables before fitting the model.\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:red\"><b>Click for Exercise 2 Solution</b></span></summary>\n",
    "  <div class=\"alert alert-success\">\n",
    "\n",
    "Use the NH11 data set that we loaded earlier. Note that the data is not perfectly clean and ready to be modeled. You will need to clean up at least some of the variables before fitting the model.\n",
    "\n",
    "1.  Use `glm()` to conduct a logistic regression to predict ever worked (`everwrk`) using age (`age_p`) and marital status (`r_maritl`). Make sure you only keep the following two levels for `everwrk` (`1 Yes` and `2 No`). Hint: use the `factor()` function. Also, make sure to drop any `r_maritl` levels that do not contain observations. Hint: see `?droplevels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  NH11 <- mutate(NH11,\n",
    "                     everwrk = factor(everwrk, levels = c(\"1 Yes\", \"2 No\")),\n",
    "                     r_maritl = droplevels(r_maritl))\n",
    "\n",
    "  mod_wk_age_mar <- glm(everwrk ~ 1 + age_p + r_maritl, \n",
    "                        data = NH11,\n",
    "                        family = binomial(link = \"logit\"))\n",
    "\n",
    "  summary(mod_wk_age_mar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Predict the probability of working for each level of marital status. Hint: use `allEffects()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "  eff <- allEffects(mod_wk_age_mar)\n",
    "  as.data.frame(eff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilevel modeling\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**GOAL: To learn about how to use the `lmer()` function to model clustered data.** In particular:\n",
    "\n",
    "1. The formula syntax for incorporating random effects into a model\n",
    "2. Calculating the intraclass correlation (ICC)\n",
    "3. Model comparison for fixed and random effects\n",
    "</div>\n",
    "\n",
    "### Multilevel modeling overview\n",
    "\n",
    "* Multi-level (AKA hierarchical) models are a type of **mixed-effects** model\n",
    "* They are used to model data that are clustered (i.e., non-independent)\n",
    "* Mixed-effecs models include two types of predictors: **fixed-effects** and **random effects**\n",
    "  + **Fixed-effects** -- observed levels are of direct interest (.e.g, sex, political party...)\n",
    "  + **Random-effects** -- observed levels not of direct interest: goal is to make inferences to a population represented by observed levels\n",
    "  + In R, the `lme4` package is the most popular for mixed effects models\n",
    "  + Use the `lmer()` function for liner mixed models, `glmer()` for generalized linear mixed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exam data\n",
    "\n",
    "The Exam data set contans exam scores of 4,059 students from 65 schools in Inner London. The variable names are as follows:\n",
    "\n",
    "| Variable | Description                             |\n",
    "|:---------|:----------------------------------------|\n",
    "| school   | School ID - a factor.                   |\n",
    "| normexam | Normalized exam score.                  |\n",
    "| standLRT | Standardised LR test score.             |\n",
    "| student  | Student id (within school) - a factor   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "  Exam <- read_rds(\"dataSets/Exam.rds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The null model & ICC\n",
    "\n",
    "As a preliminary step it is often useful to partition the variance in the dependent variable into the various levels. This can be accomplished by running a null model (i.e., a model with a random effects grouping structure, but no fixed-effects predictors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "  # anatomy of lmer() function\n",
    "  lmer(outcome ~ 1 + pred1 + pred2 + (1 | grouping_variable), \n",
    "       data = mydata, \n",
    "       REML = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # null model, grouping by school but not fixed effects.\n",
    "  Norm1 <-lmer(normexam ~ 1 + (1 | school), \n",
    "              data = na.omit(Exam), REML = FALSE)\n",
    "  summary(Norm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is .161/(.161 + .852) = .159 = 16% of the variance is at the school level. \n",
    "\n",
    "There is no consensus on how to calculate p-values for MLMs; hence why they are omitted from the `lme4` output. \n",
    "But, if you really need p-values, the `lmerTest` package will calculate p-values for you (using the Satterthwaite \n",
    "approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding fixed-effects predictors\n",
    "\n",
    "Here's a model that predicts exam scores from student's standardized tests scores:\n",
    "\n",
    "<div class=\"alert alert-secondary\">\n",
    "$$\n",
    "normexam_{ij} = \\mu + \\beta_1standLRT_{ij} + U_{0j} + \\epsilon_{ij}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "where $U_{0j}$ is the random intercept for `school`. Let's implement this in R using `lmer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  Norm2 <-lmer(normexam ~ 1 + standLRT + (1 | school),\n",
    "               data = na.omit(Exam), REML = FALSE) \n",
    "  summary(Norm2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple degree of freedom comparisons\n",
    "\n",
    "As with `lm()` and `glm()` models, you can compare the two `lmer()` models using a likelihood ratio test with the `anova()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  anova(Norm1, Norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random slopes\n",
    "\n",
    "Add a random effect of students' standardized test scores as well. Now in addition to estimating the distribution of intercepts across schools, we also estimate the distribution of the slope of exam on standardized test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  Norm3 <- lmer(normexam ~ 1 + standLRT + (1 + standLRT | school), \n",
    "                data = na.omit(Exam), REML = FALSE) \n",
    "  summary(Norm3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the significance of the random slope\n",
    "\n",
    "To test the significance of a random slope just compare models with and without the random slope term using a likelihood ratio test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  anova(Norm2, Norm3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "**Multilevel modeling**\n",
    "\n",
    "Use the `bh1996` dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install.packages(\"multilevel\")\n",
    "data(bh1996, package=\"multilevel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "From the data documentation:\n",
    "\n",
    "> Variables are Leadership Climate (`LEAD`), Well-Being (`WBEING`), and Work Hours (`HRS`). The group identifier is named `GRP`.\n",
    "\n",
    "1.  Create a null model predicting wellbeing (`WBEING`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "2.  Calculate the ICC for your null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "3.  Run a second multi-level model that adds two individual-level predictors, average number of hours worked (`HRS`) and leadership skills (`LEAD`) to the model and interpret your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "4.  Now, add a random effect of average number of hours worked (`HRS`) to the model and interpret your output. Test the significance of this random term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><span style=\"color:red\"><b>Click for Exercise 3 Solution</b></span></summary>\n",
    "  <div class=\"alert alert-success\">\n",
    "\n",
    "Use the dataset, bh1996:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  data(bh1996, package=\"multilevel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data documentation:\n",
    "\n",
    "> Variables are Leadership Climate (`LEAD`), Well-Being (`WBEING`), and Work Hours (`HRS`). The group identifier is named `GRP`.\n",
    "\n",
    "1.  Create a null model predicting wellbeing (`WBEING`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  mod_grp0 <- lmer(WBEING ~ 1 + (1 | GRP), data = bh1996)\n",
    "  summary(mod_grp0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Run a second multi-level model that adds two individual-level predictors, average number of hours worked (`HRS`) and leadership skills (`LEAD`) to the model and interpret your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  mod_grp1 <- lmer(WBEING ~ 1 + HRS + LEAD + (1 | GRP), data = bh1996)\n",
    "  summary(mod_grp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Now, add a random effect of average number of hours worked (`HRS`) to the model and interpret your output. Test the significance of this random term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "  mod_grp2 <- lmer(WBEING ~ 1 + HRS + LEAD + (1 + HRS | GRP), data = bh1996)\n",
    "  anova(mod_grp1, mod_grp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "### Feedback\n",
    "\n",
    "These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu\n",
    "\n",
    "### Resources\n",
    "\n",
    "* IQSS \n",
    "    + Workshops: <https://dss.iq.harvard.edu/workshop-materials>\n",
    "    + Data Science Services: <https://dss.iq.harvard.edu/>\n",
    "    + Research Computing Environment: <https://iqss.github.io/dss-rce/>\n",
    "\n",
    "* HBS\n",
    "    + Research Computing Services workshops: <https://training.rcs.hbs.org/workshops>\n",
    "    + Other HBS RCS resources: <https://training.rcs.hbs.org/workshop-materials>\n",
    "    + RCS consulting email: <mailto:research@hbs.edu>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "results,eval,tags,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
