[
["index.html", "Data Science Workshops Table of Contents Contributors", " Data Science Workshops January 2020 Materials for the software workshops held at the Institute for Quantitative Social Science and Harvard Business School at Harvard University. Table of Contents Data Science Tools R Installation R Introduction R Regression Models R Graphics R Data Wrangling Python Installation Python Introduction Python Web-Scraping Stata Introduction Stata Data Management Stata Modeling &amp; Graphing Contributors The contents of these workshops are the result of a collaborative effort from members of the Data Science Services team at IQSS and the Research Computing Services team at HBS. The main contributors are: Ista Zahn, Steve Worthington, Bob Freeman, Jinjie Liu, Yihan Wang, and Victoria Liublinska. These workshops are a work-in-progress, please provide feedback! Email: help@iq.harvard.edu "],
["DataScienceTools.html", "Data Science Tools Tools for working with data The puzzle pieces Examples Data storage &amp; retrieval Programming languages &amp; statistics packages Creating reports Text editors &amp; Integrated Development Environments Literate programming &amp; notebooks Big data, annoying data, &amp; computationally intensive methods Wrap up", " Data Science Tools Topics Data science tool selection Data analysis pipelines Programming languages comparison Text editor and IDE comparison Tools for creating reports Tools for working with data Working with data effectively requires learning at least one programming or scripting language. You can get by without this, but it would be like trying to cook with only a butter knife; not recommended! Compared to using a menu-driven interface (e.g., SPSS or SAS) or a spreadsheet (e.g., Excel), using a programming language allows you to: reproduce results, correct errors and update output, reuse code, collaborate with others, automate repetitive tasks, and generate manuscripts, reports, and other documents from your code. So, you need to learn a programming language for working with data, but which one should you learn? Since you’ll be writing code you’ll want to set up a comfortable environment for writing and editing that code. Which text editors are good for this? You’ll probably also want to learn at least one markup language (e.g., LaTeX, Markdown) so that you can create reproducible manuscripts. What tools are good for this? These questions will guide our discussion, the goal of which is to help you decide which tools you should invest time in learning. The puzzle pieces As we’ve noted, working effectively with data requires using a number of tools. Data analysis building blocks The basic pieces are: a data storage and retrieval system, an editor for writing code, an interpreter or compiler for executing that code, a system for presenting results, and some “glue” to make all the pieces work together. Examples Before looking in detail at each of these building blocks we’ll look at a few examples to get an intuitive feel for the basic elements. Old-school example In this example we’re going to process data in a text file in a way that would be familiar to a statistician working forty years ago. Surprisingly, it’s not much different from the way we would do it today. Programs come and go, but the basic ideas remain pretty much the same! Specifically, we’ll process the data in 1980_census.txt by writing fortran code in the vi text editor and running it through the fortran compiler. Then we’ll take the results and put them in to a TeX file, again using the vi editor to create the report. For “glue” we will use a terminal emulator running the bash shell. All of these tools were available in 1980, though some features have been added since that time. OLD SCHOOL DEMO: example data storage editor program report tool glue old school ASCII text file vi fortran TeX Bourne (compatable) shell Something old &amp; something new Next we’re going to do the same basic process, this time using a modern text editor (Atom), a different programming language (Python), and a modern report generation system (LaTeX processed via xelatex). For the glue we’re still going to use a shell. OLD AND NEW DEMO: example data storage editor program report tool glue old school ASCII text file vi fortran TeX Bourne (compatable) shell old and new ASCII text file Atom python LaTeX Bash shell A modern version Finally, we’ll produce the same report using modern tools. Remember, the process is basically the same: we’re just using different tools. MODERN DEMO: example data storage editor program report tool glue old school ASCII text file vi fortran TeX Bourne (compatable) shell old and new ASCII text file Atom python LaTeX Bash shell modern SQLite database Rstudio R R Markdown Rstudio Data storage &amp; retrieval Data storage and retrieval is a fairly dry topic, so we won’t spend too much time on it. There are roughly four types of technology for storing and retrieving data. Text files Storing data in text files (e.g., comma separated values, other delimited text formats) is simple and makes the data easy to access from just about any program. It is also good for archiving data since no specialized software is needed to read it. The main downsides are that retrieval is slow and often all-or-nothing, and the fact that storing metadata in plain text files is cumbersome. Binary files Many statistics packages and programming languages have a “native” binary data storage format. For example, Stata stores data in .dta files, and R stores data in .rds or .Rdata files. These storage formats usually more efficient than text files, and usually provide faster read/write access. They usually include a mechanism for storing metadata. The down side is that specialized software is required to read them (will Stata exist in 50 years? Are you sure?) and the ability to read them using other programs may be limited. Databases Storing data in a database requires more up-front planning and set up, but has several advantages. Databases provide fast selective retrieval and facilitate efficient storage and flexible retrieval. Distributed file storage Data that is too large to fit on a single hard drive may be stored and analyzed on a distributed file system or database such as the Hadoop Distributed File System or Cassandra. When working with data on this scale considerable infrastructure and specialized tools will be required. Programming languages &amp; statistics packages There are tens of programs for statistics and data science available. Here we will focus only on the more popular programs that offer a wide range of features. Note that for specific applications a specialized program may be better, e.g., many people use Mplus for structural equation models and another program for everything else. Programming language features Things we want a statistics program to do include: read/write data from/to a variety of data storage systems, manipulate data, perform statistical methods, visualize data and results, export results in a variety of formats, be easy to use, be well documented, have a large user community. Note that this list is deceptively simple; each item may include a diversity of complicated features. For example,“read/write data from/to a variety of data storage systems” may include reading from databases, image files, .pdf files, .html and .xml files from a website, and any number of proprietary data storage formats. Program comparison Program Statistics Visualization Machine learning Ease of use Power/flexibility Fun Stata Good Servicable Limited Very easy Low Some SPSS OK Servicable Limited Easy Low None SAS Good Not great Good Moderate Moderate None Matlab Good Good Good Moderate Good Some R Excellent Excellent Good Moderate Excellent Yes Python Good Good Excellent Moderate Excellent Yes Julia OK Excellent Good Hard Excellent Yes Examples: Read data from a file &amp; summarize In this example we will compare the syntax for reading and summarizing data stored in a file. Stata import delimited using &quot;https://github.com/IQSS/dss-workshops/raw/master/R/Rgraphics/dataSets/EconomistData.csv&quot; sum set more off &quot;EconomistData.csv&quot; Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=gasp -Dswing.aatext=true -Dsun.java2d.opengl=true (6 vars, 173 obs) sum Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- v1 | 173 87 50.08493 1 173 country | 0 hdirank | 173 95.28324 55.00767 1 187 hdi | 173 .6580867 .1755888 .286 .943 cpi | 173 4.052023 2.116782 1.5 9.5 -------------+--------------------------------------------------------- region | 0 R cpi &lt;- read.csv(&quot;https://github.com/IQSS/dss-workshops/raw/master/R/Rgraphics/dataSets/EconomistData.csv&quot;) summary(cpi) X Country HDI.Rank HDI Min. : 1 Afghanistan: 1 Min. : 1.00 Min. :0.2860 1st Qu.: 44 Albania : 1 1st Qu.: 47.00 1st Qu.:0.5090 Median : 87 Algeria : 1 Median : 96.00 Median :0.6980 Mean : 87 Angola : 1 Mean : 95.28 Mean :0.6581 3rd Qu.:130 Argentina : 1 3rd Qu.:143.00 3rd Qu.:0.7930 Max. :173 Armenia : 1 Max. :187.00 Max. :0.9430 (Other) :167 CPI Region Min. :1.500 Americas :31 1st Qu.:2.500 Asia Pacific :30 Median :3.200 East EU Cemt Asia:18 Mean :4.052 EU W. Europe :30 3rd Qu.:5.100 MENA :18 Max. :9.500 SSA :46 Matlab tmpfile = websave(tempname(), &#39;https://github.com/IQSS/dss-workshops/raw/master/R/Rgraphics/dataSets/EconomistData.csv&#39;); cpi = readtable(tmpfile); summary(cpi) tmpfile = websave(tempname(), &#39;https://github.com/IQSS/dss-workshops/raw/master/R/Rgraphics/dataSets/EconomistData.csv&#39;); cpi = readtable(tmpfile); summary(cpi) Variables: Var1: 173×1 cell array of character vectors Country: 173×1 cell array of character vectors HDI_Rank: 173×1 double Description: Original column heading: &#39;HDI.Rank&#39; Values: Min 1 Median 96 Max 187 HDI: 173×1 double Values: Min 0.286 Median 0.698 Max 0.943 CPI: 173×1 double Values: Min 1.5 Median 3.2 Max 9.5 Region: 173×1 cell array of character vectors &#39;org_babel_eoe&#39; ans = &#39;org_babel_eoe&#39; Python import pandas as pd cpi = pd.read_csv(&#39;https://github.com/IQSS/dss-workshops/raw/master/R/Rgraphics/dataSets/EconomistData.csv&#39;) cpi.describe(include = &#39;all&#39;) Python 3.6.2 (default, Jul 20 2017, 03:52:27) [GCC 7.1.1 20170630] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. Unnamed: 0 Country HDI.Rank HDI CPI Region count 173.000000 173 173.000000 173.000000 173.000000 173 unique NaN 173 NaN NaN NaN 6 top NaN Oman NaN NaN NaN SSA freq NaN 1 NaN NaN NaN 46 mean 87.000000 NaN 95.283237 0.658087 4.052023 NaN std 50.084928 NaN 55.007670 0.175589 2.116782 NaN min 1.000000 NaN 1.000000 0.286000 1.500000 NaN 25% 44.000000 NaN 47.000000 0.509000 2.500000 NaN 50% 87.000000 NaN 96.000000 0.698000 3.200000 NaN 75% 130.000000 NaN 143.000000 0.793000 5.100000 NaN max 173.000000 NaN 187.000000 0.943000 9.500000 NaN Examples: Fit a linear regression Fitting statistical models is pretty straight-forward in all popular programs. Stata regress hdi cpi regress hdi cpi Source | SS df MS Number of obs = 173 -------------+---------------------------------- F(1, 171) = 168.85 Model | 2.63475703 1 2.63475703 Prob &gt; F = 0.0000 Residual | 2.6682467 171 .015603782 R-squared = 0.4968 -------------+---------------------------------- Adj R-squared = 0.4939 Total | 5.30300372 172 .030831417 Root MSE = .12492 ------------------------------------------------------------------------------ hdi | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- cpi | .0584696 .0044996 12.99 0.000 .0495876 .0673515 _cons | .4211666 .0205577 20.49 0.000 .3805871 .4617462 ------------------------------------------------------------------------------ R summary(lm(HDI ~ CPI, data = cpi)) Call: lm(formula = HDI ~ CPI, data = cpi) Residuals: Min 1Q Median 3Q Max -0.28452 -0.08380 0.01372 0.09157 0.24104 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.42117 0.02056 20.49 &lt;2e-16 *** CPI 0.05847 0.00450 12.99 &lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1249 on 171 degrees of freedom Multiple R-squared: 0.4968, Adjusted R-squared: 0.4939 F-statistic: 168.9 on 1 and 171 DF, p-value: &lt; 2.2e-16 Matlab fitlm(cpi, &#39;HDI~CPI&#39;) fitlm(cpi, &#39;HDI~CPI&#39;) ans = Linear regression model: HDI ~ 1 + CPI Estimated Coefficients: Estimate SE tStat pValue ________ _________ ______ __________ (Intercept) 0.42117 0.020558 20.487 6.7008e-48 CPI 0.05847 0.0044996 12.994 2.6908e-27 Number of observations: 173, Error degrees of freedom: 171 Root Mean Squared Error: 0.125 R-squared: 0.497, Adjusted R-Squared 0.494 F-statistic vs. constant model: 169, p-value = 2.69e-27 &#39;org_babel_eoe&#39; ans = &#39;org_babel_eoe&#39; Python import statsmodels.formula.api as model X = cpi[[&#39;CPI&#39;]] Y = cpi[[&#39;HDI&#39;]] model.OLS(Y, X).fit().summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; OLS Regression Results ============================================================================== Dep. Variable: HDI R-squared: 0.885 Model: OLS Adj. R-squared: 0.884 Method: Least Squares F-statistic: 1325. Date: Thu, 31 Aug 2017 Prob (F-statistic): 9.89e-83 Time: 23:16:45 Log-Likelihood: 8.1584 No. Observations: 173 AIC: -14.32 Df Residuals: 172 BIC: -11.16 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ CPI 0.1402 0.004 36.401 0.000 0.133 0.148 ============================================================================== Omnibus: 10.423 Durbin-Watson: 1.616 Prob(Omnibus): 0.005 Jarque-Bera (JB): 11.099 Skew: -0.599 Prob(JB): 0.00389 Kurtosis: 2.674 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. &quot;&quot;&quot; Examples: Extract links for .html file Retrieving data from a website is a common task. Here we parse a simple web page containing links to files we wish to download. Stata disp &quot;Ha ha ha! No, you do not want to use Stata for this!&quot; disp &quot;Ha ha ha! No, you do not want to use Stata for this!&quot; Ha ha ha! No, you do not want to use Stata for this! R library(xml2) index_page &lt;- read_html(&quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/&quot;) all_anchors &lt;- xml_find_all(index_page, &quot;//a&quot;) all_hrefs &lt;- xml_attr(all_anchors, &quot;href&quot;) data_hrefs &lt;- grep(&quot;\\\\.csv$&quot;, all_hrefs, value = TRUE) data_links &lt;- paste0(&quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/&quot;, data_hrefs) data_links [1] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1996.csv&quot; [2] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1997.csv&quot; [3] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1998.csv&quot; [4] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1999.csv&quot; [5] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2000.csv&quot; [6] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2001.csv&quot; [7] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2002.csv&quot; [8] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2003.csv&quot; [9] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2004.csv&quot; [10] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2005.csv&quot; [11] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2006.csv&quot; [12] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2007.csv&quot; [13] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2008.csv&quot; [14] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2009.csv&quot; [15] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2010.csv&quot; [16] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2011.csv&quot; [17] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2012.csv&quot; [18] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2013.csv&quot; [19] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2014.csv&quot; [20] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2015.csv&quot; [21] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1996.csv&quot; [22] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1997.csv&quot; [23] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1998.csv&quot; [24] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1999.csv&quot; [25] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2000.csv&quot; [26] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2001.csv&quot; [27] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2002.csv&quot; [28] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2003.csv&quot; [29] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2004.csv&quot; [30] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2005.csv&quot; [31] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2006.csv&quot; [32] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2007.csv&quot; [33] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2008.csv&quot; [34] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2009.csv&quot; [35] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2010.csv&quot; [36] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2011.csv&quot; [37] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2012.csv&quot; [38] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2013.csv&quot; [39] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2014.csv&quot; [40] &quot;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2015.csv&quot; Matlab index_page = urlread(&#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/&#39;); all_hrefs = regexp(index_page, &#39;&lt;a href=&quot;([^&quot;]*\\.csv)&quot;&gt;&#39;, &#39;tokens&#39;)&#39;; all_hrefs = [all_hrefs{:}]&#39;; all_links = strcat(&#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/&#39;, all_hrefs) index_page = urlread(&#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/&#39;); all_hrefs = regexp(index_page, &#39;&lt;a href=&quot;([^&quot;]*\\.csv)&quot;&gt;&#39;, &#39;tokens&#39;)&#39;; all_hrefs = [all_hrefs{:}]&#39;; all_links = strcat(&#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/&#39;, all_hrefs) all_links = 40×1 cell array &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1996.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1997.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1998.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1999.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2000.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2001.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2002.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2003.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2004.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2005.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2006.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2007.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2008.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2009.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2010.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2011.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2012.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2013.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2014.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2015.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1996.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1997.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1998.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1999.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2000.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2001.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2002.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2003.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2004.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2005.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2006.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2007.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2008.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2009.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2010.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2011.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2012.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2013.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2014.csv&#39; &#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2015.csv&#39; &#39;org_babel_eoe&#39; ans = &#39;org_babel_eoe&#39; Python from lxml import etree import requests index_text = requests.get(&#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/&#39;).text index_page = etree.HTML(index_text) all_hrefs = [a.values() for a in index_page.findall(&quot;.//a&quot;)] data_links = [&#39;http://tutorials.iq.harvard.edu/example_data/baby_names/EW/&#39; + href[0] for href in all_hrefs if &#39;csv&#39; in href[0]] for link in data_links: print(link) http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1996.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1997.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1998.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_1999.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2000.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2001.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2002.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2003.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2004.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2005.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2006.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2007.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2008.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2009.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2010.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2011.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2012.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2013.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2014.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/boys_2015.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1996.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1997.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1998.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_1999.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2000.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2001.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2002.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2003.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2004.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2005.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2006.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2007.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2008.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2009.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2010.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2011.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2012.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2013.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2014.csv http://tutorials.iq.harvard.edu/example_data/baby_names/EW/girls_2015.csv Creating reports Once you’ve analyzed your data you’ll most likely want to communicate your results. For short informal projects this might take the form of a blog post or an email to your colleagues. For larger more formal projects you’ll likely want to prepare a substantial report or manuscript for disseminating your findings via a journal publication or other means. Other common means of reporting research findings include posters or slides for a conference talk. Regardless of the type of report, you may choose to use either a markup language or a WYSIWYG application like Microsoft Word/Powerpoint of a desktop publishing application such as Adobe InDesign. Markup languages A markup language is a system for producing a formatted document from a text file using information by the markup. A major advantage of markup languages is that the formatting instructions can be easily generated by the program you use for analyzing your data. Markup languages include HTML, LaTeX, Markdown and many others. LaTeX and Markdown are currently popular among data scientists, although others are used as well. Markdown is easy to write and designed to be human-readable. It is newer and somewhat less feature-full compared to LaTeX. It’s main advantage is simplicity. LaTeX is more verbose but provides for just about any feature you’ll ever need. MARKDOWN DEMO LATEX DEMO Word processors Modern word processors are largely just graphical user interfaces that write a markup language (usually XML) for you. They are commonly used for creating reports, but care must be taken when doing so. If you use a word processor to produce your reports you should use the structured outline feature, link rather than embed external resources (figures, tables, etc.), use cross-referencing features, and use a bibliography management system. WORD PROCESSOR DEMO Text editors &amp; Integrated Development Environments A text editor edits text obviously. But that is not all! At a minimum, a text editor will also have a mechanism for reading and writing text files. Most text editors do much more than this. An IDE provides tools for working with code, such as syntax highlighting, code completion, jump-to-definition, execute/compile, package management, refactoring, etc. Of course an IDE includes a text editor. Editors and IDE’s are not really separate categories; as you add features to a text editor it becomes more like an IDE, and a simple IDE may provide little more than a text editor. For example, Emacs is commonly referred to as a text editor, but it provides nearly every feature you would expect an IDE to have. A more useful distinction is between language-specific editors/IDEs and general purpose editors/IDEs. The former are typically easier to set up since they come pre-configured for use with a specific language. General purpose editors/IDEs typically provide language support via plugins and may require extensive configuration for each language. Language specific editors &amp; IDEs Editor Features Ease of use Language RStudio Excellent Easy R Spyder Excellent Easy Python Stata do file editor OK Easy Stata SPSS syntax editor OK Easy SPSS LANGUAGE SPECIFIC IDE DEMO General purpose editors &amp; IDEs Editor Features Ease of use Language support Vim Excellent Hard Good Emacs Excellent Hard Excellent VS code Excellent Easy Very good Atom Good Moderate Good Eclipse Excellent Easy Good Sublime Text Good Easy Good Notepad++ OK Easy OK Textmate Good Moderate Good Kate OK Easy Good GENERAL PURPOSE EDITOR DEMO Literate programming &amp; notebooks In one of the Early demos we say an example of embedding R code in a markdown document. A closely related approach is to create a notebook that includes the prose of the report, the code used for the analysis, and the results produced by that code. Literate programming Literate programming is the practice of embedding computer code in a natural language document. For example, using RMarkdown we can embed R code in a report authored using Markdown. Python and Stata have their own versions of literate programming using Markdown. Notebooks Notebooks go one step farther, and include the output produced by the original program directly in the notebook. Examples include Jupyter, Appache Zeppelin, and Emacs Org Mode. NOTEBOOKS DEMO Big data, annoying data, &amp; computationally intensive methods Thus far we’ve discussed popular programming languages, data storage and retrieval options, text editors, and reporting technology. These are the basic building blocks I recommend using just about any time you find yourself working with data. There are times however when more is needed. For example, you may wish to use distributed computing for large or resource intensive computations. Computing clusters at Harvard Harvard provides a number of computing clusters, including Odyssey and the Research Computing Environment. Using these systems will be much easier if you know the basic tools well. After all, you’re still going to need data storage/retrieval, you’ll still need a text editor write code, and a programming language to write it in. My advice is to master these basics, and learn the rest as you need it. Wrap up Feedback These workshops are a work-in-progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu "],
["Rinstall.html", "R Installation Software Materials Launch an R session Installing R packages tidyverse for data science Installing the tidyverse Resources", " R Installation Before starting any of our R workshops, it is necessary to complete 4 tasks. Please make sure all these tasks are completed before you attend your workshop, as, depending on your internet speed, they may take a long time. If you are unable to complete all of the tasks, please stop by the training room at least 20 minutes prior to the start of the workshop. download and install R download and install RStudio download and unzip class materials install the tidyverse suite of R packages Software You must install both R and RStudio; it is essential that you have these pre-installed so that we can start the workshop on time. Mac OS X: Install R by downloading and running this .pkg file from CRAN. Install the RStudio Desktop IDE by downloading and running this .dmg file. Windows: Install R by downloading and running this .exe file from CRAN. Install the RStudio Desktop IDE by downloading and running this .exe file. Linux: Install R by downloading the binary files for your distribution from CRAN. Or you can use your package manager (e.g., for Debian/Ubuntu run sudo apt-get install r-base and for Fedora run sudo yum install R). Install the RStudio Desktop IDE for your distribution. Success? After both installations, please launch RStudio. If you were successful with the installations, you should see a window similar to this: (Please note that the version of R reported may be newer). If you are having any difficulties with the installations or your RStudio screen does not look like this one, please stop by the training room at least 20 minutes prior to the start of the workshop. Materials Download class materials for your workshop: R Introduction: https://github.com/IQSS/dss-workshops/raw/master/R/Rintro.zip R Regression Models: https://github.com/IQSS/dss-workshops/raw/master/R/Rmodels.zip R Graphics: https://github.com/IQSS/dss-workshops/raw/master/R/Rgraphics.zip R Data Wrangling: https://github.com/IQSS/dss-workshops/raw/master/R/RDataWrangling.zip Extract materials from the zipped directory (Right-click -&gt; Extract All on Windows, double-click on Mac) and move them to your desktop! Launch an R session Start RStudio and create a new project: On Windows click the start button and search for RStudio. On Mac RStudio will be in your applications folder. In Rstudio go to File -&gt; New Project. Choose Existing Directory and browse to the workshop materials directory on your desktop. Choose File -&gt; Open File and select the file with the word “BLANK” in the name. Installing R packages R is a modular environment that is extended by the use of packages. Packages are collections of functions or commands that are designed to perform specific tasks (e.g., fit a type of regression model). A large number of contributed packages are available (&gt; 15,000). Using an R package is a two step process: Install the package onto your computer using the install.packages() function. This only needs to be done the first time you use the package. Load the package into your R session’s search path using the library() function. This needs to be done each time you use the package. tidyverse for data science While R’s built-in packages are powerful, in recent years there has been a big surge in well-designed contributed packages for R. In particular, a collection of R packages called tidyverse have been designed specifically for data science. All packages included in tidyverse share an underlying design philosophy, grammar, and data structures. This philosopy is rooted in the idea of “tidy data”: A typical workflow for using tidyverse packages looks like this: Installing the tidyverse We will use the tidyverse suite of packages throughout these R workshops, so let’s install them now by entering the command: install.packages(\"tidyverse\") at the command prompt in the console (the bottom left panel in RStudio) and press Enter. When you install tidyverse for the first time you will be asked a question about installing packages from source - please answer by typing “no” in the console. A number of messages will then scroll by, and there will be a long minute or two pause where nothing appears to happen (but the installation is actually occurring). At last, the output parade should end with a message like: The downloaded source/binary packages are in.... If not, please stop by the training room at least 20 minutes before the start of your workshop. We will also install the rmarkdown package, which will allow us to combine our text and code into a formatted document at the end of the workshops. Please enter the command: install.packages(\"rmarkdown\") at the command prompt in the console (the bottom left panel in RStudio), press Enter, and then wait for the stream of messages to end with: The downloaded source/binary packages are in.... If all the above steps have been completed successfuly, you should now be ready to start your workshop. If you ran into any problems, please stop by the training room at least 20 minutes before the start of your workshop. Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu "],
["Rintro.html", "R Introduction Setup R basics Manipulating data Plotting data Creating variables Aggregating variables Saving work Exercise solutions Wrap-up", " R Introduction Topics Functions Assignment Finding help Importing packages Basic data manipulation Operations within groups of data Saving data Setup Class Structure Informal — Ask questions at any time. Really! Collaboration is encouraged - please spend a minute introducing yourself to your neighbors! Prerequisites None — assumes no prior knowledge of R Relatively slow-paced Goals We will learn about the R language by analyzing a dataset of baby names. In particular, our goals are to learn about: How we can interact with R Foundations of the language (functions, assignment) The tidyverse package ecosystem for data science Basic data manipulation useful for cleaning datasets Working with grouped data Aggregating data to create summaries Saving objects, data, and scripts This workshop will not cover how to iterate over collections of data, create your own functions, produce publication quality graphics, or fit models to data. These topics are covered in our R Data Wrangling, R Graphics, and R Regression Models workshops. R basics GOAL: To learn about the foundations of the R language. R interfaces Functions Assignment Getting help tidyverse package ecosystem for data science What is R? R is a free language and environment for statistical computing and graphics. R has existed for over 25 years and is now the most popular software for data analysis. It has an extensive ecosystem of about 15,000 add on packages covering all aspects of statistics and machine learning. Interfaces Text editors, IDEs, &amp; Notebooks There are different ways of interacting with R. The two main ways are through: text editors or Integrated Development Environments (IDEs): Text editors and IDEs are not really separate categories; as you add features to a text editor it becomes more like an IDE. Some editors/IDEs are language-specific while others are general purpose — typically providing language support via plugins. For these workshops we will use RStudio; it is a good R-specific IDE with many useful features. Here are a few popular editors/IDEs that can be used with R: Editor / IDE Features Ease of use Language support RStudio Excellent Easy R only Jupyter Lab Good Easy Excellent VS code Excellent Easy Very good Atom Good Moderate Good Vim Excellent Hard Good Emacs Excellent Hard Excellent Notebooks: Web-based applications that allow you to create and share documents that contain live code, equations, visualizations, and narrative text. A popular notebook is the open source Jupyter Notebook that has support for 40+ languages. Source code &amp; literate programming There are also several different formats available for writing code in R. These basically boil down to a choice between: Source code: the practice of writing code, and possibly comments, in a plain text document. In R this is done by writing code in a text file with a .R or .r extension. Writing source code has the great advantage of being simple. Souce code is the format of choice if you intend to run your code as a complete script - for example, from the command line. Literate programming: the practice of embedding computer code in a natural language document. In R this is often done using Rmarkdown, which involves embeddeding R code in a document that is authored using Markdown and which has a .Rmd extension. Markdown is easy to write and designed to be human-readable. Markdown is the format of choice if you intend to run your code interactively, by running small pieces of code and looking at each output. Many researchers use Markdown to write their journal papers, dissertations, and statistics/math class notes, since it is easy to convert into other formats later, such as HTML (for a webpage), MS Word, or PDF (via LaTeX). Here are some resources for learning more about Rmarkdown and RStudio: https://rmarkdown.rstudio.com/authoring_quick_tour.html https://cran.r-project.org/web/packages/rmarkdown/vignettes/rmarkdown.html https://rstudio.com/wp-content/uploads/2019/01/Cheatsheets_2019.pdf Launch a session Start RStudio and create a new project: On Windows click the start button and search for RStudio. On Mac RStudio will be in your applications folder. In Rstudio go to File -&gt; New Project. Choose Existing Directory and browse to the workshop materials directory on your desktop. Choose File -&gt; Open File and select the file with the word “BLANK” in the name. Exercise 0 The purpose of this exercise is to give you an opportunity to explore the interface provided by RStudio. You may not know how to do these things; that’s fine! This is an opportunity to figure it out. Also keep in mind that we are living in a golden age of tab completion. If you don’t know the name of an R function, try guessing the first two or three letters and pressing TAB. If you guessed correctly the function you are looking for should appear in a pop up! Try to get R to add 2 plus 2. ## Try to calculate the square root of 10. ## R includes extensive documentation, including a manual named “An introduction to R”. Use the RStudio help pane. to locate this manual. Syntax rules R is case sensitive R ignores white space Object names should start with a letter Function calls The general form for calling R functions is ## FunctionName(arg.1 = value.1, arg.2 = value.2, ..., arg.n - value.n) Arguments can be matched by name; unnamed arguments will be matched by position. values &lt;- c(1.45, 2.34, 5.68) round(x = values, digits = 1) # match by name ## [1] 1.4 2.3 5.7 round(values, 1) # match by position ## [1] 1.4 2.3 5.7 round(1, values) # be careful when matching by position! ## [1] 1 1 1 round(digits = 1, x = values) # matching by name is safer! ## [1] 1.4 2.3 5.7 Assignment Values can be assigned names and used in subsequent operations The gets &lt;- operator (less than followed by a dash) is used to save values The name on the left gets the value on the right. sqrt(10) ## calculate square root of 10; result is not stored anywhere ## [1] 3.162278 x &lt;- sqrt(10) # assign result to a variable named x Names should start with a letter, and contain only letters, numbers, underscores, and periods. Asking for help You can ask R for help using the help function, or the ? shortcut. help(help) ?help ?sqrt The help function can be used to look up the documentation for a function, or to look up the documentation to a package. We can learn how to use the stats package by reading its documentation like this: help(package = &quot;stats&quot;) If you know the name of the package you want to use, then Googling “R package-name” will often get you to the documentation. Packages are hosted on several different repositories, including: CRAN: https://cran.r-project.org/web/packages/available_packages_by_name.html Bioconductor: https://www.bioconductor.org/packages/release/bioc/ Github: http://rpkg.gepuro.net/ R-Forge: https://r-forge.r-project.org/R/?group_id=1326 If you know the type of analysis you want to perform, you can Google “CRAN Task Views”, where there are curated lists of packages https://cran.r-project.org/web/views/. If you want to know which packages are popular, you can look at https://r-pkg.org. Reading data R has data reading functionality built-in – see e.g., help(read.table). However, faster and more robust tools are available, and so to make things easier on ourselves we will use a contributed package instead. This requires that we learn a little bit about packages in R. Installing &amp; using packages R is a modular environment that is extended by the use of packages. Packages are collections of functions or commands that are designed to perform specific tasks (e.g., fit a type of regression model). A large number of contributed packages are available (&gt; 15,000). Using an R package is a two step process: Install the package onto your computer using the install.packages() function. This only needs to be done the first time you use the package. Load the package into your R session’s search path using the library() function. This needs to be done each time you use the package. The tidyverse While R’s built-in packages are powerful, in recent years there has been a big surge in well-designed contributed packages for R. In particular, a collection of R packages called tidyverse have been designed specifically for data science. All packages included in tidyverse share an underlying design philosophy, grammar, and data structures. This philosopy is rooted in the idea of “tidy data”: A typical workflow for using tidyverse packages looks like this: You should have already installed the tidyverse and rmarkdown packages onto your computer before the workshop — see R Installation. Now let’s load these packages into the search path of our R session. library(tidyverse) library(rmarkdown) Readers for common file types To read data from a file, you have to know what kind of file it is. The table below lists functions from the readr package, which is part of tidyverse, that can import data from common plain-text formats. Data Type Function comma separated read_csv() tab separated read_delim() other delimited formats read_table() fixed width read_fwf() Note You may be confused by the existence of similar functions, e.g., read.csv and read.delim. These are legacy functions that tend to be slower and less robust than the readr functions. One way to tell them apart is that the faster more robust versions use underscores in their names (e.g., read_csv) while the older functions use dots (e.g., read.csv). My advice is to use the more robust newer versions, i.e., the ones with underscores. Baby names data As an example project we will analyze the popularity of baby names in the US from 1960 through 2017. The data were retrieved from https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data. Here are the questions we will use R to answer: In which year did your name (or another name) occur most frequently by count? Which names have the highest popularity by proportion for each sex and year? How does the percentage of babies given one of the top 10 names of the year change over time? Exercise 1 Reading the baby names data Make sure you have installed the tidyverse suite of packages and attached them with library(tidyverse). Open the read_csv() help page to determine how to use it to read in data. ## Read the baby names data using the read_csv() function and assign the result with the name baby_names. ## BONUS (optional): Save the baby_names data as a Stata data set babynames.dta and as an R data set babynames.rds. ## Manipulating data GOAL: To learn about basic data manipulation used to clean datasets. In particular: Filtering data by choosing rows — using the filter() function Selecting data by choosing columns — using the select() function Arranging data by reordering rows — using the arrange() function Using the pipe %&gt;% operator to simplify sequential operations In this section we will pull out specific names from the baby names data and examine changes in their popularity over time. The baby_names object we created in the last exercise is a data.frame. There are many other data structures in R, but for now we’ll focus on working with data.frames. Think of a data.frame as a spreadsheet. R has decent data manipulation tools built-in – see e.g., help(Extract). But, tidyverse packages often provide more intuitive syntax for accomplishing the same task. In particular, we will use the dplyr package from tidyverse to filter, select, and arrange data, as well as create new variables. Filter, select, &amp; arrange One way to find the year in which your name was the most popular is to filter out just the rows corresponding to your name, and then arrange (sort) by Count. To demonstrate these techniques we’ll try to determine whether “Alex”\" or “Mark” was more popular in 1992. We start by filtering the data so that we keep only rows where Year is equal to 1992 and Name is either “Alex” or “Mark”. ## Read in the baby names data if you haven&#39;t already baby_names &lt;- read_csv(&quot;babyNames.csv&quot;) baby_names_alexmark &lt;- filter(baby_names, Year == 1992 &amp; (Name == &quot;Alex&quot; | Name == &quot;Mark&quot;)) print(baby_names_alexmark) # explicit printing ## # A tibble: 4 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alex Girls 366 1992 ## 2 Mark Girls 20 1992 ## 3 Mark Boys 8743 1992 ## 4 Alex Boys 7348 1992 baby_names_alexmark # implicit printing ## # A tibble: 4 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alex Girls 366 1992 ## 2 Mark Girls 20 1992 ## 3 Mark Boys 8743 1992 ## 4 Alex Boys 7348 1992 Notice that we can combine conditions using &amp; (AND) and | (OR). In this case it’s pretty easy to see that “Mark” is more popular, but to make it even easier we can arrange the data so that the most popular name is listed first. arrange(baby_names_alexmark, Count) ## # A tibble: 4 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mark Girls 20 1992 ## 2 Alex Girls 366 1992 ## 3 Alex Boys 7348 1992 ## 4 Mark Boys 8743 1992 arrange(baby_names_alexmark, desc(Count)) ## # A tibble: 4 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mark Boys 8743 1992 ## 2 Alex Boys 7348 1992 ## 3 Alex Girls 366 1992 ## 4 Mark Girls 20 1992 We can also use the select() function to subset the data.frame by columns. We can then assign the output to a new object. If we would just like to glance at the first few lines we can use the head() function: baby_names_subset &lt;- select(baby_names, Name, Count) head(baby_names_subset) ## # A tibble: 6 x 2 ## Name Count ## &lt;chr&gt; &lt;dbl&gt; ## 1 Mary 51474 ## 2 Susan 39200 ## 3 Linda 37314 ## 4 Karen 36376 ## 5 Donna 34133 ## 6 Lisa 33702 head(baby_names_subset, n = 6) # default is n = 6 ## # A tibble: 6 x 2 ## Name Count ## &lt;chr&gt; &lt;dbl&gt; ## 1 Mary 51474 ## 2 Susan 39200 ## 3 Linda 37314 ## 4 Karen 36376 ## 5 Donna 34133 ## 6 Lisa 33702 Logical operators In the previous example we used == to filter rows. Other relational and logical operators are listed below. Operator Meaning == equal to != not equal to &gt; greater than &gt;= greater than or equal to &lt; less than &lt;= less than or equal to %in% contained in These operators may be combined with &amp; (and) or | (or). For example: x &lt;- 1:10 # a vector x ## [1] 1 2 3 4 5 6 7 8 9 10 x &gt; 7 # a simple condition ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE x &gt; 7 | x &lt; 3 # two conditions combined ## [1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE # x %in% vector # elements of x matched in vector x %in% c(1, 5, 10) ## [1] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE Control flow We won’t be covering examples of control flow in this workshop, but we will briefly introduce this important concept here. For more details see: https://swcarpentry.github.io/r-novice-gapminder/07-control-flow/. Our R Data Wrangling workshop also covers some of these topics. There are two major tools for controlling the flow of code in a script: Choices: such as if and else statements, allow you to run different code depending on the input. The basic form is: if (condition) true_action else false_action If condition is TRUE, true_action is evaluated; if condition is FALSE, the optional false_action is evaluated. Loops: such as for and while loops, allow you to repeatedly run code, typically with changing options. The basic form is: for (item in collection) perform_action For each item in collection, perform_action is called once; updating the value of item each time. Exercise 2.1 Peak popularity of your name In this exercise you will discover the year your name reached its maximum popularity. Read in the “babyNames.csv” file if you have not already done so, assigning the result to baby_names. Make sure you have installed the tidyverse suite of packages and attached them with library(tidyverse). Use filter to extract data for your name (or another name of your choice). ## Arrange the data you produced in step 1 above by Count. In which year was the name most popular? ## BONUS (optional): Filter the data to extract only the row containing the most popular boys name in 1999. ## Pipe operator There is one very special operator in R called a pipe operator that looks like this: %&gt;%. It allows us to “chain” several function calls and, as each function returns an object, feed it into the next call in a single statement, without needing extra variables to store the intermediate results. The point of the pipe is to help you write code in a way that is easier to read and understand as we will see below. There is no need to load any additional packages as the operator is made available via the magrittr package installed as part of tidyverse. Let’s rewrite the sequence of commands to output ordered counts for names “Alex” or “Mark”. # unpiped version baby_names_alexmark &lt;- filter(baby_names, Year == 1992 &amp; (Name == &quot;Alex&quot; | Name == &quot;Mark&quot;)) arrange(baby_names_alexmark, desc(Count)) ## # A tibble: 4 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mark Boys 8743 1992 ## 2 Alex Boys 7348 1992 ## 3 Alex Girls 366 1992 ## 4 Mark Girls 20 1992 # piped version baby_names %&gt;% filter(Year == 1992 &amp; (Name == &quot;Alex&quot; | Name == &quot;Mark&quot;)) %&gt;% arrange(desc(Count)) ## # A tibble: 4 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mark Boys 8743 1992 ## 2 Alex Boys 7348 1992 ## 3 Alex Girls 366 1992 ## 4 Mark Girls 20 1992 Hint: try pronouncing “then” whenever you see %&gt;%. Using pseudocode, we can see what the pipe is doing: # unpiped version filter(dataset, condition) # piped version dataset %&gt;% filter(condition) # what the pipe is doing output_of_thing_on_left %&gt;% becomes_input_of_thing_on_right Advantages of using the pipe: We can avoid creating intermediate variables, such as baby_names_alexmark Less to type Easier to read and follow the logic (especially avoiding using nested functions) Exercise 2.2 Rewrite the solution to Exercise 2.1 using pipes. Remember that we were looking for the year your name reached its maximum popularity. For that, we filtered the data and then arranged by Count. ## Plotting data GOAL: Plot baby name trends over time – using the qplot() function It can be difficult to spot trends when looking at summary tables. Plotting the data makes it easier to identify interesting patterns. R has decent plotting tools built-in – see e.g., help(plot). However, again, we will make use of a contributed package from tidyverse called ggplot2. For quick and simple plots we can use the qplot() function from ggplot2. For example, we can plot the number of babies given the name “Diana” over time like this: baby_names_diana &lt;- filter(baby_names, Name == &quot;Diana&quot;) qplot(x = Year, y = Count, data = baby_names_diana) Interestingly, there are usually some gender-atypical names, even for very strongly gendered names like “Diana”. Splitting these trends out by Sex is very easy: qplot(x = Year, y = Count, color = Sex, data = baby_names_diana) Exercise 3 Plot peak popularity of your name Make sure the tidyverse suite of packages is installed, and that you have attached them using library(tidyverse). Use filter to extract data for your name (same as previous exercise) ## Plot the data you produced in step 1 above, with Year on the x-axis and Count on the y-axis. ## Adjust the plot so that is shows boys and girls in different colors. ## BONUS (Optional): Adjust the plot to use lines instead of points. ## Creating variables GOAL: To learn how to create new variables with and without grouped data. In particular: Creating new variables (columns) — using the mutate() function Creating new variables within groups — by combining the mutate() and group_by() functions We want to use these skills to find out which names have been the most popular. Create or modify columns So far we’ve used Count as a measure of popularity. A better approach is to use proportion to avoid confounding popularity with the number of babies born in a given year. The mutate() function makes it easy to add or modify the columns of a data.frame. For example, we can use it to rescale the count of each name in each year: baby_names &lt;- baby_names %&gt;% mutate(Count_1k = Count/1000) head(baby_names) ## # A tibble: 6 x 5 ## Name Sex Count Year Count_1k ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mary Girls 51474 1960 51.5 ## 2 Susan Girls 39200 1960 39.2 ## 3 Linda Girls 37314 1960 37.3 ## 4 Karen Girls 36376 1960 36.4 ## 5 Donna Girls 34133 1960 34.1 ## 6 Lisa Girls 33702 1960 33.7 Operating by group Because of the nested nature of our data, we want to compute proportion or rank within each Sex by Year group. The dplyr package has a group_by() function that makes this relatively straightforward. Here’s the logic behind this process: Note that the group_by() function converts a data frame into a grouped data frame — that is, a data frame with metadata identifying the groups. The data remain grouped until you change the groups by running group_by() again or remove the grouping metadata using ungroup(). Here’s the code that implements the calculation: baby_names &lt;- baby_names %&gt;% group_by(Year, Sex) %&gt;% mutate(Rank = rank(Count_1k)) %&gt;% ungroup() head(baby_names) ## # A tibble: 6 x 6 ## Name Sex Count Year Count_1k Rank ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mary Girls 51474 1960 51.5 7331 ## 2 Susan Girls 39200 1960 39.2 7330 ## 3 Linda Girls 37314 1960 37.3 7329 ## 4 Karen Girls 36376 1960 36.4 7328 ## 5 Donna Girls 34133 1960 34.1 7327 ## 6 Lisa Girls 33702 1960 33.7 7326 Exercise 4 Most popular names In this exercise your goal is to identify the most popular names for each year. Use mutate() and group_by() to create a column named Proportion where Proportion = Count/sum(Count) for each Year X Sex group. Use pipes wherever it makes sense. ## Use mutate() and group_by() to create a column named Rank where Rank = rank(desc(Count)) for each Year X Sex group. ## Filter the baby names data to display only the most popular name for each Year X Sex group. Keep only the columns: Year, Name, Sex, and Proportion. ## Plot the data produced in step 3, putting Year on the x-axis and Proportion on the y-axis. How has the proportion of babies given the most popular name changed over time? ## BONUS (optional): Which names are the most popular for both boys and girls? ## Aggregating variables GOAL: To learn how to aggregate data to create summaries with and without grouped data. In particular: Collapsing data into summaries — using the summarize() function Creating summaries within groups — by combining the summarize() and group_by() functions You may have noticed that the percentage of babies given the most popular name of the year appears to have decreased over time. We can compute a more robust measure of the popularity of the most popular names by calculating the number of babies given one of the top 10 girl or boy names of the year. To compute this measure we need to operate within groups, as we did using mutate() above, but this time we need to collapse each group into a single summary statistic. We can achieve this using the summarize() function. First, let’s see how this function works without grouping. The following code outputs the total number of girls and boys in the data: baby_names %&gt;% summarize(Girls_n = sum(Sex==&quot;Girls&quot;), Boys_n = sum(Sex==&quot;Boys&quot;)) ## # A tibble: 1 x 2 ## Girls_n Boys_n ## &lt;int&gt; &lt;int&gt; ## 1 641084 407491 Next, using group_by() and summarize() together, we can calculate the number of babies born each year. Here’s the logic behind this process: Here’s the code that implements the calculation: bn_by_year &lt;- baby_names %&gt;% group_by(Year) %&gt;% summarize(Total = sum(Count)) %&gt;% ungroup() head(bn_by_year) ## # A tibble: 6 x 2 ## Year Total ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1960 4154377 ## 2 1961 4140244 ## 3 1962 4035234 ## 4 1963 3958791 ## 5 1964 3887800 ## 6 1965 3626029 Exercise 5 Popularity of the most popular names In this exercise we will plot trends in the proportion of boys and girls given one of the 10 most popular names each year. Filter the baby_names data, retaining only the 10 most popular girl and boy names for each year. ## Summarize the data produced in step one to calculate the total Proportion of boys and girls given one of the top 10 names each year. ## Plot the data produced in step 2, with year on the x-axis and total proportion on the y axis. Color by Sex and notice the trend. ## Saving work GOAL: To learn how to save objects, data, and scripts for later use. Now that we have made some changes to our data set, we might want to save those changes to a file. Saving individual datasets You might find functions write_csv() and write_rds() from package readr handy! # write data to a .csv file write_csv(baby_names, &quot;babyNames.csv&quot;) # write data to an R file write_rds(baby_names, &quot;babyNames.rds&quot;) Saving multiple datasets ls() # list objects in our workspace save(baby_names_diana, bn_by_year, baby_names_subset, file=&quot;myDataFiles.RData&quot;) ## Load the &quot;myDataFiles.RData&quot; ## load(&quot;myDataFiles.RData&quot;) Saving &amp; loading workspaces In addition to importing individual datasets, R can save and load entire “workspaces”. The workspace is your current R working environment and includes any user-defined objects. At the end of a session, you can save an “image” of the current workspace, which allows you to automatically reload the objects you previously created. ls() # list objects in our workspace save.image(file=&quot;myWorkspace.RData&quot;) # save workspace rm(list=ls()) # remove all objects from our workspace ls() # list stored objects to make sure they are deleted ## Load the &quot;myWorkspace.RData&quot; file and check that it is restored load(&quot;myWorkspace.RData&quot;) # load myWorkspace.RData ls() # list objects Exercise solutions Ex 0: prototype 2 plus 2 2 + 2 ## [1] 4 # or sum(2, 2) ## [1] 4 square root of 10 sqrt(10) ## [1] 3.162278 # or 10^(1/2) ## [1] 3.162278 Find “An Introduction to R”. # Go to the main help page by running &#39;help.start() or using the GUI # menu, find and click on the link to &quot;An Introduction to R&quot;. Ex 1: prototype Open the read_csv() help page to determine how to use it to read in data. ?read_csv Read the baby names data using the read_csv() function and assign the result with the name baby_names. baby_names &lt;- read_csv(&quot;babyNames.csv&quot;) BONUS (optional): Save the baby_names data as a Stata data set babynames.dta and as an R data set babynames.rds. write_dta(baby_names, version = 15, path = “babynames.dta”) write_rds(baby_names, file = “babynames.rds”) Ex 2.1: prototype Use filter to extract data for your name (or another name of your choice). baby_names_george &lt;- filter(baby_names, Name == &quot;George&quot;) Arrange the data you produced in step 1 above by Count. In which year was the name most popular? arrange(baby_names_george, desc(Count)) ## # A tibble: 97 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 George Boys 14063 1960 ## 2 George Boys 13638 1961 ## 3 George Boys 12553 1962 ## 4 George Boys 12084 1963 ## 5 George Boys 11793 1964 ## 6 George Boys 10683 1965 ## 7 George Boys 9942 1966 ## 8 George Boys 9702 1967 ## 9 George Boys 9388 1968 ## 10 George Boys 9203 1969 ## # … with 87 more rows BONUS (optional): Filter the data to extract only the row containing the most popular boys name in 1999. baby_names_boys_1999 &lt;- filter(baby_names, Year == 1999 &amp; Sex == &quot;Boys&quot;) filter(baby_names_boys_1999, Count == max(Count)) ## # A tibble: 1 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jacob Boys 35361 1999 Ex 2.2: prototype Rewrite the solution to Exercise 2.1 using pipes. baby_names %&gt;% filter(Name == &quot;George&quot;) %&gt;% arrange(desc(Count)) ## # A tibble: 97 x 4 ## Name Sex Count Year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 George Boys 14063 1960 ## 2 George Boys 13638 1961 ## 3 George Boys 12553 1962 ## 4 George Boys 12084 1963 ## 5 George Boys 11793 1964 ## 6 George Boys 10683 1965 ## 7 George Boys 9942 1966 ## 8 George Boys 9702 1967 ## 9 George Boys 9388 1968 ## 10 George Boys 9203 1969 ## # … with 87 more rows Ex 3: prototype Use filter() to extract data for your name (same as previous exercise). baby_names_george &lt;- filter(baby_names, Name == &quot;George&quot;) Plot the data you produced in step 1 above, with Year on the x-axis and Count on the y-axis. qplot(x = Year, y = Count, data = baby_names_george) Adjust the plot so that is shows boys and girls in different colors. qplot(x = Year, y = Count, color = Sex, data = baby_names_george) BONUS (Optional): Adjust the plot to use lines instead of points. qplot(x = Year, y = Count, color = Sex, data = baby_names_george, geom = &quot;line&quot;) Ex 4: prototype Use mutate() and group_by() to create a column named Proportion where Proportion = Count/sum(Count) for each Year X Sex group. baby_names &lt;- baby_names %&gt;% group_by(Year, Sex) %&gt;% mutate(Proportion = Count/sum(Count)) %&gt;% ungroup() head(baby_names) ## # A tibble: 6 x 5 ## Name Sex Count Year Proportion ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mary Girls 51474 1960 0.0255 ## 2 Susan Girls 39200 1960 0.0194 ## 3 Linda Girls 37314 1960 0.0185 ## 4 Karen Girls 36376 1960 0.0180 ## 5 Donna Girls 34133 1960 0.0169 ## 6 Lisa Girls 33702 1960 0.0167 Use mutate() and group_by() to create a column named Rank where Rank = rank(desc(Count)) for each Year X Sex group. baby_names &lt;- baby_names %&gt;% group_by(Year, Sex) %&gt;% mutate(Rank = rank(desc(Count))) %&gt;% ungroup() head(baby_names) ## # A tibble: 6 x 6 ## Name Sex Count Year Proportion Rank ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mary Girls 51474 1960 0.0255 1 ## 2 Susan Girls 39200 1960 0.0194 2 ## 3 Linda Girls 37314 1960 0.0185 3 ## 4 Karen Girls 36376 1960 0.0180 4 ## 5 Donna Girls 34133 1960 0.0169 5 ## 6 Lisa Girls 33702 1960 0.0167 6 Filter the baby names data to display only the most popular name for each Year X Sex group. top1 &lt;- baby_names %&gt;% filter(Rank == 1) %&gt;% select(Year, Name, Sex, Proportion) head(top1) ## # A tibble: 6 x 4 ## Year Name Sex Proportion ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1960 Mary Girls 0.0255 ## 2 1960 David Boys 0.0403 ## 3 1961 Mary Girls 0.0236 ## 4 1961 Michael Boys 0.0409 ## 5 1962 Lisa Girls 0.0234 ## 6 1962 Michael Boys 0.0411 Plot the data produced in step 3, putting Year on the x-axis and Proportion on the y-axis. How has the proportion of babies given the most popular name changed over time? qplot(x = Year, y = Proportion, color = Sex, data = top1, geom = &quot;line&quot;) BONUS (optional): Which names are the most popular for both boys and girls? girls_and_boys &lt;- inner_join(filter(baby_names, Sex == &quot;Boys&quot;), filter(baby_names, Sex == &quot;Girls&quot;), by = c(&quot;Year&quot;, &quot;Name&quot;)) girls_and_boys &lt;- mutate(girls_and_boys, Product = Count.x * Count.y, Rank = rank(desc(Product))) filter(girls_and_boys, Rank == 1) ## # A tibble: 1 x 12 ## Name Sex.x Count.x Year Proportion.x Rank.x Sex.y Count.y Proportion.y ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Tayl… Boys 7688 1993 0.00392 51 Girls 21266 0.0118 ## # … with 3 more variables: Rank.y &lt;dbl&gt;, Product &lt;dbl&gt;, Rank &lt;dbl&gt; Ex 5: prototype Filter the baby_names data, retaining only the 10 most popular girl and boy names for each year. most_popular &lt;- baby_names %&gt;% group_by(Year, Sex) %&gt;% filter(Rank &lt;= 10) head(most_popular, n = 10) ## # A tibble: 10 x 6 ## # Groups: Year, Sex [1] ## Name Sex Count Year Proportion Rank ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mary Girls 51474 1960 0.0255 1 ## 2 Susan Girls 39200 1960 0.0194 2 ## 3 Linda Girls 37314 1960 0.0185 3 ## 4 Karen Girls 36376 1960 0.0180 4 ## 5 Donna Girls 34133 1960 0.0169 5 ## 6 Lisa Girls 33702 1960 0.0167 6 ## 7 Patricia Girls 32102 1960 0.0159 7 ## 8 Debra Girls 26737 1960 0.0132 8 ## 9 Cynthia Girls 26725 1960 0.0132 9 ## 10 Deborah Girls 25264 1960 0.0125 10 Summarize the data produced in step one to calculate the total Proportion of boys and girls given one of the top 10 names each year. top10 &lt;- most_popular %&gt;% # it is already grouped by Year and Sex summarize(TotalProportion = sum(Proportion)) Plot the data produced in step 2, with year on the x-axis and total proportion on the y axis. Color by Sex. qplot(x = Year, y = TotalProportion, color = Sex, data = top10, geom = &quot;line&quot;) Wrap-up Feedback These workshops are a work-in-progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu Software (all free!): R and R package download: http://cran.r-project.org Rstudio download: http://rstudio.org ESS (emacs R package): http://ess.r-project.org/ Cheatsheets https://rstudio.com/wp-content/uploads/2019/01/Cheatsheets_2019.pdf Online tutorials https://swirlstats.com/ https://r4ds.had.co.nz/ https://hbs-rcs.github.io/R_Intro-gapminder/base-r/ https://www.pluralsight.com/search?q=R https://www.datacamp.com/ https://rmarkdown.rstudio.com/lesson-1.html Getting help: Documentation and tutorials: http://cran.r-project.org/other-docs.html Recommended R packages by topic: http://cran.r-project.org/web/views/ Mailing list: https://stat.ethz.ch/mailman/listinfo/r-help StackOverflow: http://stackoverflow.com/questions/tagged/r R-Bloggers: https://www.r-bloggers.com/ Coming from … Stata: http://www.princeton.edu/~otorres/RStata.pdf SAS/SPSS: http://r4stats.com/books/free-version/ Matlab: http://www.math.umaine.edu/~hiebeler/comp/matlabR.pdf Python: http://mathesaurus.sourceforge.net/matlab-python-xref.pdf "],
["Rmodels.html", "R Regression Models Setup Before fitting a model Models with continuous outcomes Interactions &amp; factors Models with binary outcomes Multilevel modeling Exercise solutions Wrap-up", " R Regression Models Topics R formula interface Run and interpret variety of regression models in R Factor contrasts to test specific hypotheses Model comparisons Predicted marginal effects Setup Class Structure Informal — Ask questions at any time. Really! Collaboration is encouraged - please spend a minute introducing yourself to your neighbors! Prerequisites This is an intermediate R course: Assumes working knowledge of R Relatively fast-paced This is not a statistics course! We assume you know the theory behind the models. Launch an R session Start RStudio and create a new project: On Windows click the start button and search for RStudio. On Mac RStudio will be in your applications folder. In Rstudio go to File -&gt; New Project. Choose Existing Directory and browse to the workshop materials directory on your desktop. Choose File -&gt; Open File and select the file with the word “BLANK” in the name. Packages You should have already installed the tidyverse and rmarkdown packages onto your computer before the workshop — see R Installation. Now let’s load these packages into the search path of our R session. library(tidyverse) library(rmarkdown) Finally, lets install some packages that will help with modeling: # install.packages(&quot;lme4&quot;) library(lme4) # for mixed models # install.packages(&quot;emmeans&quot;) library(emmeans) # for marginal effects # install.packages(&quot;effects&quot;) library(effects) # for predicted marginal means Workshop Outline Preliminary steps before modeling Modeling continuous outcomes Modeling binary outcomes Modeling clustered data Before fitting a model Load the data List the data files we’re going to work with: list.files(&quot;dataSets&quot;) We’re going to use the states data first, which originally appeared in Statistics with Stata by Lawrence C. Hamilton. # read the states data states_data &lt;- read_rds(&quot;dataSets/states.rds&quot;) # look at the last few rows tail(states_data) Variable Description csat Mean composite SAT score expense Per pupil expenditures percent % HS graduates taking SAT income Median household income, $1,000 region Geographic region: West, N. East, South, Midwest house House ’91 environ. voting, % senate Senate ’91 environ. voting, % energy Per capita energy consumed, Btu metro Metropolitan area population, % waste Per capita solid waste, tons Examine the data Start by examining the data to check for problems. # summary of expense and csat columns, all rows sts_ex_sat &lt;- states_data %&gt;% select(expense, csat) summary(sts_ex_sat) # correlation between expense and csat cor(sts_ex_sat, use = &quot;pairwise&quot;) Plot the data Plot the data to look for multivariate outliers, non-linear relationships etc. # scatter plot of expense vs csat plot(sts_ex_sat) Models with continuous outcomes Ordinary least squares (OLS) regression models can be fit with the lm() function. In R, we convert our theoretical model into a formula — a symbolic representation of the model: # R regression formula outcome ~ pred1 + pred2 + pred3 # NOTE the ~ is a tilde For example, we can use lm() to predict SAT scores based on per-pupil expenditures: # Fit our regression model sat_mod &lt;- lm(csat ~ 1 + expense, # regression formula data = states_data) # data # Summarize and print the results summary(sat_mod) %&gt;% coef() # show regression coefficients table Why is the association between expense &amp; SAT scores negative? Many people find it surprising that the per-capita expenditure on students is negatively related to SAT scores. The beauty of multiple regression is that we can try to pull these apart. What would the association between expense and SAT scores be if there were no difference among the states in the percentage of students taking the SAT? lm(csat ~ 1 + expense + percent, data = states_data) %&gt;% summary() The lm class &amp; methods OK, we fit our model. Now what? Examine the model object: class(sat_mod) str(sat_mod) names(sat_mod) methods(class = class(sat_mod)) Use function methods to get more information about the fit: summary(sat_mod) summary(sat_mod) %&gt;% coef() methods(&quot;summary&quot;) confint(sat_mod) Selected post-estimation tools you may find useful: Function Package Output summary() stats base R standard errors, test statistics, p-values, GOF stats confint() stats base R confidence intervals anova() stats base R anova table (one model), model comparison (&gt; one model) coef() stats base R point estimates drop1() stats base R model comparison predict() stats base R predicted response values fitted() stats base R predicted response values (for observed data) residuals() stats base R residuals fixef() lme4 fixed effect point estimates (mixed models only) ranef() lme4 random effect point estimates (mixed models only) coef() lme4 empirical Bayes estimates (mixed models only) allEffects() effects predicted marginal means emmeans() emmeans predicted marginal means &amp; marginal effects margins() margins predicted marginal means &amp; marginal effects OLS regression assumptions OLS regression relies on several assumptions, including: The model includes all relevant variables (i.e., no omitted variable bias). The model is linear in the parameters (i.e., the coefficients and error term). The error term has an expected value of zero. All right-hand-side variables are uncorrelated with the error term. No right-hand-side variables are a perfect linear function of other RHS variables. Observations of the error term are uncorrelated with each other. The error term has constant variance (i.e., homoscedasticity). (Optional - only needed for inference). The error term is normally distributed. Investigate assumptions #7 and #8 visually by plotting your model: par(mfrow = c(2, 2)) # splits the plotting window into 4 panels plot(sat_mod) Comparing models Do congressional voting patterns predict SAT scores over and above expense? Fit two models and compare them: # fit another model, adding house and senate as predictors sat_voting_mod &lt;- lm(csat ~ 1 + expense + house + senate, data = na.omit(states_data)) summary(sat_voting_mod) %&gt;% coef() What does na.omit() do? # fake data dat &lt;- data.frame( x = 1:5, y = c(3, 2, 1, NA, 5), z = c(6, NA, 2, 7, 3)) dat na.omit(dat) # listwise deletion of observations # also see # ?complete.cases dat[with(dat, complete.cases(x, y, z)), ] Now let’s update our first model using na.omit(): sat_mod &lt;- update(sat_mod, data = na.omit(states_data)) # compare using an F-test with the anova() function anova(sat_mod, sat_voting_mod) Exercise 0 Ordinary least squares regression Use the states.rds data set. Fit a model predicting energy consumed per capita (energy) from the percentage of residents living in metropolitan areas (metro). Be sure to Examine/plot the data before fitting the model ## Print and interpret the model summary() ## plot() the model to look for deviations from modeling assumptions ## Select one or more additional predictors to add to your model and repeat steps 1-3. Is this model significantly better than the model with metro as the only predictor? ## Interactions &amp; factors Modeling interactions Interactions allow us assess the extent to which the association between one predictor and the outcome depends on a second predictor. For example: Does the association between expense and SAT scores depend on the median income in the state? # Add the interaction to the model sat_expense_by_percent &lt;- lm(csat ~ 1 + expense + income + expense : income, data = states_data) sat_expense_by_percent &lt;- lm(csat ~ 1 + expense * income, data = states_data) # same as above, but shorter syntax # Show the regression coefficients table summary(sat_expense_by_percent) %&gt;% coef() Regression with categorical predictors Let’s try to predict SAT scores from region, a categorical variable. Note that you must make sure R does not think your categorical variable is numeric. # make sure R knows region is categorical str(states_data$region) states_data$region &lt;- factor(states_data$region) # arguments to the factor() function # factor(x, levels, labels) levels(states_data$region) # Add region to the model sat_region &lt;- lm(csat ~ 1 + region, data = states_data) # Show the results summary(sat_region) %&gt;% coef() # show the regression coefficients table anova(sat_region) # show ANOVA table Again, make sure to tell R which variables are categorical by converting them to factors! Setting factor reference groups &amp; contrasts The default contrasts in R are “treatment” contrasts, or “dummy coding”, with the first level as the reference category. We can also easily change the reference group or get all sets of pairwise contrasts. # change the reference group states_data$region &lt;- relevel(states_data$region, ref = &quot;Midwest&quot;) m1 &lt;- lm(csat ~ 1 + region, data = states_data) summary(m1) %&gt;% coef() # get all pairwise contrasts between means means &lt;- emmeans(m1, specs = ~ region) means contrast(means, method = &quot;pairwise&quot;) Exercise 1 Interactions &amp; factors Use the states data set. Add on to the regression equation that you created in Exercise 1 by generating an interaction term and testing the interaction. ## Try adding region to the model. Are there significant differences across the four regions? ## Models with binary outcomes Logistic regression This far we have used the lm() function to fit our regression models. lm() is great, but limited–in particular it only fits models for continuous dependent variables. For categorical dependent variables we can use the glm() function. For these models we will use a different dataset, drawn from the National Health Interview Survey. From the CDC website: The National Health Interview Survey (NHIS) has monitored the health of the nation since 1957. NHIS data on a broad range of health topics are collected through personal household interviews. For over 50 years, the U.S. Census Bureau has been the data collection agent for the National Health Interview Survey. Survey results have been instrumental in providing data to track health status, health care access, and progress toward achieving national health objectives. Load the National Health Interview Survey data: NH11 &lt;- read_rds(&quot;dataSets/NatHealth2011.rds&quot;) Logistic regression example Motivation for a logistic regression model — with a binary response: Errors will not be normally distributed Variance will not be homoskedastic Predictions should be constrained to be on the interval [0, 1] Anatomy of a generalized linear model: # OLS model using lm() lm(outcome ~ 1 + pred1 + pred1, data = mydata) # OLS model using glm() glm(outcome ~ 1 + pred1 + pred1, data = mydata, family = gaussian(link = &quot;identity&quot;)) # logistic model using glm() glm(outcome ~ 1 + pred1 + pred1, data = mydata, family = binomial(link = &quot;logit&quot;)) The family argument sets the error distribution for the model, while the link function argument relates the predictors to the expected value of the outcome. Let’s predict the probability of being diagnosed with hypertension based on age, sex, sleep, and bmi: str(NH11$hypev) # check stucture of hypev levels(NH11$hypev) # check levels of hypev # collapse all missing values to NA NH11$hypev &lt;- factor(NH11$hypev, levels=c(&quot;2 No&quot;, &quot;1 Yes&quot;)) # run our regression model hyp_out &lt;- glm(hypev ~ 1 + age_p + sex + sleep + bmi, data = NH11, family = binomial(link = &quot;logit&quot;)) summary(hyp_out) %&gt;% coef() Logistic regression coefficients Generalized linear models use link functions, so raw coefficients are difficult to interpret. For example, the age coefficient of .06 in the previous model tells us that for every one unit increase in age, the log odds of hypertension diagnosis increases by 0.06. Since most of us are not used to thinking in log odds this is not too helpful! One solution is to transform the coefficients to make them easier to interpret. Here we transform them into odds ratios by exponentiating: # point estimates coef(hyp_out) %&gt;% exp() # confidence intervals confint(hyp_out) %&gt;% exp() Packages for computing &amp; graphing predicted values Instead of doing this manually, we can use the effects package to compute quantities of interest for us. eff &lt;- allEffects(hyp_out) plot(eff, type = &quot;response&quot;) # &quot;response&quot; refers to the probability scale # generate a sequence at which to get predictions of the outcome seq(20, 80, by = 5) # override defaults eff &lt;- allEffects(hyp_out, xlevels = list(age_p = seq(20, 80, by = 5))) eff_df &lt;- as.data.frame(eff) # confidence intervals eff_df Exercise 2 Logistic regression Use the NH11 data set that we loaded earlier. Use glm() to conduct a logistic regression to predict ever worked (everwrk) using age (age_p) and marital status (r_maritl). Make sure you only keep the following two levels for everwrk (1 Yes and 2 No). Hint: use the factor() function. Also, make sure to drop any r_maritl levels that do not contain observations. Hint: see ?droplevels. ## Predict the probability of working for each level of marital status. Hint: use allEffects() ## Note that the data are not perfectly clean and ready to be modeled. You will need to clean up at least some of the variables before fitting the model. Multilevel modeling Multilevel modeling overview Multi-level (AKA hierarchical) models are a type of mixed-effects models Used to model data that are clustered Mixed-effecs models include two types of predictors: fixed-effects and random effects Fixed-effects – observed levels are of direct interest (.e.g, sex, political party…) Random-effects – observed levels not of direct interest: goal is to make inferences to a population represented by observed levels In R the lme4 package is the most popular for mixed effects models Use the lmer() function for liner mixed models, glmer() for generalized linear mixed models The Exam data The Exam data set contans exam scores of 4,059 students from 65 schools in Inner London. The variable names are as follows: Variable Description school School ID - a factor. normexam Normalized exam score. standLRT Standardised LR test score. student Student id (within school) - a factor Exam &lt;- read_rds(&quot;dataSets/Exam.rds&quot;) The null model &amp; ICC As a preliminary step it is often useful to partition the variance in the dependent variable into the various levels. This can be accomplished by running a null model (i.e., a model with a random effects grouping structure, but no fixed-effects predictors). # anatomy of lmer() function lmer(outcome ~ 1 + pred1 + pred2 + (1 | grouping_variable), data = mydata, REML = FALSE) # null model, grouping by school but not fixed effects. Norm1 &lt;-lmer(normexam ~ 1 + (1 | school), data = na.omit(Exam), REML = FALSE) summary(Norm1) The is .161/(.161 + .852) = .159 = 16% of the variance is at the school level. There is no consensus on how to calculate p-values for MLMs; hence why they are omitted from the lme4 output. But, if you really need p-values, the lmerTest package will calculate p-values for you (using the Satterthwaite approximation). Adding fixed-effects predictors Predict exam scores from student’s standardized tests scores Norm2 &lt;-lmer(normexam ~ 1 + standLRT + (1 | school), data = na.omit(Exam), REML = FALSE) summary(Norm2) Multiple degree of freedom comparisons As with lm() and glm() models, you can compare the two lmer() models using a likelihood ratio test with the anova() function. anova(Norm1, Norm2) Random slopes Add a random effect of students’ standardized test scores as well. Now in addition to estimating the distribution of intercepts across schools, we also estimate the distribution of the slope of exam on standardized test. Norm3 &lt;- lmer(normexam ~ 1 + standLRT + (1 + standLRT | school), data = na.omit(Exam), REML = FALSE) summary(Norm3) Test the significance of the random slope To test the significance of a random slope just compare models with and without the random slope term using a likelihood ratio test: anova(Norm2, Norm3) Exercise 3 Multilevel modeling Use the bh1996 dataset: ## install.packages(&quot;multilevel&quot;) data(bh1996, package=&quot;multilevel&quot;) From the data documentation: Variables are Leadership Climate (LEAD), Well-Being (WBEING), and Work Hours (HRS). The group identifier is named GRP. Create a null model predicting wellbeing (WBEING) ## Calculate the ICC for your null model ## Run a second multi-level model that adds two individual-level predictors, average number of hours worked (HRS) and leadership skills (LEAD) to the model and interpret your output. ## Now, add a random effect of average number of hours worked (HRS) to the model and interpret your output. Test the significance of this random term. ## Exercise solutions Ex 0: prototype Use the states.rds data set. states &lt;- read_rds(&quot;dataSets/states.rds&quot;) Fit a model predicting energy consumed per capita (energy) from the percentage of residents living in metropolitan areas (metro). Be sure to: Examine/plot the data before fitting the model. states_en_met &lt;- subset(states, select = c(&quot;metro&quot;, &quot;energy&quot;)) summary(states_en_met) plot(states_en_met) cor(states_en_met, use = &quot;pairwise&quot;) Print and interpret the model summary(). mod_en_met &lt;- lm(energy ~ metro, data = states) summary(mod_en_met) plot() the model to look for deviations from modeling assumptions. plot(mod_en_met) Select one or more additional predictors to add to your model and repeat steps 1-3. Is this model significantly better than the model with metro as the only predictor? states_en_met_pop_wst &lt;- subset(states, select = c(&quot;energy&quot;, &quot;metro&quot;, &quot;pop&quot;, &quot;waste&quot;)) summary(states_en_met_pop_wst) plot(states_en_met_pop_wst) cor(states_en_met_pop_wst, use = &quot;pairwise&quot;) mod_en_met_pop_waste &lt;- lm(energy ~ 1 + metro + pop + waste, data = states) summary(mod_en_met_pop_waste) anova(mod_en_met, mod_en_met_pop_waste) Ex 1: prototype Use the states data set. Add on to the regression equation that you created in exercise 1 by generating an interaction term and testing the interaction. mod_en_metro_by_waste &lt;- lm(energy ~ 1 + metro * waste, data = states) Try adding a region to the model. Are there significant differences across the four regions? mod_en_region &lt;- lm(energy ~ 1 + metro * waste + region, data = states) anova(mod_en_region) Ex 2: prototype Use the NH11 data set that we loaded earlier. Note that the data is not perfectly clean and ready to be modeled. You will need to clean up at least some of the variables before fitting the model. Use glm() to conduct a logistic regression to predict ever worked (everwrk) using age (age_p) and marital status (r_maritl). Make sure you only keep the following two levels for everwrk (1 Yes and 2 No). Hint: use the factor() function. Also, make sure to drop any r_maritl levels that do not contain observations. Hint: see ?droplevels. NH11 &lt;- mutate(NH11, everwrk = factor(everwrk, levels = c(&quot;1 Yes&quot;, &quot;2 No&quot;)), r_maritl = droplevels(r_maritl)) mod_wk_age_mar &lt;- glm(everwrk ~ 1 + age_p + r_maritl, data = NH11, family = binomial(link = &quot;logit&quot;)) summary(mod_wk_age_mar) Predict the probability of working for each level of marital status. Hint: use allEffects(). eff &lt;- allEffects(mod_wk_age_mar) as.data.frame(eff) Ex 3: prototype Use the dataset, bh1996: data(bh1996, package=&quot;multilevel&quot;) From the data documentation: Variables are Leadership Climate (LEAD), Well-Being (WBEING), and Work Hours (HRS). The group identifier is named GRP. Create a null model predicting wellbeing (WBEING). mod_grp0 &lt;- lmer(WBEING ~ 1 + (1 | GRP), data = bh1996) summary(mod_grp0) Run a second multi-level model that adds two individual-level predictors, average number of hours worked (HRS) and leadership skills (LEAD) to the model and interpret your output. mod_grp1 &lt;- lmer(WBEING ~ 1 + HRS + LEAD + (1 | GRP), data = bh1996) summary(mod_grp1) Now, add a random effect of average number of hours worked (HRS) to the model and interpret your output. Test the significance of this random term. mod_grp2 &lt;- lmer(WBEING ~ 1 + HRS + LEAD + (1 + HRS | GRP), data = bh1996) anova(mod_grp1, mod_grp2) Wrap-up Feedback These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu "],
["Rgraphics.html", "R Graphics Setup Why ggplot2? Geometric objects &amp; aesthetics Statistical transformations Scales Faceting Themes Saving plots The #1 FAQ Exercise solutions Wrap-up", " R Graphics Topics R ggplot2 package Geometric objects and aesthetics Setup basic plots Add and modify scales and legends Manipulate plot labels Change and create plot themes Setup Class Structure Informal — Ask questions at any time. Really! Collaboration is encouraged - please spend a minute introducing yourself to your neighbors! Prerequisites This is an intermediate R course: Assumes working knowledge of R Relatively fast-paced Launch an R session Start RStudio and create a new project: On Windows click the start button and search for RStudio. On Mac RStudio will be in your applications folder. In Rstudio go to File -&gt; New Project. Choose Existing Directory and browse to the workshop materials directory on your desktop. Choose File -&gt; Open File and select the file with the word “BLANK” in the name. Packages You should have already installed the tidyverse and rmarkdown packages onto your computer before the workshop — see R Installation. Now let’s load these packages into the search path of our R session. library(tidyverse) library(rmarkdown) The ggplot2 package is contained within tidyverse, but we also want to install two additional packages, scales and ggrepel, which provide additional functionality. # install.packages(&quot;scales&quot;) library(scales) # install.packages(&quot;ggrepel&quot;) library(ggrepel) Goals We will learn about the grammar of graphics — a system for understanding the building blocks of a graph — using the ggplot2 package. In particular, we’ll learn about: Basic plots, aesthetic mapping and inheritance Tailoring statistical transformations to particular plots Modifying scales to change axes and add labels Faceting to create many small plots Changing plot themes Why ggplot2? ggplot2 is a package within in the tidyverse suite of packages. Advantages of ggplot2 include: consistent underlying grammar of graphics (Wilkinson, 2005) very flexible — plot specification at a high level of abstraction theme system for polishing plot appearance many users, active mailing list That said, there are some things you cannot (or should not) do with ggplot2: 3-dimensional graphics (see the rgl package) Graph-theory type graphs (nodes/edges layout; see the igraph package) Interactive graphics (see the ggvis package) What is the Grammar Of Graphics? The basic idea: independently specify plot building blocks and combine them to create just about any kind of graphical display you want. Building blocks of a graph include the following (bold denotes essential elements): data aesthetic mapping geometric object statistical transformations scales coordinate system position adjustments faceting themes By the end of this workshop, you should understand what these building blocks do and how to use them to create the following plot: ggplot2 VS base graphics Compared to base graphics, ggplot2 is more verbose for simple / canned graphics is less verbose for complex / custom graphics does not have methods (data should always be in a data.frame) has sensible defaults for generating legends Geometric objects &amp; aesthetics Aesthetic mapping In ggplot land aesthetic means “something you can see”. Examples include: position (i.e., on the x and y axes) color (“outside” color) fill (“inside” color) shape (of points) linetype size Each type of geom accepts only a subset of all aesthetics; refer to the geom help pages to see what mappings each geom accepts. Aesthetic mappings are set with the aes() function. Geometric objects (geom) Geometric objects are the actual marks we put on a plot. Examples include: points (geom_point(), for scatter plots, dot plots, etc.) lines (geom_line(), for time series, trend lines, etc.) boxplot (geom_boxplot(), for boxplots!) A plot must have at least one geom; there is no upper limit. You can add a geom to a plot using the + operator. Each geom_ has a particular set of aesthetic mappings associated with it. Some examples are provided below, with required aesthetics in bold and optional aesthetics in plain text: geom_ Usage Aesthetics geom_point() Scatter plot x,y,alpha,color,fill,group,shape,size,stroke geom_line() Line plot x,y,alpha,color,linetype,size geom_bar() Bar chart x,y,alpha,color,fill,group,linetype,size geom_boxplot() Boxplot x,lower,upper,middle,ymin,ymax,alpha,color,fill geom_density() Density plot x,y,alpha,color,fill,group,linetype,size,weight geom_smooth() Conditional means x,y,alpha,color,fill,group,linetype,size,weight geom_label() Text x,y,label,alpha,angle,color,family,fontface,size You can get a list of all available geometric objects and their associated aesthetics at https://ggplot2.tidyverse.org/reference/. Or, simply type geom_&lt;tab&gt; in any good R IDE (such as Rstudio or ESS) to see a list of functions starting with geom_. Points (scatterplot) Now that we know about geometric objects and aesthetic mapping, we can make a ggplot(). geom_point() requires mappings for x and y, all others are optional. Example data: housing prices Let’s look at housing prices. housing &lt;- read_csv(&quot;dataSets/landdata-states.csv&quot;) head(housing[1:5]) # create a subset for 1st quarter 2001 hp2001Q1 &lt;- filter(housing, Date == 2001.25) Step 1: create a blank canvas by specifying data: ggplot(data = hp2001Q1) Step 2: specify aesthetic mappings (how you want to map variables to visual aspects): # here we map &quot;Land_Value&quot; and &quot;Structure_Cost&quot; to the x- and y-axes. ggplot(data = hp2001Q1, mapping = aes(x = Land_Value, y = Structure_Cost)) Step 3: add new layers of geometric objects that will show up on the plot: # here we use geom_point() to add a layer with point (dot) elements # as the geometric shapes to represent the data. ggplot(data = hp2001Q1, mapping = aes(x = Land_Value, y = Structure_Cost)) + geom_point() Lines (prediction line) A plot constructed with ggplot() can have more than one geom. In that case the mappings established in the ggplot() call are plot defaults that can be added to or overridden — this is referred to as aesthetic inheritance. Our plot could use a regression line: # get predicted values from a linear regression hp2001Q1$pred_SC &lt;- lm(Structure_Cost ~ log(Land_Value), data = hp2001Q1) %&gt;% predict() p1 &lt;- ggplot(hp2001Q1, aes(x = log(Land_Value), y = Structure_Cost)) p1 + geom_point(aes(color = Home_Value)) + # values for x and y are inherited from the ggplot() call above geom_line(aes(y = pred_SC)) # add predicted values to the plot overriding the y values from the ggplot() call above Smoothers Not all geometric objects are simple shapes; the smooth geom includes a line and a ribbon. p1 + geom_point(aes(color = Home_Value)) + geom_smooth() Text (label points) Each geom accepts a particular set of mappings; for example geom_text() accepts a label mapping. p1 + geom_text(aes(label=State), size = 3) p1 + geom_point() + geom_text_repel(aes(label=State), size = 3) Aesthetic mapping VS assignment Variables are mapped to aesthetics within the aes() function p1 + geom_point(aes(size = Home_Value)) Constants are fixed to aesthetics outside the aes() call p1 + geom_point(size = 2) This sometimes leads to confusion, as in this example: p1 + geom_point(aes(size = 2),# incorrect! 2 is not a variable color=&quot;red&quot;) # this is fine -- all points red Mapping variables to other aesthetics Other aesthetics are mapped in the same way as x and y in the previous example. p1 + geom_point(aes(color = Home_Value, shape = region)) Exercise 0 The data for the exercises is available in the dataSets/EconomistData.csv file. Read it in with dat &lt;- read_csv(&quot;dataSets/EconomistData.csv&quot;) Original sources for these data are http://www.transparency.org/content/download/64476/1031428 http://hdrstats.undp.org/en/indicators/display_cf_xls_indicator.cfm?indicator_id=103106&amp;lang=en These data consist of Human Development Index and Corruption Perception Index scores for several countries. Create a scatter plot with CPI on the x axis and HDI on the y axis. ## Color the points in the previous plot blue. ## Map the color of the the points to Region. ## Keeping color mapped to Region, make the points bigger by setting size to 2 ## Keeping color mapped to Region, map the size of the points to HDI_Rank ## Statistical transformations Why transform data? Some plot types (such as scatterplots) do not require transformations; each point is plotted at x and y coordinates equal to the original value. Other plots, such as boxplots, histograms, prediction lines etc. require statistical transformations: for a boxplot the y values must be transformed to the median and 1.5(IQR) for a smoother the y values must be transformed into predicted values Each geom has a default statistic, but these can be changed. For example, the default statistic for geom_histogram() is stat_bin(): args(geom_histogram) args(stat_bin) Here is a list of geoms and their default statistics https://ggplot2.tidyverse.org/reference/ Setting arguments Arguments to stat_ functions can be passed through geom_ functions. This can be slightly annoying because in order to change it you have to first determine which stat the geom uses, then determine the arguments to that stat. For example, here is the default histogram of Home_Value: p2 &lt;- ggplot(housing, aes(x = Home_Value)) p2 + geom_histogram() can change it by passing the binwidth argument to the stat_bin() function: p2 + geom_histogram(stat = &quot;bin&quot;, binwidth=4000) Changing the transformation Sometimes the default statistical transformation is not what you need. This is often the case with pre-summarized data: housing_sum &lt;- housing %&gt;% group_by(State) %&gt;% summarize(Home_Value_Mean = mean(Home_Value)) %&gt;% ungroup() head(housing_sum) ggplot(housing_sum, aes(x=State, y=Home_Value_Mean)) + geom_bar() What is the problem with the previous plot? Basically we take binned and summarized data and ask ggplot to bin and summarize it again (remember, geom_bar() defaults to stat = stat_count; obviously this will not work. We can fix it by telling geom_bar() to use a different statistical transformation function: ggplot(housing_sum, aes(x=State, y=Home_Value_Mean)) + geom_bar(stat=&quot;identity&quot;) Exercise 1 Re-create a scatter plot with CPI on the x axis and HDI on the y axis (as you did in the previous exercise). ## Overlay a smoothing line on top of the scatter plot using geom_smooth(). ## Overlay a smoothing line on top of the scatter plot using geom_smooth(), but use a linear model for the predictions. Hint: see ?stat_smooth. ## Overlay a smoothing line on top of the scatter plot using the default loess method for geom_smooth(), but make it less smooth. Hint: see ?stat_smooth and the span argument. ## BONUS: Overlay a loess (method = \"loess\") smoothing line on top of the scatter plot using geom_line(). Hint: change the statistical transformation. ## Scales Controlling aesthetic mapping Aesthetic mapping (i.e., with aes()) only says that a variable should be mapped to an aesthetic. It doesn’t say how that should happen. For example, when mapping a variable to shape with aes(shape = x) you don’t say what shapes should be used. Similarly, aes(color = y) doesn’t say what colors should be used. Also, aes(size = z) doesn’t say what sizes should be used. Describing what colors/shapes/sizes etc. to use is done by modifying the corresponding scale. In ggplot2 scales include position color and fill size shape line type Scales are modified with a series of functions using a scale_&lt;aesthetic&gt;_&lt;type&gt; naming scheme. Try typing scale_&lt;tab&gt; to see a list of scale modification functions. Common scale arguments The following arguments are common to most scales in ggplot2: name: the axis or legend title limits: the minimum and maximum of the scale breaks: the points along the scale where labels should appear labels: the labels that appear at each break Specific scale functions may have additional arguments; for example, the scale_color_continuous() function has arguments low and high for setting the colors at the low and high end of the scale. Scale modification examples Start by constructing a dotplot showing the distribution of home values by Date and State. p4 &lt;- ggplot(housing, aes(x = State, y = Home_Price_Index)) + geom_point(aes(color = Date), alpha = 0.5, size = 1.5, position = position_jitter(width = 0.25, height = 0)) Now modify the breaks for the color scales p4 + scale_color_continuous(name=&quot;&quot;, breaks = c(1976, 1994, 2013), labels = c(&quot;&#39;76&quot;, &quot;&#39;94&quot;, &quot;&#39;13&quot;)) Next change the low and high values to blue and red: p4 + scale_color_continuous(name=&quot;&quot;, breaks = c(1976, 1994, 2013), labels = c(&quot;&#39;76&quot;, &quot;&#39;94&quot;, &quot;&#39;13&quot;), low = &quot;blue&quot;, high = &quot;red&quot;) Using different color scales ggplot2 has a wide variety of color scales; here is an example using scale_color_gradient2() to interpolate between three different colors. p4 + scale_color_gradient2(name=&quot;&quot;, breaks = c(1976, 1994, 2013), labels = c(&quot;&#39;76&quot;, &quot;&#39;94&quot;, &quot;&#39;13&quot;), low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;gray60&quot;, midpoint = 1994) Available scales Partial combination matrix of available scales scale_ Types Examples scale_color_ identity scale_fill_continuous() scale_fill_ manual scale_color_discrete() scale_size_ continuous scale_size_manual() discrete scale_size_discrete() scale_shape_ discrete scale_shape_discrete() scale_linetype_ identity scale_shape_manual() manual scale_linetype_discrete() scale_x_ continuous scale_x_continuous() scale_y_ discrete scale_y_discrete() reverse scale_x_log() log scale_y_reverse() date scale_x_date() datetime scale_y_datetime() Note that in RStudio you can type scale_ followed by tab to get the whole list of available scales. For a complete list of available scales see https://ggplot2.tidyverse.org/reference/ Exercise 2 Create a scatter plot with CPI on the x axis and HDI on the y axis. Color the points to indicate Region. ## Modify the x, y, and color scales so that they have more easily-understood names (e.g., spell out “Human development Index” instead of HDI). Hint: see ?scale_x_discrete. ## Modify the color scale to use specific values of your choosing. Hint: see ?scale_color_manual. NOTE: you can specify color by name (e.g., “blue”) or by “Hex value” — see https://www.color-hex.com/. ## Faceting What is faceting? Faceting is ggplot2 parlance for small multiples The idea is to create separate graphs for subsets of data ggplot2 offers two functions for creating small multiples: facet_wrap(): define subsets as the levels of a single grouping variable facet_grid(): define subsets as the crossing of two grouping variables Facilitates comparison among plots, not just of geoms within a plot What is the trend in housing prices in each state? Start by using a technique we already know; map State to color: p5 &lt;- ggplot(housing, aes(x = Date, y = Home_Value)) p5 + geom_line(aes(color = State)) There are two problems here; there are too many states to distinguish each one by color, and the lines obscure one another. Faceting to the rescue We can remedy the deficiencies of the previous plot by faceting by State rather than mapping State to color. p5 &lt;- p5 + geom_line() + facet_wrap(~ State, ncol = 10) p5 Themes What are themes? The ggplot2 theme system handles non-data plot elements such as: Axis label properties (e.g., font, size, color, etc.) Plot background Facet label background Legend appearance Built-in themes include: theme_gray() (default) theme_bw() theme_classic() p5 + theme_linedraw() p5 + theme_light() You can see a list of available built-in themes here https://ggplot2.tidyverse.org/reference/ Overriding theme defaults Specific theme elements can be overridden using theme(). For example: # theme(thing_to_modify = modifying_function(arg1, arg2)) p5 + theme_minimal() + theme(text = element_text(color = &quot;turquoise&quot;)) All theme options are documented in ?theme. We can also see the existing default values using: theme_get() Creating &amp; saving new themes You can create new themes, as in the following example: theme_new &lt;- theme_bw() + theme(plot.background = element_rect(size = 1, color = &quot;blue&quot;, fill = &quot;black&quot;), text = element_text(size = 12, color = &quot;ivory&quot;), axis.text.y = element_text(colour = &quot;purple&quot;), axis.text.x = element_text(colour = &quot;red&quot;), panel.background = element_rect(fill = &quot;pink&quot;), strip.background = element_rect(fill = muted(&quot;orange&quot;))) p5 + theme_new Saving plots We can save a plot to either a vector (e.g., pdf, eps, ps, svg) or raster (e.g., jpg, png, tiff, bmp, wmf) graphics file using the ggsave() function: ggsave(filename = &quot;myplot.pdf&quot;, plot = p5, device = &quot;pdf&quot;, height = 6, width = 6, units = &quot;in&quot;) The #1 FAQ Map aesthetic to different columns The most frequently asked question goes something like this: I have two variables in my data.frame, and I’d like to plot them as separate points, with different color depending on which variable it is. How do I do that? Wrong Fixing, rather than mapping, the color aesthetic: Produces verbose code when using many colors Results in no legend being produced Means you cannot change color scales housing_byyear &lt;- housing %&gt;% group_by(Date) %&gt;% summarize(Home_Value_Mean = mean(Home_Value), Land_Value_Mean = mean(Land_Value)) %&gt;% ungroup() ggplot(housing_byyear, aes(x=Date)) + geom_line(aes(y=Home_Value_Mean), color=&quot;red&quot;) + geom_line(aes(y=Land_Value_Mean), color=&quot;blue&quot;) Right To avoid these pitfalls, we need to map our data to the color aesthetic. We can do this by reshaping our data from wide format to long format. Here is the logic behind this process: Here’s the code that implements this transformation: home_land_byyear &lt;- gather(housing_byyear, value = &quot;value&quot;, key = &quot;type&quot;, Home_Value_Mean, Land_Value_Mean) ggplot(home_land_byyear, aes(x=Date, y=value, color=type)) + geom_line() Exercise 3 For this exercise, we’re going to use the built-in midwest dataset: data(&quot;midwest&quot;, package = &quot;ggplot2&quot;) head(midwest) Create a scatter plot with area on the x axis and the log of poptotal on the y axis. ## Within the geom_point() call, map color to state, map size to the log of popdensity, and fix transparency (alpha) to 0.3. ## Add a smoother and turn off plotting the confidence interval. Hint: see the se argument to geom_smooth(). ## Facet the plot by state. Set the scales argument to facet_wrap() to allow separate ranges for the x-axis. ## Change the default color scale to use the discrete RColorBrewer palette called Set1. Hint: see ?scale_color_brewer. ## BONUS: Change the default theme to theme_bw() and modify it so that the axis text and facet label background are blue. Hint: see ?theme and especially axis.text and strip.background. ## Exercise solutions Ex 0: prototype Create a scatter plot with CPI on the x axis and HDI on the y axis. ggplot(dat, aes(x = CPI, y = HDI)) + geom_point() Color the points in the previous plot blue. ggplot(dat, aes(x = CPI, y = HDI)) + geom_point(color = &quot;blue&quot;) Map the color of the the points to Region. ggplot(dat, aes(x = CPI, y = HDI)) + geom_point(aes(color = Region)) Keeping color mapped to Region, make the points bigger by setting size to 2. ggplot(dat, aes(x = CPI, y = HDI)) + geom_point(aes(color = Region), size = 2) Keeping color mapped to Region, map the size of the points to HDI_Rank. ggplot(dat, aes(x = CPI, y = HDI)) + geom_point(aes(color = Region, size = HDI_Rank)) Ex 1: prototype Re-create a scatter plot with CPI on the x axis and HDI on the y axis (as you did in the previous exercise). ggplot(dat, aes(x = CPI, y = HDI)) + geom_point() Overlay a smoothing line on top of the scatter plot using geom_smooth(). ggplot(dat, aes(x = CPI, y = HDI)) + geom_point() + geom_smooth() Overlay a smoothing line on top of the scatter plot using geom_smooth(), but use a linear model for the predictions. Hint: see ?stat_smooth. ggplot(dat, aes(x = CPI, y = HDI)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Overlay a smoothing line on top of the scatter plot using the default loess method for geom_smooth(), but make it less smooth. Hint: see ?loess. ggplot(dat, aes(x = CPI, y = HDI)) + geom_point() + geom_smooth(span = .4) BONUS: Overlay a loess (method = \"loess\") smoothing line on top of the scatter plot using geom_line(). Hint: change the statistical transformation. ggplot(dat, aes(x = CPI, y = HDI)) + geom_point() + geom_line(stat = &quot;smooth&quot;, method = &quot;loess&quot;) Ex 2: prototype Create a scatter plot with CPI on the x axis and HDI on the y axis. Color the points to indicate Region. ggplot(dat, aes(x = CPI, y = HDI, color = Region)) + geom_point() Modify the x, y, and color scales so that they have more easily-understood names (e.g., spell out “Human development Index” instead of HDI). ggplot(dat, aes(x = CPI, y = HDI, color = Region)) + geom_point() + scale_x_continuous(name = &quot;Corruption Perception Index&quot;) + scale_y_continuous(name = &quot;Human Development Index&quot;) + scale_color_discrete(name = &quot;Region of the world&quot;) Modify the color scale to use specific values of your choosing. Hint: see ?scale_color_manual. NOTE: you can specify color by name (e.g., “blue”) or by “Hex value” — see https://www.color-hex.com/. ggplot(dat, aes(x = CPI, y = HDI, color = Region)) + geom_point() + scale_x_continuous(name = &quot;Corruption Perception Index&quot;) + scale_y_continuous(name = &quot;Human Development Index&quot;) + scale_color_manual(name = &quot;Region of the world&quot;, values = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;orange&quot;, &quot;grey&quot;, &quot;brown&quot;)) Ex 3: prototype Create a scatter plot with area on the x axis and the log of poptotal on the y axis. p6 &lt;- ggplot(midwest, aes(x=area, y=log(poptotal))) p6 + geom_point() Within the geom_point() call, map color to state, map size to the log of popdensity, and fix transparency (alpha) to 0.3. p6 &lt;- p6 + geom_point(aes(color=state, size=log(popdensity)), alpha = 0.3) Add a smoother and turn off plotting the confidence interval. Hint: see the se argument to geom_smooth(). p6 &lt;- p6 + geom_smooth(method=&quot;loess&quot;, se=FALSE) Facet the plot by state. Set the scales argument to facet_wrap() to allow separate ranges for the x-axis. p6 &lt;- p6 + facet_wrap(~ state, scales = &quot;free_x&quot;) Change the default color scale to use the discrete RColorBrewer palette called Set1. Hint: see ?scale_color_brewer. p6 &lt;- p6 + scale_color_brewer(palette = &quot;Set1&quot;) BONUS: Change the default theme to theme_bw() and modify it so that the axis text and facet label background are blue. Hint: see ?theme and especially axis.text and strip.background. p6 &lt;- p6 + theme_bw() + theme(axis.title = element_text(color = &quot;blue&quot;, face = &quot;bold&quot;), strip.background = element_rect(fill = &quot;yellow&quot;)) Here’s the complete code for the Exercise 3 plot: p6 &lt;- ggplot(midwest, aes(x=area, y=log(poptotal))) + geom_point(aes(color=state, size=log(popdensity)), alpha = 0.3) + geom_smooth(method=&quot;loess&quot;, se=FALSE) + facet_wrap(~ state, scales = &quot;free_x&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) + theme_bw() + theme(axis.title = element_text(color = &quot;blue&quot;, face = &quot;bold&quot;), strip.background = element_rect(fill = &quot;yellow&quot;)) Wrap-up Feedback These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu ggplot2 Reference: https://ggplot2.tidyverse.org/reference/ Cheatsheets: https://rstudio.com/wp-content/uploads/2019/01/Cheatsheets_2019.pdf Examples: http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html Tutorial: https://uc-r.github.io/ggplot_intro Mailing list: http://groups.google.com/group/ggplot2 Wiki: https://github.com/hadley/ggplot2/wiki Website: http://had.co.nz/ggplot2/ StackOverflow: http://stackoverflow.com/questions/tagged/ggplot "],
["RDataWrangling.html", "R Data Wrangling Setup Working with Excel worksheets Reading Excel data files Data cleanup Data organization &amp; storage Exercise solutions Wrap-up", " R Data Wrangling Topics Loading Excel worksheets Iterating over files Writing your own functions Filtering with regular expressions (regex) Reshaping data Setup Class Structure Informal — Ask questions at any time. Really! Collaboration is encouraged - please spend a minute introducing yourself to your neighbors! Prerequisites This is an intermediate / advanced R course: Assumes intermediate knowledge of R Relatively fast-paced Launch an R session Start RStudio and create a new project: On Windows click the start button and search for RStudio. On Mac RStudio will be in your applications folder. In Rstudio go to File -&gt; New Project. Choose Existing Directory and browse to the workshop materials directory on your desktop. Choose File -&gt; Open File and select the file with the word “BLANK” in the name. Packages You should have already installed the tidyverse and rmarkdown packages onto your computer before the workshop — see R Installation. Now let’s load these packages into the search path of our R session. library(tidyverse) library(rmarkdown) library(readxl) # installed with tidyverse, but not loaded into R session Workshop Outline Example data The UK Office for National Statistics provides yearly data on the most popular boys names going back to 1996. The data is provided separately for boys and girls and is stored in Excel spreadsheets. Overall Goal Our mission is to extract and graph the top 100 boys names in England and Wales for every year since 1996. Exercise 0: Problems with the data There are several things that make our goal challenging. Let’s take a look at the data: Locate the files named 1996boys_tcm77-254026.xlsx and 2015boysnamesfinal.xlsx and open them separately in a spreadsheet program. (If you don’t have a spreadsheet program installed on your computer you can download one from https://www.libreoffice.org/download/download/). What issues can you identify that might make working with these data difficult? In what ways is the format different between the two files? Steps to accomplish this goal: Explore example data to highlight problems (see Exercise 0) Reading data from multiple Excel worksheets into R data frames list Excel file names in a character vector read Excel sheetnames into a list of character vectors read Excel data for “Table 1” only into a list of data frames Clean up data within each R data frame sort and merge columns within each data frame inside the list drop missing values from each data frame reshape format from wide to long Organize the data into one large data frame and store it create a year column within each data frame in the list append all the data frames in the list into one large data frame NOTE: please make sure you close the Excel files before continuing with the workshop, otherwise you may encounter issues with file paths when reading the data into R. Working with Excel worksheets As you can see, the data is in quite a messy state. Note that this is not a contrived example; this is exactly the way the data came to us from the UK government website! Let’s start cleaning and organizing it. Each Excel file contains a worksheet with the boy names data we want. Each file also contains additional supplemental worksheets that we are not currently interested in. As noted above, the worksheet of interest differs from year to year, but always has “Table 1” in the sheet name. The first step is to get a character vector of file names. boy_file_names &lt;- list.files(&quot;dataSets/boys&quot;, full.names = TRUE) Now that we’ve told R the names of the data files, we can start working with them. For example, the first file is boy_file_names[1] and we can use the excel_sheets() function from the readxl package within tidyverse to list the worksheet names from this file. excel_sheets(boy_file_names[1]) Iterating with map() Now that we know how to retrieve the names of the worksheets in an Excel file, we could start writing code to extract the sheet names from each file, e.g., excel_sheets(boy_file_names[1]) excel_sheets(boy_file_names[2]) ## ... excel_sheets(boy_file_names[20]) This is not a terrible idea for a small number of files, but it is more convenient to let R do the iteration for us. We could use a for loop, or sapply(), but the map() family of functions from the purrr package within tidyverse gives us a more consistent alternative, so we’ll use that. # map(object to iterate over, function that does task within each iteration) map(boy_file_names, excel_sheets) Filtering strings using regex To extract the correct worksheet names we need a way to extract strings containing “Table 1”. Base R provides some string manipulation capabilities (see ?regex, ?sub and ?grep), but we will use the stringr package within tidyverse because it is more user-friendly. stringr provides functions to: detect locate extract match replace combine split strings. Here we want to detect the pattern “Table 1”, and only return elements with this pattern. We can do that using the str_subset() function: The first argument to str_subset() is character vector we want to search in. The second argument is a regular expression matching the pattern we want to retain. If you are not familiar with regular expressions (regex), http://www.regexr.com/ is a good place to start. Regex is essentially just a programmatic way of doing operations like “find” or “find and replace” in MS Word or Excel. Now that we know how to filter character vectors using str_subset() we can identify the correct sheet in a particular Excel file. For example, # str_subset(character_vector, regex_pattern) # nesting functions str_subset(excel_sheets(boy_file_names[1]), pattern = &quot;Table 1&quot;) # piping functions excel_sheets(boy_file_names[1]) %&gt;% str_subset(pattern = &quot;Table 1&quot;) Writing your own functions The next step is to retrieve worksheet names and subset them. The map* functions are useful when you want to apply a function to a vector of inputs and obtain the return values for each input. This is very convenient when a function already exists that does exactly what you want. In the examples above we mapped the excel_sheets() function to the elements of a character vector containing file names. However, there is no function that both: Retrieves worksheet names, and Subsets the names So, we will have to write one. Fortunately, writing functions in R is easy. Functions require 3 elements: A name One or more arguments A body containing computations Anatomy of a function: function_name &lt;- function(arg1, arg2, ....) { body of function # where stuff happens return( results ) } Simple examples: myfun &lt;- function(x) { x^2 } myfun(1:10) myfun2 &lt;- function(x, y) { z &lt;- x^2 + y return(z) } myfun2(x=1:10, y=42) Examples using the Excel data: get_data_sheet_name &lt;- function(file, term){ excel_sheets(file) %&gt;% str_subset(pattern = term) } # the goal is generalization get_data_sheet_name(boy_file_names[1], term = &quot;Table 1&quot;) get_data_sheet_name(boy_file_names[1], term = &quot;Table 2&quot;) Now we can map this new function over our vector of file names. # map(object to iterate over, # function that does task within each iteration, # arguments to previous function) map(boy_file_names, # list object get_data_sheet_name, # function term = &quot;Table 1&quot;) # argument to previous function Reading Excel data files Now that we know the correct worksheet from each file, we can actually read those data into R. We can do that using the read_excel() function. We’ll start by reading the data from the first file, just to check that it works. Recall that the actual data starts on row 7, so we want to skip the first 6 rows. We can use the glimpse() function from the dplyr package within tidyverse to view the output. tmp &lt;- read_excel( path = boy_file_names[1], sheet = get_data_sheet_name(boy_file_names[1], term = &quot;Table 1&quot;), skip = 6 ) glimpse(tmp) Note that R has added a suffix to each column name ...1, ...2, ...3, etc. because duplicate names are not allowed, so the suffix serves to disambiguate. The trailing number represents the index of the column. Exercise 1 Write a function called read_boys_names that takes a file name as an argument and reads the worksheet containing “Table 1” from that file. Don’t forget to skip the first 6 rows. ## Test your function by using it to read one of the boys names Excel files. ## Use the map() function to create a list of data frames called boysNames from all the Excel files, using the function you wrote in step 1. ## Data cleanup Now that we’ve read in the data, we can see that there are some problems we need to fix. Specifically, we need to: fix column names get rid of blank row and the top and the notes at the bottom get rid of extraneous “changes in rank” columns if they exist transform the side-by-side tables layout to a single table. # Rank 1:50 --- Names / Counts are in columns 2 and 3 # Rank 51:100 --- Names / Counts are in columns 6 and 7 glimpse(boysNames[[1]]) # Rank 1:50 --- Names / Counts are in columns 2 and 3 # Rank 51:100 --- Names / Counts are in columns 7 and 8 glimpse(boysNames[[10]]) # Rank 1:50 --- Names / Counts are in columns 2 and 3 # Rank 51:100 --- Names / Counts are in columns 8 and 9 glimpse(boysNames[[20]]) In short, we want to go from this: to this: There are many ways to do this kind of data manipulation in R. We’re going to use the dplyr and tidyr packages from within tidyverse to make our lives easier. Selecting columns Next we want to retain just the Name...2, Name...6, Count...3 and Count...7 columns. We can do that using the select() function: boysNames[[1]] boysNames[[1]] &lt;- select(boysNames[[1]], Name...2, Name...6, Count...3, Count...7) boysNames[[1]] Data types and structures We’ve now encountered several different data types and data structures. Let’s take a step back and survey the options available in R. Data structures: In R, the most foundational data structure is the vector. Vectors are containers that can hold a collection of values. Vectors come in two basic forms: atomic: only hold elements of the same type; they are homogeneous The c() function can be used to create atomic vectors list: can hold elements of different types; they are heterogeneous The list() function can be used to create list vectors NULL is closely related to vectors and often serves the role of a zero length vector. From these two basic forms, the following six structures are derived: Type Elements Description atomic vector homogeneous contains elements of the same type, one of: character, integer, double, logical array homogeneous an atomic vector with attributes giving dimensions (1, 2, or &gt;2) matrix homogeneous an array with 2 dimensions factor homogeneous an atomic integer vector containing only predefined values, storing categorical data list heterogeneous a container whose elements can encompass any mixture of data types data.frame heterogeneous a rectangular list with elements (columns) containing atomic vectors of equal length Each vector can have attributes, which are a named list of metadata that can include the vector’s dimensions and its class. The latter is a property assigned to an object that determines how generic functions operate with it, and thus which methods are available for it. The class of an object can be queried using the class() function. You can learn more details about R data structures here: https://adv-r.hadley.nz/vectors-chap.html Data types: There are four primary types of atomic vectors. Collectively, integer and double vectors are known as numeric vectors. You can query the type of an object using the typeof() function. Type Description character “a”, “swc” integer 2L (the L tells R to store this as an integer) double (floating point) 2, 15.5 logical TRUE, FALSE Coercion: If heterogeneous elements are stored in an atomic vector, R will coerce the vector to the simplest type required to store all the information. The order of coercion is roughly: logical -&gt; integer -&gt; double -&gt; character -&gt; list. For example: x &lt;- c(1.5, 2.7, 3.9) typeof(x) y &lt;- c(1.5, 2.7, 3.9, &quot;a&quot;) typeof(y) List indexing Now that we know about data structures more generally, let’s focus on the list structure we created for boysNames. Why are we using double brackets [[ to index this list object, instead of the single brackets [ we used to index atomic vectors? # various data structures numbers &lt;- 1:10 letters &lt;- LETTERS[1:4] dat &lt;- head(mtcars) x &lt;- 237L # combine in a list mylist &lt;- list(numbers, letters, dat, x) # indexing the list mylist[2] class(mylist[2]) # a list mylist[[2]] class(mylist[[2]]) # a character vector Dropping missing values Next we want to remove blank rows and rows used for notes. An easy way to do that is to use drop_na() from the tidyr package within tidyverse to remove rows with missing values. boysNames[[1]] boysNames[[1]] &lt;- boysNames[[1]] %&gt;% drop_na() boysNames[[1]] Exercise 2 Write a function called namecount that takes a data frame as an argument and returns a modified version, which keeps only columns that include the strings Name and Count in the column names. HINT: see the ?matches function. ## Test your function on the first data frame in the list of boys names data. ## Use the map() function to each data frame in the list of boys names data and save it to the list called boysNames. ## Reshaping from wide to long Our final task is to re-arrange the data so that it is all in a single table instead of in two side-by-side tables. For many similar tasks the gather() function in the tidyr package is useful, but in this case we will be better off using a combination of select() and bind_rows(). Here’s the logic behind this step: Here’s the code that implements the transformation: boysNames[[1]] first_columns &lt;- select(boysNames[[1]], Name = Name...2, Count = Count...3) second_columns &lt;- select(boysNames[[1]], Name = Name...6, Count = Count...7) bind_rows(first_columns, second_columns) Exercise 3 Cleanup all the data In the previous examples we learned how to drop empty rows with drop_na(), select only relevant columns with select(), and re-arrange our data with select() and bind_rows(). In each case we applied the changes only to the first element of our boysNames list. NOTE: some Excel files include extra blank columns between the first and second set of Name and Count columns, resulting in different numeric suffixes for the second set of columns. You will need to use a regular expression to match each of these different column names. HINT: see the ?matches function. Create a new function called cleanupNamesData that: # 1) subsets data to include only those columns that include the term `Name` and `Count` and apply listwise deletion # 2) subset two separate data frames, with first and second set of `Name` and `Count` columns # 3) append the two datasets Your task now is to use the map() function to apply each of these transformations to all the elements in boysNames. ## Data organization &amp; storage Now that we have the data cleaned up and augmented, we can turn our attention to organizing and storing the data. A list of data frames Right now we have a list of data frames; one for each year. This is not a bad way to go. It has the advantage of making it easy to work with individual years; it has the disadvantage of making it more difficult to examine questions that require data from multiple years. To make the arrangement of the data clearer it helps to name each element of the list with the year it corresponds to. head(boysNames) %&gt;% glimpse() head(boy_file_names) # use regex to extract years from file names Years &lt;- str_extract(boy_file_names, pattern = &quot;[0-9]{4}&quot;) Years names(boysNames) # returns NULL - no names in the list # assign years to list names names(boysNames) &lt;- Years names(boysNames) # returns the years as list names head(boysNames) %&gt;% glimpse() One big data frame While storing the data in separate data frames by year makes some sense, many operations will be easier if the data is simply stored in one big data frame. We’ve already seen how to turn a list of data frames into a single data.frame using bind_rows(), but there is a problem; The year information is stored in the names of the list elements, and so flattening the data.frames into one will result in losing the year information! Fortunately it is not too much trouble to add the year information to each data frame before flattening. # apply name of the list element (.y) as a new column in the data.frame (.x) boysNames &lt;- imap(boysNames, ~ mutate(.x, Year = as.integer(.y))) boysNames[1] Exercise 4 Make one big data.frame Turn the list of boys names data frames into a single data frame. HINT: see ?bind_rows. ## Create a new directory called all within dataSets and write the data to a .csv file. HINT: see the ?dir.create and ?write_csv functions. ## What were the five most popular names in 2013? ## How has the popularity of the name “ANDREW” changed over time? ## Exercise solutions Ex 0: prototype Locate the files named `1996boys_tcm77-254026.xlsx` and `2015boysnamesfinal.xlsx` and open them separately in a spreadsheet program. (If you don&#39;t have a spreadsheet program installed on your computer you can download one from https://www.libreoffice.org/download/download/). What issues can you identify that might make working with these data difficult? In what ways is the format different between the two files? Multiple Excel sheets in each file, each with a different name, but each file contains a Table 1. The data does not start on row one. Headers are on row 7, followed by a blank line, followed by the actual data. The data is stored in an inconvenient way, with ranks 1-50 in the first set of columns and ranks 51-100 in a second set of columns. The second worksheet 2015boysnamesfinal.xlsx contains extra columns between the data of interest, resulting in the second set of columns (ranks 51-100) being placed in a different position. The year from which the data comes is only reported in the Excel file name, not within the data itself. There are notes below the data. These differences will make it more difficult to automate re-arranging the data since we have to write code that can handle different input formats. Ex 1: prototype Write a function that takes a file name as an argument and reads the worksheet containing “Table 1” from that file. read_boys_names &lt;- function(file, sheet_name) { read_excel( path = file, sheet = get_data_sheet_name(file, term = sheet_name), skip = 6 ) } Test your function by using it to read one of the boys names Excel files. read_boys_names(boy_file_names[1], sheet_name = &quot;Table 1&quot;) %&gt;% glimpse() Use the map() function to read data from all the Excel files, using the function you wrote in step 1. boysNames &lt;- map(boy_file_names, read_boys_names, sheet_name = &quot;Table 1&quot;) Ex 2: prototype Write a function that takes a data frame as an argument and returns a modified version, which keeps only columns that include the strings Name and Count in the column names. HINT: see the ?matches function. namecount &lt;- function(data) { select(data, matches(&quot;Name|Count&quot;)) } Test your function on the first data frame in the list of boys names data. namecount(boysNames[[1]]) Use the map() function to each data frame in the list of boys names data. boysNames &lt;- map(boysNames, namecount) Ex 3: prototype Create a new function called cleanupNamesData that: cleanupNamesData &lt;- function(file){ # subset data to include only those columns that include the term `Name` and `Count` subsetted_file &lt;- file %&gt;% select(matches(&quot;Name|Count&quot;)) %&gt;% drop_na() # subset two separate data frames, with first and second set of `Name` and `Count` columns first_columns &lt;- select(subsetted_file, Name = Name...2, Count = Count...3) second_columns &lt;- select(subsetted_file, Name = matches(&quot;Name...6|Name...7|Name...8&quot;), Count = matches(&quot;Count...7|Count...8|Count...9&quot;)) # append the two datasets bind_rows(first_columns, second_columns) } ## test it out on the second data frame in the list boysNames[[2]] %&gt;% glimpse() # before cleanup boysNames[[2]] %&gt;% cleanupNamesData() %&gt;% glimpse() # after cleanup Your task now is to use the map() function to apply each of these transformations to all the elements in boysNames. boysNames &lt;- map(boysNames, cleanupNamesData) Ex 4: prototype Turn the list of boys names data frames into a single data frame. boysNames &lt;- bind_rows(boysNames) glimpse(boysNames) Create a new directory called all within dataSets and write the data to a .csv file. HINT: see the ?dir.create and ?write_csv functions. dir.create(&quot;dataSets/all&quot;) write_csv(boysNames, &quot;dataSets/all/boys_names.csv&quot;) What were the five most popular names in 2013? boysNames %&gt;% filter(Year == 2013) %&gt;% arrange(desc(Count)) %&gt;% head() How has the popularity of the name “ANDREW” changed over time? andrew &lt;- filter(boysNames, Name == &quot;ANDREW&quot;) ggplot(andrew, aes(x = Year, y = Count)) + geom_line() + ggtitle(&quot;Popularity of Andrew, over time&quot;) Wrap-up Feedback These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu R Learn from the best: http://adv-r.had.co.nz/; http://r4ds.had.co.nz/ R documentation: http://cran.r-project.org/manuals.html Collection of R tutorials: http://cran.r-project.org/other-docs.html R for Programmers (by Norman Matloff, UC–Davis) http://heather.cs.ucdavis.edu/~matloff/R/RProg.pdf Calling C and Fortran from R (by Charles Geyer, UMinn) http://www.stat.umn.edu/~charlie/rc/ State of the Art in Parallel Computing with R (Schmidberger et al.) http://www.jstatso|.org/v31/i01/paper "],
["PythonInstall.html", "Python Installation Software Materials Resources", " Python Installation Software Before starting any of our Python workshops, it is necessary to complete 2 tasks. Please make sure both of these tasks are completed before you attend your workshop, as, depending on your internet speed, they may take a long time. If you are unable to complete both tasks, please stop by the training room at least 20 minutes prior to the start of the workshop. download and install Anaconda Python 3 distribution download and unzip class materials The Anaconda Python distribution is designed with data Science in mind and contains a curated set of 270+ pre-installed Python packages. Mac OS X: Install Anaconda Python 3 by downloading and running this .pkg file. Accept the defaults proposed by the Anaconda installer. Windows: Install Anaconda Python 3 by downloading and running this .exe file. Accept the defaults proposed by the Anaconda installer. Linux: Install Anaconda Python 3 by downloading and running this .sh file. Accept the defaults proposed by the Anaconda installer. Success? After installing, please start the Anaconda Navigator program. If you were successful with the installation, you should see a window similar to this: To check that the installation is working correctly, click the Launch button under Jupyter Notebook. A new blank notebook should be created. If you are having any difficulties with the installation, please stop by the training room at least 20 minutes prior to the start of the workshop. Materials Download class materials for your workshop: Python Introduction: https://github.com/IQSS/dss-workshops/raw/master/Python/PythonIntro.zip Python Webscraping: https://github.com/IQSS/dss-workshops/raw/master/Python/PythonWebScrape.zip Extract materials from the zipped directory (Right-click =&gt; Extract All on Windows, double-click on Mac) and move them to your desktop! Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu "],
["PythonIntro.html", "Python Introduction Setup Python basics Using object methods &amp; lists Iterating over collections of data Importing packages Exercise solutions Wrap-up", " Python Introduction Topics Functions Assignment Finding help List and dictionary structures Indexing data structures Iterating over collections of data Importing packages Setup Class Structure Informal — Ask questions at any time. Really! Collaboration is encouraged - please spend a minute introducing yourself to your neighbors! Prerequisites This is an introductory Python course: Assumes no knowledge of Python Relatively slow-paced Goals We will learn about the Python language by analyzing the text of Lewis Carroll’s Alice’s Adventures in Wonderland. In particular, our goals are to learn about: How we can interact with Python Foundations of the language (functions, objects, assignment, methods) Using methods and lists to analyze data Iterating over collections of data to automate repetitive tasks Storing related data in dictionaries (as key - value pairs) Importing packages to add functionality Python basics GOAL: To learn about the foundations of the Python language. Python interfaces Functions Objects Assignment Methods What is Python? Python is a relatively easy to learn general purpose programming language. People use Python to manipulate, analyze, and visualize data, make web sites, write games, and much more. Youtube, DropBox, and BitTorrent are among the things people used python to make. Like most popular open source programming languages, Python can be thought of as a platform that runs a huge number and variety of packages. The language itself is mostly valuable because it makes it easy to create and use a large number of useful packages. Interfaces Text editors, IDEs, &amp; Notebooks There are different ways of interacting with Python. The two main ways are through: text editors or Integrated Development Environments (IDEs): Text editors and IDEs are not really separate categories; as you add features to a text editor it becomes more like an IDE. Some editors/IDEs are language-specific while others are general purpose — typically providing language support via plugins. Here are a few popular editors/IDEs that can be used with Python: Editor / IDE Features Ease of use Language support Spyder Excellent Easy Python only PyCharm Excellent Moderate Python only Jupyter Lab Good Easy Excellent VS code Excellent Easy Very good Atom Good Moderate Good Vim Excellent Hard Good Emacs Excellent Hard Excellent Notebooks: Web-based applications that allow you to create and share documents that contain live code, equations, visualizations, and narrative text. For these workshops, we will use a Jupyter Notebook; an open source notebook that has support for 40+ languages. Source code &amp; literate programming There are also several different formats available for writing code in Python. These basically boil down to a choice between: Source code: the practice of writing code, and possibly comments, in a plain text document. In Python this is done by writing code in a text file with a .py extension. Writing source code has the great advantage of being simple. Souce code is the format of choice if you intend to run your code as a complete script - for example, from the command line. Literate programming: the practice of embedding computer code in a natural language document. In Python this is often done using the aformentioned Jupyter Notebook, which is a JSON document containing an ordered list of input/output cells which can contain code, text (using Markdown), mathematics, plots, and rich media, usually ending with the .ipynb extension. Jupyter Notebooks are easy to write, human-readable, and the format of choice if you intend to run your code interactively, by running small pieces of code and looking at each output. Researchers can use Notebooks to write their journal papers, dissertations, and statistics/math class notes, since it is easy to convert into other formats later, such as HTML (for a webpage), MS Word, or PDF (via LaTeX). Here are some resources for learning more about Jupyter Notebooks: https://www.dataquest.io/blog/jupyter-notebook-tutorial/ https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook https://realpython.com/jupyter-notebook-introduction/ Launch Jupyter Notebook Start the Anaconda Navigator program Click the Launch button under Jupyter Notebook Click upload, then in the pop-up window select the PythonIntro folder on the desktop Open and upload 3 files (note, this is TWO steps): PythonIntro.ipnb, Alice_in_wonderland.txt, and Characters.txt A Jupyter Notebook contains one or more cells containing notes or code. To insert a new cell click the + button in the upper left. To execute a cell, select it and press Control+Enter or click the Run button at the top. Function calls In Python, functions perform tasks and take the form: # function_name(arg1, arg2, arg3, ... argn) where arg1 etc. are arguments to the function. Assignment In Python we can assign a result to an name using the = operator. # name = thing_to_assign x = 10 The name on the left of the equals sign is one that we chose. When choosing names, they must: start with a letter use only letters, numbers and underscores Reading data Reading information from a file is the first step in many projects, so we’ll use functions to read data into Python and assign them to a named object. The workshop materials you downloaded earlier include a file named Alice_in_wonderland.txt which contains the text of Lewis Carroll’s Alice’s Adventures in Wonderland. We can use the open() function to create a file object that makes a connection to the file: alice_file = open(&quot;Alice_in_wonderland.txt&quot;) The alice_file object name we just created does not contain the contents of Alice_in_wonderland.txt. It is a representation in Python of the file itself rather than the contents of the file. Object methods The alice_file object provides methods that we can use to do things with it. Methods are invoked using syntax that looks like ObjectName.method(). You can see the methods available for acting on an object by typing the object’s name followed by a . and pressing the tab key. For example, typing alice_file. and pressing tab will display a list of methods as shown below. . Among the methods we have for doing things with our alice_file object is one named read. We can use the help function to learn more about it. help(alice_file.read) Since alice_file.read looks promising, we will invoke this method and see what it does. alice_txt = alice_file.read() print(alice_txt[:500]) # the [:500] gets the first 500 character -- more on this later. That’s all there is to it! We’ve read the contents of Alice_in_wonderland.txt and stored this text in a Python object we named alice_txt. Now let’s start to explore this object, and learn some more things about Python along the way. Using object methods &amp; lists GOAL: To learn how to use methods and lists to analyze data. We will do this using the Alice text to count: Words Chapters Paragraphs How many words does the text contain? To answer this question, we can split the text up so there is one element per word, and then count the number of words. Splitting a string into a list of words How do we figure out how to split strings in Python? We can ask Python what our alice_txt object is and what methods it provides. We can ask Python what things are using the type() function, like this: type(alice_txt) Python tells us that alice_txt is of type str (i.e., it is a string). We can find out what methods are available for working strings by typing alice_txt. and pressing tab. We’ll see that among the methods is one named split, as shown below. To learn how to use this method we can check the documentation. help(alice_txt.split) Since the default is to split on whitespace (spaces, newlines, tabs) we can get a reasonable word count simply by calling the split method and counting the number of elements in the result. alice_words = alice_txt.split() # returns a list type(alice_words) Working with lists The split methods we used to break up the text of Alice in Wonderland into words produced a list. A lot of the techniques we’ll use later to analyze this text also produce lists, so its worth taking a few minutes to learn more about them. Note that the displayed representation of lists and other data structures in Python often closely matches the syntax used to create them. For example, we can create a list using square brackets, just as we see when we print a list. A list in Python is used to store a collection of items: # create a list y = [1, &quot;b&quot;, 3, &quot;D&quot;, 5, 6] As with other types in Python, you can get a list of methods by typing the name of the object followed by a . and pressing tab. Extracting subsets from lists Among the things you can do with a list is extract subsets of items using bracket indexing notation. This is useful in many situations, including the current one where we want to inspect a long list without printing out the whole thing. The examples below show how indexing works in Python. First using pseudocode: # syntax # object[ start : end : by ] # defaults # object[ 0 : end : 1 ] Then using a real list: # create a list y = [1, &quot;b&quot;, 3, &quot;D&quot;, 5, 6] y[0] # returns first element - the number 1 (yes, the index counts from zero!) y[1] # returns second element - the letter &quot;b&quot; y[ :3] # returns a list with only the first 3 elements, but index is of length 4 (0 to 3) because last index is excluded y[2:5] # returns a list with elements 3, &quot;D&quot;, 5 y[-1] # returns last element - the number 6 y[-4: ] # returns a list with last 4 elements alice_words[11:20] # returns a list with words 11 through 19 alice_words[-10: ] # returns a list with the last 10 words Using sets to count unique items Now that we have a list containing the individual words from Alice’s Adventures in Wonderland, we can calculate how many words there are in total using the len() (length) function: len(alice_words) # counts elements in a data structure According to our above computation, there are about 26 thousand total words in the Alice text. But how many unique words are there? Python has a special data structure called a set that makes it easy to find out. A set drops all duplicates, giving a collection of the unique elements. Here’s a simple example: # set example mySet = {1, 5, 9, 9, 4, 5} len(mySet) We can now use the set() function to convert the list of all words (alice_words) into a set of unique words and then count them: len(set(alice_words)) # counts unique elements in a data structure There are 5295 unique words in the text. Exercise 0 Reading text from a file &amp; splitting Alice’s Adventures in Wonderland is full of memorable characters. The main characters from the story are listed, one-per-line, in the file named Characters.txt. NOTE: we will not always explicitly demonstrate everything you need to know in order to complete an exercise. Instead we focus on teaching you how to discover available methods and how use the help function to learn how to use them. It is expected that you will spend some time during the exercises looking for appropriate methods and perhaps reading documentation. Open the Characters.txt file and read its contents. ## Split text on newlines to produce a list with one element per line. Store the result as alice_characters. HINT: you can split on newlines using the \\n separator. ## Counting list elements Now that we know how to split a string and how to work with the resulting list, we can split on chapter markers to count the number of chapters. All we need to do is specify the string to split on. Since each chapter is marked with the string 'CHAPTER ' followed by the chapter number, we can split the text up into chapters using this as the separator. alice_chapters = alice_txt.split(&quot;CHAPTER &quot;) len(alice_chapters) Since the first element contains the material before the first chapter, this tells us there are twelve chapters in the book. We can count paragraphs in a similar way. Paragraphs are indicated by a blank line, i.e., two newlines in a row. When working with strings we can represent newlines with \\n. Paragraphs are indicated by two new lines, and so our basic paragraph separator is \\n\\n. We can see this separator by looking at the content. print(alice_txt[:500]) # explicit printing --- formats text nicely alice_txt[:500] # returns content without printing it alice_paragraphs = alice_txt.split(&quot;\\n\\n&quot;) Before counting the number of paragraphs, I want to inspect the result to see if it looks correct: print(alice_paragraphs[0], &quot;\\n==========&quot;) print(alice_paragraphs[1], &quot;\\n==========&quot;) print(alice_paragraphs[2], &quot;\\n==========&quot;) We’re counting the title, author, and chapter lines as paragraphs, but this will do for a rough count. len(alice_paragraphs) Exercise 1 Count the number of main characters So far we’ve learned that there are 12 chapters, around 830 paragraphs, and about 26 thousand words in Alice’s Adventures in Wonderland. Along the way we’ve also learned how to open a file and read its contents, split strings, calculate the length of objects, discover methods for string and list objects, and index/subset lists in Python. Now it is time for you to put these skills to use to learn something about the main characters in the story. Count the number of main characters in the story (i.e., get the length of the list you created in previous exercise). ## Extract and print just the first character from the list you created in the previous exercise. ## Iterating over collections of data GOAL: To learn how to automate repetitive tasks by iterating over collections of data. We will do this using the Alice text to count: Words nested within paragraphs Paragraphs nested with chapters This far our analysis has treated the text as a “flat” data structure. For example, when we counted words we just counted words in the whole document, rather than counting the number of words in each chapter. If we want to treat our document as a nested structure, with words forming sentences, sentences forming paragraphs, paragraphs forming chapters, and chapters forming the book, we need to learn some additional tools. Specifically, we need to learn how to iterate over lists (or other collections) and do things with each element in a collection. There are several ways to iterate in Python, of which we will focus on for loops. Iterating over lists using for-loops A for loop is a way of cycling through the elements of a collection and doing something with each one. The for loop logic is: The for loop syntax is: # for &lt;thing&gt; in &lt;collection&gt;: # do stuff with &lt;thing&gt; Notice that the body of the for-loop is indented. This is important, because it is this indentation that defines the body of the loop — the place where things are done. White space matters in Python! A simple example: for i in range(10): print(i) print(&#39;DONE.&#39;) Notice that “DONE.” is only printed once, since print('DONE.') is not indented and is therefore outside of the body of the loop. As a simple example using the Alice text, we can cycle through the first 6 paragraphs and print each one. Cycling through with a loop makes it easy to insert a separator between the paragraphs, making it much clearer to read the output: for paragraph in alice_paragraphs[:6]: print(paragraph) print(&#39;==================================&#39;) print(&#39;DONE.&#39;) Loops in Python are great because the syntax is relatively simple, and because they are very powerful. Inside of the body of a loop you can use all the tools you use elsewhere in Python. Here is one more example of a loop, this time iterating over all the chapters and calculating the number of paragraphs in each chapter. for chapter in alice_chapters[1:]: paragraphs = chapter.split(&quot;\\n\\n&quot;) print(len(paragraphs)) Organizing results in dictionaries It’s often useful to store separate pieces of data that are related to one another in a dict (i.e., “dictionary”), which is designed to store key-value pairs. For example, we can calculate the number of times “Alice” is mentioned per chapter and associate each count with the chapter title it corresponds to. The dictionary structure looks like: # mydict = {key1:value1, key2:value2, key3:value3} A simple example: mydict = {&quot;apple&quot;:5, &quot;pear&quot;:6, &quot;grape&quot;:10} print(mydict) # compare the above dict to a list mylist =[5, 6, 10] print(mylist) To associate chapter titles with “Alice” counts, we will first need to learn how to append elements to a list: container = [] # a list for i in range(10): container.append(i) # append elements to the list print(container) Now, with the Alice text, first we can iterate over each chapter and grab just the first line (that is, the chapter titles). These will become our keys. chapter_titles = [] for chapter in alice_chapters[1:]: chapter_titles.append(chapter.split(sep=&quot;\\n&quot;)[0]) print(chapter_titles) Next, we can iterate over each chapter and count the number of times “Alice” was mentioned. These will become our values. chapter_Alice = [] for chapter in alice_chapters[1:]: chapter_Alice.append(chapter.count(&quot;Alice&quot;)) Finally we can combine the chapter titles (keys) and “Alice” counts (values) and convert them to a dictionary. # combine titles and counts mydict = dict(zip(chapter_titles, chapter_Alice)) print(mydict) help(zip) Exercise 2 Iterating &amp; counting things Now that we know how to iterate using for-loops, the possibilities really start to open up. For example, we can use these techniques to count the number of times each character appears in the story. Make sure you have both the text and the list of characters. Open and read both “Alice_in_wonderland.txt” and “Characters.txt” if you have not already done so. ## Which chapter has the most words? Split the text into chapters (i.e., split on “CHAPTER”) and use a for-loop to iterate over the chapters. For each chapter, split it into words and calculate the length. ## How many times is each character mentioned in the text? Iterate over the list of characters using a for-loop. For each character, call the count method with that character as the argument. ## (BONUS, optional): Put the character counts computed above in a dictionary with character names as the keys and counts as the values. ## Importing packages GOAL: To learn how to expand Python’s functionality by importing packages. Import numpy Calculate simple statistics Now that we know how to iterate over lists and calculate numbers for each element, we may wish to do some simple math using these numbers. For example, we may want to calculate the mean and standard deviation of the distribution of the number of paragraphs in each chapter. Python has a handful of math functions built-in (e.g., min() and max()) but built-in math support is pretty limited. When you find that something isn’t available in Python itself, its time to look for a package that does it. Although it is somewhat overkill for simply calculating a mean we’re going to use a popular package called numpy for this. The numpy package is included in the Anaconda Python distribution we are using, so we don’t need to install it separately. To use numpy or other packages, you must first import them. # import &lt;package-name&gt; We can import numpy as follows: import numpy To use functions from a package, we can prefix the function with the package name, separated by a period: # &lt;package-name&gt;.&lt;function_name&gt;() The numpy package is very popular and includes a lot of useful functions. For example, we can use it to calculate means and standard deviations: print(numpy.mean(chapter_Alice)) print(numpy.std(chapter_Alice)) Exercise solutions Ex 0: prototype # 1. Open the Characters.txt file and read its contents. characters_file = open(&quot;Characters.txt&quot;) characters_txt = characters_file.read() # 2. Split text on newlines to produce a list with one element per line. # Store the result as &quot;alice_characters&quot;. alice_characters = characters_txt.split(sep=&quot;\\n&quot;) alice_characters Ex 1: prototype # 1. Count the number of main characters in the story (i.e., get the length of the list you created in previous exercise). len(alice_characters) # 2. Extract and print just the first character from the list you created in the previous exercise. print(alice_characters[0]) # 3. (BONUS, optional): Sort the list you created in step 2 alphabetically, # and then extract the last element. alice_characters.sort() alice_characters[-1] Ex 2: prototype # 1. Make sure you have both the text and the list of characters. # Open and read both &quot;Alice_in_wonderland.txt&quot; and &quot;Characters.txt&quot; if you have not already done so. characters_txt = open(&quot;Characters.txt&quot;).read() alice_txt = open(&quot;Alice_in_wonderland.txt&quot;).read() # 2. Which chapter has the most words? # Split the text into chaptes (i.e., split on &quot;CHAPTER &quot;) and use a for-loop to iterate over the chapters. # For each chapter, split it into words and calculate the length. words_per_chapter = [] for chapter in alice_chapters: words_per_chapter.append(len(chapter.split())) words_per_chapter # 3. How many times is each character mentioned in the text? # Iterate over the list of characters using a for-loop. # For each character, call the count method with that character as the argument. num_per_character = [] for character in characters_txt.split(sep=&quot;\\n&quot;): num_per_character.append(alice_txt.count(character)) num_per_character # 4. (BONUS, optional): Put the character counts computed above in a # dictionary with character names as the keys and counts as the values. characters = characters_txt.split(sep=&quot;\\n&quot;) dict(zip(characters, num_per_character)) Wrap-up Feedback These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu Graphics matplotlib: https://matplotlib.org/ seaborn: https://seaborn.pydata.org/ plotly: https://plot.ly/python/ Quantitative Data Analysis numpy: http://www.numpy.org/ scipy: https://www.scipy.org/ pandas: https://pandas.pydata.org/ scikit-learn: http://scikit-learn.org/stable/ statsmodels: http://www.statsmodels.org/stable/ Text analysis textblob: https://textblob.readthedocs.io/en/dev/ nltk: http://www.nltk.org/ Gensim: https://radimrehurek.com/gensim/ Webscraping scrapy: https://scrapy.org/ requests: http://docs.python-requests.org/en/master/ lxml: https://lxml.de/ BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/ Social Network Analysis networkx: https://networkx.github.io/ graph-tool: https://graph-tool.skewed.de/ "],
["PythonWebScrape.html", "Python Web-Scraping Setup Preliminary questions How does the web work? Example project Take shortcuts if you can Parse HTML if you have to Scrapy: for large / complex projects Browser drivers: a last resort Exercise solutions Wrap-up", " Python Web-Scraping Topics Web basics Making web requests Inspecting web sites Retrieving web data Using Xpaths to retrieve html content Parsing html content Cleaning and storing text from html Setup Class Structure Informal — Ask questions at any time. Really! Collaboration is encouraged - please spend a minute introducing yourself to your neighbors! Prerequisites This is an intermediate Python course: Assumes knowledge of Python, including: lists dictionaries logical indexing iteration with for-loops Assumes basic knowledge of web page structure Relatively fast-paced If you need an introduction to Python or a refresher, we recommend our Python Introduction. Learning Outcomes learn basic web scraping principles and techniques learn how to use the requests package in Python practice making requests and manipulating responses from the server Note also that this workshop will not teach you everything you need to know in order to retrieve data from any web service you might wish to scrape. Workshop Outline This workshop is organized into two parts: Retrive information in JSON format Parse HTML file Preliminary questions What is web scraping? Web scraping is the activity of automating retrieval of information from a web service designed for human interaction. Is web scraping legal? Is it ethical? It depends. If you have legal questions seek legal counsel. You can mitigate some ethical issues by building delays and restrictions into your web scraping program so as to avoid impacting the availability of the web service for other users or the cost of hosting the service for the service provider. How does the web work? Components Computers connected to the web are called clients and servers. A simplified diagram of how they interact might look like this: Clients are the typical web user’s internet-connected devices (for example, your computer connected to your Wi-Fi) and web-accessing software available on those devices (usually a web browser like Firefox or Chrome). Servers are computers that store webpages, sites, or apps. When a client device wants to access a webpage, a copy of the webpage is downloaded from the server onto the client machine to be displayed in the user’s web browser. Your internet connection allows you to send and receive data on the web. TCP/IP are the communication protocols that define how data should travel across the web. Domain Name Servers (DNS) are like an address book for websites. HTTP is an application protocol that defines a language for clients and servers to speak to each other. So What Happens, Exactly? When you type a web address into your browser: The browser goes to the DNS server, and finds the real address of the server that the website lives on. The browser sends an HTTP request message to the server, asking it to send a copy of the website to the client. This message, and all other data sent between the client and the server, is sent across your internet connection using TCP/IP. If the server approves the client’s request, the server sends the client a “200 OK” message, and then starts sending the website’s files to the browser as a series of small chunks called data packets. The browser assembles the small chunks into a complete website and displays it to you. Example project In this workshop I will demonstrate web scraping techniques using the Collections page at https://www.harvardartmuseums.org/collections and let you use the skills you’ll learn to retrieve information from other parts of the Harvard Art Museums website. The basic strategy is pretty much the same for most scraping projects. We will use our web browser (Chrome or Firefox recommended) to examine the page you wish to retrieve data from, and copy/paste information from your web browser into your scraping program. Take shortcuts if you can We wish to extract information from https://www.harvardartmuseums.org/collections. Like most modern web pages, a lot goes on behind the scenes to produce the page we see in our browser. Our goal is to pull back the curtain to see what the website does when we interact with it. Once we see how the website works we can start retrieving data from it. If we are lucky we’ll find a resource that returns the data we’re looking for in a structured format like JSON or XML. Examine the website’s structure We start by opening the collections web page in a web browser and inspecting it. If we scroll down to the bottom of the Collections page, we’ll see a button that says “Load More”. Let’s see what happens when we click on that button. To do so, click on “Network” in the developer tools window, then click the “Load More Collections” button. You should see a list of requests that were made as a result of clicking that button, as shown below. If we look at that second request, the one to a script named browse, we’ll see that it returns all the information we need, in a convenient format called JSON. All we need to retrieve collection data is call make GET requests to https://www.harvardartmuseums.org/browse with the correct parameters. Launch Jupyter Notebook Start the Anaconda Navigator program Click the Launch button under Jupyter Notebook Click upload, then in the pop-up window select the PythonWebScrape folder on the desktop Open and upload the PythonWebScrape.ipynb file (note, this is TWO steps) A Jupyter Notebook contains one or more cells containing notes or code. To insert a new cell click the + button in the upper left. To execute a cell, select it and press Control+Enter or click the Run button at the top. Making requests The URL we want to retrieve data from has the following structure scheme domain path parameters https www.harvardartmuseums.org browse load_amount=10&amp;offset=0 It is often convenient to create variables containing the domain(s) and path(s) you’ll be working with, as this allows you to swap out paths and parameters as needed. Note that the path is separated from the domain with / and the parameters are separated from the path with ?. If there are multiple parameters they are separated from each other with a &amp;. For example, we can define the domain and path of the collections URL as follows: museum_domain = &#39;https://www.harvardartmuseums.org&#39; collection_path = &#39;browse&#39; collection_url = (museum_domain + &quot;/&quot; + collection_path) print(collection_url) Note that we omit the parameters here because it is usually easier to pass them as a dict when using the requests library in Python. This will become clearer shortly. Now that we’ve constructed the URL we wish interact with we’re ready to make our first request in Python. import requests collections1 = requests.get( collection_url, params = {&#39;load_amount&#39;: 10, &#39;offset&#39;: 0} ) # ### Parsing JSON data # We already know from inspecting network traffic in our web # browser that this URL returns JSON, but we can use Python to verify # this assumption. collections1.headers[&#39;Content-Type&#39;] Since JSON is a structured data format, parsing it into python data structures is easy. In fact, there’s a method for that! collections1 = collections1.json() print(collections1) That’s it. Really, we are done here. Everyone go home! OK not really, there is still more we can lean. But you have to admit that was pretty easy. If you can identify a service that returns the data you want in structured from, web scraping becomes a pretty trivial enterprise. We’ll discuss several other scenarios and topics, but for some web scraping tasks this is really all you need to know. Organizing &amp; saving the data The records we retrieved from https://www.harvardartmuseums.org/browse are arranged as a list of dictionaries. We can easily select the fields of arrange these data into a pandas DataFrame to facilitate subsequent analysis. import pandas as pd records1 = pd.DataFrame.from_records(collections1[&#39;records&#39;]) print(records1) and write the data to a file. records1.to_csv(&quot;records1.csv&quot;) Iterating to retrieve all the data Of course we don’t want just the first page of collections. How can we retrieve all of them? Now that we know the web service works, and how to make requests in Python, we can iterate in the usual way. records = [] for offset in range(0, 50, 10): param_values = {&#39;load_amount&#39;: 10, &#39;offset&#39;: offset} current_request = requests.get(collection_url, params = param_values) records += current_request.json()[&#39;records&#39;] ## convert list of dicts to a `DataFrame` records_final = pd.DataFrame.from_records(records) # write the data to a file. records_final.to_csv(&quot;records_final.csv&quot;) print(records_final) Exercise 0 Retrieve exhibits data In this exercise you will retrieve information about the art exhibitions at Harvard Art Museums from https://www.harvardartmuseums.org/visit/exhibitions Using a web browser (Firefox or Chrome recommended) inspect the page at https://www.harvardartmuseums.org/visit/exhibitions. Examine the network traffic as you interact with the page. Try to find where the data displayed on that page comes from. Make a get request in Python to retrieve the data from the URL identified in step1. Write a loop or list comprehension in Python to retrieve data for the first 5 pages of exhibitions data. Bonus (optional): Convert the data you retrieved into a pandas DataFrame and save it to a .csv file. Parse HTML if you have to As we’ve seen, you can often inspect network traffic or other sources to locate the source of the data you are interested in and the API used to retrieve it. You should always start by looking for these shortcuts and using them where possible. If you are really lucky, you’ll find a shortcut that returns the data as JSON or XML. If you are not quite so lucky, you will have to parse HTML to retrieve the information you need. Document Object Model (DOM) To parse HTML, we need to have a nice tree structure that contains the whole HTML file through which we can locate the information. This tree-like structure is the Document Object Model (DOM). DOM is a cross-platform and language-independent interface that treats an XML or HTML document as a tree structure wherein each node is an object representing a part of the document. The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects. DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. The following is an example of DOM hierarchy in an HTML document: Retrieving HTML When I inspected the network traffic while interacting with https://www.harvardartmuseums.org/visit/calendar I didn’t see any requests that returned JSON data. The best we can do appears to be https://www.harvardartmuseums.org/visit/calendar?date=, which unfortunately returns HTML. The first step is the same as before: we make at GET request. calendar_path = &#39;visit/calendar&#39; calendar_url = (museum_domain # recall that we defined museum_domain earlier + &quot;/&quot; + calendar_path) print(calendar_url) events0 = requests.get(calendar_url, params = {&#39;date&#39;: &#39;2018-11&#39;}) As before we can check the headers to see what type of content we received in response to our request. events0.headers[&#39;Content-Type&#39;] Parsing HTML using the lxml library Like JSON, HTML is structured; unlike JSON it is designed to be rendered into a human-readable page rather than simply to store and exchange data in a computer-readable format. Consequently, parsing HTML and extracting information from it is somewhat more difficult than parsing JSON. While JSON parsing is built into the Python requests library, parsing HTML requires a separate library. I recommend using the HTML parser from the lxml library; others prefer an alternative called BeautyfulSoup. from lxml import html events_html = html.fromstring(events0.text) Using XPath to extract content from HTML XPath is a tool for identifying particular elements withing a HTML document. The developer tools built into modern web browsers make it easy to generate XPaths that can used to identify the elements of a web page that we wish to extract. We can open the html document we retrieved and inspect it using our web browser. html.open_in_browser(events_html, encoding = &#39;UTF-8&#39;) Once we identify the element containing the information of interest we can use our web browser to copy the XPath that uniquely identifies that element. Next we can use python to extract the element of interest: events_list_html = events_html.xpath(&#39;//*[@id=&quot;events_list&quot;]&#39;)[0] Once again we can use a web browser to inspect the HTML we’re currently working with, and to figure out what we want to extract from it. Let’s look at the first element in our events list. first_event_html = events_list_html[0] html.open_in_browser(first_event_html, encoding = &#39;UTF-8&#39;) As before we can use our browser to find the xpath of the elements we want. (Note that the html.open_in_browser function adds enclosing html and body tags in order to create a complete web page for viewing. This requires that we adjust the xpath accordingly.) By repeating this process for each element we want, we can build a list of the xpaths to those elements. elements_we_want = {&#39;figcaption&#39;: &#39;div/figure/div/figcaption&#39;, &#39;date&#39;: &#39;div/div/header/time&#39;, &#39;title&#39;: &#39;div/div/header/h2/a&#39;, &#39;time&#39;: &#39;div/div/div/p[1]/time&#39;, &#39;localtion1&#39;: &#39;div/div/div/p[2]/span/span[1]&#39;, &#39;location2&#39;: &#39;div/div/div/p[2]/span/span[2]&#39; } Finally, we can iterate over the elements we want and extract them. first_event_values = {} for key in elements_we_want.keys(): element = first_event_html.xpath(elements_we_want[key])[0] first_event_values[key] = element.text_content().strip() print(first_event_values) Iterating to retrieve content from a list of HTML elements So far we’ve retrieved information only for the first event. To retrieve data for all the events listed on the page we need to iterate over the events. If we are very lucky, each event will have exactly the same information structured in exactly the same way and we can simply extend the code we wrote above to iterate over the events list. Unfortunately not all these elements are available for every event, so we need to take care to handle the case where one or more of these elements is not available. We can do that by defining a function that tries to retrieve a value and returns an empty string if it fails. def get_event_info(event, path): try: info = event.xpath(path)[0].text.strip() except: info = &#39;&#39; return info Armed with this function we can iterate over the list of events and extract the available information for each one. all_event_values = {} for key in elements_we_want.keys(): key_values = [] for event in events_list_html: key_values.append(get_event_info(event, elements_we_want[key])) all_event_values[key] = key_values For convenience we can arrange these values in a pandas DataFrame and save them as .csv files, just as we did with our exhibitions data earlier. all_event_values = pd.DataFrame.from_dict(all_event_values) all_event_values.to_csv(&quot;all_event_values.csv&quot;) print(all_event_values) Exercise 1 parsing HTML In this exercise you will retrieve information about the physical layout of the Harvard Art Museums. The web page at https://www.harvardartmuseums.org/visit/floor-plan contains this information in HTML from. Using a web browser (Firefox or Chrome recommended) inspect the page at https://www.harvardartmuseums.org/visit/floor-plan. Copy the XPath to the element containing the list of level information. (HINT: the element if interest is a ul, i.e., unordered list.) Make a get request in Python to retrieve the web page at https://www.harvardartmuseums.org/visit/floor-plan. Extract the content from your request object and parse it using html.fromstring from the lxml library. Use your web browser to find the XPaths to the facilities housed on level one. Use Python to extract the text from those Xpaths. Bonus (optional): Write a loop or list comprehension in Python to retrieve data for all the levels. Scrapy: for large / complex projects Scraping websites using the requests library to make GET and POST requests, and the lxml library to process HTML is a good way to learn basic web scraping techniques. It is a good choice for small to medium size projects. For very large or complicated scraping tasks the scrapy library offers a number of conveniences, including asynchronously retrieval, session management, convenient methods for extracting and storing values, and more. More information about scrapy can be found at https://doc.scrapy.org. Browser drivers: a last resort It is sometimes necessary (or sometimes just easier) to use a web browser as an intermediary rather than communicating directly with a web service. This method has the advantage of being about to use the javascript engine and session management features of a web browser; the main disadvantage is that it is slower and tends to be more fragile than using requests or scrapy to make requests directly from python. For small scraping projects involving complicated sites with CAPTHAs or lots of complicated javascript using a browser driver can be a good option. More information is available at https://www.seleniumhq.org/docs/03_webdriver.jsp. Exercise solutions Ex 0: prototype Ex 1: prototype Wrap-up Feedback These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu "],
["StataIntro.html", "Stata Introduction Setup Why stata? Reading data Statistics &amp; graphs Basic data management Working on subsets Generating &amp; replacing variables Exercise solutions Wrap-up", " Stata Introduction Topics Stata interface and Do-files Finding help Reading and writing data Basic summary statistics Basic graphs Basic data management Setup Software &amp; Materials Laptop users: you will need a copy of Stata installed on your machine. Harvard FAS affiliates can install a licensed version from http://downloads.fas.harvard.edu/download Download class materials at https://github.com/IQSS/dss-workshops/raw/master/Stata/StataIntro.zip Extract materials from the zipped directory StataIntro.zip (Right-click =&gt; Extract All on Windows, double-click on Mac) and move them to your desktop! Organization Please feel free to ask questions at any point if they are relevant to the current topic (or if you are lost!) Collaboration is encouraged - please introduce yourself to your neighbors! If you are using a laptop, you will need to adjust file paths accordingly Make comments in your Do-file - save on flash drive or email to yourself Goals This is an introduction to Stata Assumes no/very little knowledge of Stata Not appropriate for people already familiar with Stata Learning Objectives: Familiarize yourself with the Stata interface Get data in and out of Stata Compute statistics and construct graphical displays Compute new variables and transformations Why stata? Used in a variety of disciplines User-friendly Great guides available on web Excellent modeling capabilities Student and other discount packages available at reasonable cost Stata interface Review and Variable windows can be closed (user preference) Command window can be shortened (recommended) Do-files You can type all the same commands into the Do-file that you would type into the command window BUT…the Do-file allows you to save your commands Your Do-file should contain ALL commands you executed – at least all the “correct” commands! I recommend never using the command window or menus to make CHANGES to data Saving commands in Do-file allows you to keep a written record of everything you have done to your data Allows easy replication Allows you to go back and re-run commands, analyses and make modifications Stata help To get help in Stata type help followed by topic or command, e.g., help codebook. Syntax rules Most Stata commands follow the same basic syntax: Command varlist, options. Use comments liberally — start with a comment describing your Do-file and use comments throughout * Use &#39;*&#39; to comment a line and &#39;//&#39; for in-line comments * Make Stata say hello: disp &quot;Hello &quot; &quot;World!&quot; // &#39;disp&#39; is short for &#39;display&#39; Use /// to break varlists over multiple lines: disp &quot;Hello&quot; /// &quot; World!&quot; Let’s get started Launch the Stata program (MP or SE, does not matter unless doing computationally intensive work) Open up a new Do-file Run our first Stata code! * change directory // cd &quot;C://Users/dataclass/Desktop/StataIntro&quot; Reading data Data file commands Next, we want to open our data file Open/save data sets with “use” and “save”: cd dataSets // open the gss.dta data set use gss.dta, clear // save data file: save newgss.dta, replace // &quot;replace&quot; option means OK to overwrite existing file A note about path names If your path has no spaces in the name (that means all directories, folders, file names, etc. can have no spaces), you can write the path as is If there are spaces, you need to put your pathname in quotes Best to get in the habit of quoting paths Where’s my data? Data editor (browse) Data editor (edit) Using the data editor is discouraged (why?) Always keep any changes to your data in your Do-file Avoid temptation of making manual changes by viewing data via the browser rather than editor Reading non-Stata data Import delimited text files * import data from a .csv file import delimited gss.csv, clear * save data to a .csv file export delimited gss_new.csv, replace Import data from SAS * import/export SAS xport files clear import sasxport gss.xpt export sasxport gss_new, replace Import data from Excel * import/export Excel files clear import excel gss.xlsx export excel gss_new, replace What if my data is from another statistical software program? SPSS/PASW will allow you to save your data as a Stata file Go to: file -&gt; save as -&gt; Stata (use most recent version available) Then you can just go into Stata and open it Another option is StatTransfer, a program that converts data from/to many common formats, including SAS, SPSS, Stata, and many more Exercise 0 Importing data Save any work you’ve done so far. Close down Stata and open a new session. Start Stata and open your .do file. Change directory (cd) to the dataSets folder. Try opening the following files: A comma separated value file: gss.csv An Excel file: gss.xlsx Statistics &amp; graphs Frequently used commands Commands for reviewing and inspecting data: describe // labels, storage type etc. sum // statistical summary (mean, sd, min/max etc.) codebook // storage type, unique values, labels list // print actuall values tab // (cross) tabulate variables browse // view the data in a spreadsheet-like window First, let’s ask Stata for help about these commands: help sum use gss.dta, clear sum educ // statistical summary of education codebook region // information about how region is coded tab sex // numbers of male and female participants If you run these commands without specifying variables, Stata will produce output for every variable Basic graphing commands Univariate distribution(s) using hist /* Histograms */ hist educ // histogram with normal curve; see &quot;help hist&quot; for other options hist age, normal View bivariate distributions with scatterplots /* scatterplots */ twoway (scatter educ age) graph matrix educ age inc The by command Sometimes, you’d like to generate output based on different categories of a grouping variable The “by” command does just this * By Processing bysort sex: tab happy // tabulate happy separately for men and women bysort marital: sum educ // summarize eudcation by marital status Exercise 1 Descriptive statistics Use the dataset, gss.dta Examine a few selected variables using the describe, sum and codebook commands Tabulate the variable, “marital,” with and without labels Summarize the variable, “income” by marital status Cross-tabulate marital with region Summarize the variable happy for married individuals only Basic data management Labels You never know why and when your data may be reviewed ALWAYS label every variable no matter how insignificant it may seem Stata uses two sets of labels: variable labels and value labels Variable labels are very easy to use – value labels are a little more complicated Variable &amp; value labels Variable labels /* Labelling and renaming */ // Label variable inc &quot;household income&quot; label var inc &quot;household income&quot; // change the name &#39;educ&#39; to &#39;education&#39; rename educ education // you can search names and labels with &#39;lookfor&#39; lookfor household Value labels are a two step process: define a value label, then assign defined label to variable(s) /*define a value label for sex */ label define mySexLabel 1 &quot;Male&quot; 2 &quot;Female&quot; /* assign our label set to the sex variable*/ label val sex mySexLabel Exercise 2 Variable labels &amp; value labels Open the data set gss.csv Familiarize yourself with the data using describe, sum, etc. Rename and label variables using the following codebook: Var Rename to Label with v1 marital marital status v2 age age of respondent v3 educ education v4 sex respondent’s sex v5 inc household income v6 happy general happiness v7 region region of interview Add value labels to your marital variable using this codebook: Value Label 1 “married” 2 “widowed” 3 “divorced” 4 “separated” 5 “never married” Working on subsets It is often useful to select just those rows of your data where some condition holds–for example select only rows where sex is 1 (male) The following operators allow you to do this: Operator Meaning == equal to != not equal to &gt; greater than &gt;= greater than or equal to &lt; less than &lt;= less than or equal to &amp; and | or Note the double equals signs for testing equality Generating &amp; replacing variables Create new variables using gen // create a new variable named mc_inc // equal to inc minus the mean of inc gen mc_inc = inc - 15.37 Sometimes useful to start with blank values and fill them in based on values of existing variables /* the &#39;generate and replace&#39; strategy */ // generate a column of missings gen age_wealth = . // Next, start adding your qualifications replace age_wealth=1 if age&lt;30 &amp; inc &lt; 10 replace age_wealth=2 if age&lt;30 &amp; inc &gt; 10 replace age_wealth=3 if age&gt;30 &amp; inc &lt; 10 replace age_wealth=4 if age&gt;30 &amp; inc &gt; 10 // conditions can also be combined with &quot;or&quot; gen young=0 replace young=1 if age_wealth==1 | age_wealth==2 Exercise 3 Manipulating variables Use the dataset, gss.dta Generate a new variable, age2 equal to age squared Generate a new high_income variable that will take on a value of “1” if a person has an income value greater than “15” and “0” otherwise Generate a new divorced_separated dummy variable that will take on a value of “1” if a person is either divorced or separated and “0” otherwise Exercise solutions Ex 0: prototype ** Ex 1: prototype ** Ex 2: prototype ** Ex 3: prototype ** Wrap-up Feedback These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu Stata UCLA website: http://www.ats.ucla.edu/stat/Stata/ Stata website: http://www.stata.com/help.cgi?contents Email list: http://www.stata.com/statalist/ "],
["StataDatMan.html", "Stata Data Management Setup Opening Files Generating &amp; replacing variables By processing Missing values Variable types Merging, appending, &amp; joining Creating summarized data sets Exercise Solutions Wrap-up", " Stata Data Management Topics Generating and replacing variables Processing with by statements Missing values Variable types and conversion Merging, appending, and joining Creating summarized data sets Setup Software &amp; Materials Laptop users: you will need a copy of Stata installed on your machine. Harvard FAS affiliates can install a licensed version from http://downloads.fas.harvard.edu/download Download class materials at https://github.com/IQSS/dss-workshops/raw/master/Stata/StataDatMan.zip Extract materials from the zipped directory StataDatMan.zip (Right-click =&gt; Extract All on Windows, double-click on Mac) and move them to your desktop! Organization Please feel free to ask questions at any point if they are relevant to the current topic (or if you are lost!) Collaboration is encouraged - please introduce yourself to your neighbors! If you are using a laptop, you will need to adjust file paths accordingly Make comments in your Do-file - save on flash drive or email to yourself Goals This is an introduction to data management in Stata Assumes basic knowledge of Stata Not appropriate for people already familiar with Stata If you are catching on before the rest of the class, experiment with command features described in help files Learning Objectives: Basic data manipulation commands Processing with by statements Dealing with missing values Variable types and conversion Merging and appending datasets Opening Files Look at bottom left hand corner of Stata screen This is the directory Stata is currently reading from Files are located in the StataDatMan folder in your home directory Start by telling Stata where to look for these // change directory cd &quot;~/Desktop/Stata/StataDatMan&quot; // Use dir to see what is in the directory: dir dir dataSets // use the gss data set use dataSets/gss.dta set more off cd &quot;~/Desktop/Stata/StataDatMan&quot; /nfs/www/edu-harvard-iq-tutorials/Stata/StataDatMan dir total 100 drwxrwsr-x. 2 apache tutorwww 4096 Oct 9 08:44 dataSets/ -rwxrwxr-x. 1 izahn tutorwww 1302 Oct 9 08:44 Exercises.do* drwxrwsr-x. 2 apache tutorwww 4096 Oct 9 08:44 images/ drwxrwsr-x. 4 apache tutorwww 4096 Oct 9 08:44 StataDatMan/ -rwxrwxr-x. 1 izahn tutorwww 17446 Oct 9 08:44 StataDatMan.do* -rwxrwxr-x. 1 izahn tutorwww 38153 Oct 9 08:44 StataDatMan.html* -rwxrwxr-x. 1 izahn tutorwww 20463 Oct 9 08:44 StataDatMan.org* dir dataSets total 2644 -rwxrwxr-x. 1 izahn tutorwww 275705 Oct 9 08:44 gss1.dta* -rwxrwxr-x. 1 izahn tutorwww 263324 Oct 9 08:44 gss2.dta* -rwxrwxr-x. 1 izahn tutorwww 532880 Oct 9 08:44 gssAddObserve.dta* -rwxrwxr-x. 1 izahn tutorwww 527005 Oct 9 08:44 gssAppend.dta* -rwxrwxr-x. 1 izahn tutorwww 527005 Oct 9 08:44 gsscompare1.dta* -rwxrwxr-x. 1 izahn tutorwww 538755 Oct 9 08:44 gss.dta* -rwxrwxr-x. 1 izahn tutorwww 1139 Oct 9 08:44 marital.dta* use dataSets/gss.dta Generating &amp; replacing variables Data Manipulation Commands Basic commands you’ll use for generating new variables or recoding existing variables: gen egen replace recode Many different means of accomplishing the same thing in Stata – find what is comfortable (and easy) for you! Generate &amp; Replace The replace command is often used with logic statements. Available logical operators include the following: Operator Meaning == equal to != not equal to &gt; greater than &gt;= greater than or equal to &lt; less than &lt;= less than or equal to &amp; and For example: //create &quot;hapnew&quot; variable gen hapnew = . //set to 0 if happy equals 1 replace hapnew=0 if happy==1 //set to 1 if happy both and hapmar are greater than 3 replace hapnew=1 if happy&gt;3 &amp; hapmar&gt;3 // tabulate the new tab hapnew gen hapnew = . (1,419 missing values generated) replace hapnew=0 if happy==1 (435 real changes made) replace hapnew=1 if happy&gt;3 &amp; hapmar&gt;3 (4 real changes made) tab hapnew hapnew | Freq. Percent Cum. ------------+----------------------------------- 0 | 435 99.09 99.09 1 | 4 0.91 100.00 ------------+----------------------------------- Total | 439 100.00 Recode The recode command is basically generate and replace combined. You can recode an existing variable OR use recode to create a new variable (via the gen option). // recode the wrkstat variable recode wrkstat (1=8) (2=7) (3=6) (4=5) (5=4) (6=3) (7=2) (8=1) // recode wrkstat into a new variable named wrkstat2 recode wrkstat (1=8), gen(wrkstat2) // tabulate workstat tab wrkstat recode wrkstat (1=8) (2=7) (3=6) (4=5) (5=4) (6=3) (7=2) (8=1) (wrkstat: 1419 changes made) recode wrkstat (1=8), gen(wrkstat2) (32 differences between wrkstat and wrkstat2) tab wrkstat LABOR FRCE | STATUS | Freq. Percent Cum. -----------------+----------------------------------- WORKING FULLTIME | 32 2.26 2.26 WORKING PARTTIME | 155 10.92 13.18 TEMP NOT WORKING | 34 2.40 15.57 UNEMPL, LAID OFF | 214 15.08 30.66 RETIRED | 29 2.04 32.70 SCHOOL | 35 2.47 35.17 KEEPING HOUSE | 146 10.29 45.45 OTHER | 774 54.55 100.00 -----------------+----------------------------------- Total | 1,419 100.00 The table below illustrates common forms of recoding Rule Example Meaning #=# 3=1 3 recoded to 1 ##=# 2. 9 | 2 and . recoded to 9 | | #/# # 1/5=4 nonmissing=# nonmiss=8 nonmissing recoded to 8 missing=# miss=9 missing recoded to 9 egen The egen command (“extensions” to the gen command) provides convenient methods for performing many common data manipulation tasks. For example, we can use egen to create a new variable that counts the number of “yes” responses on computer, email and internet use: // count number of yes on use comp email and net egen compuser= anycount(usecomp usemail usenet), values(1) tab compuser egen compuser= anycount(usecomp usemail usenet), values(1) tab compuser usecomp | usemail | usenet == 1 | Freq. Percent Cum. ------------+----------------------------------- 0 | 623 43.90 43.90 1 | 142 10.01 53.91 2 | 78 5.50 59.41 3 | 576 40.59 100.00 ------------+----------------------------------- Total | 1,419 100.00 Here are some additional examples of egen in action: // assess how much missing data each participant has: egen countmiss = rowmiss(age-wifeft) codebook countmiss // compare values on multiple variables egen ftdiff=diff(wkftwife wkfthusb) codebook ftdiff egen countmiss = rowmiss(age-wifeft) codebook countmiss ------------------------------------------------------------------------------- countmiss (unlabeled) ------------------------------------------------------------------------------- type: numeric (float) range: [0,7] units: 1 unique values: 6 missing .: 0/1,419 tabulation: Freq. Value 296 0 215 1 113 2 7 3 782 6 6 7 egen ftdiff=diff(wkftwife wkfthusb) codebook ftdiff ------------------------------------------------------------------------------- ftdiff diff wkftwife wkfthusb ------------------------------------------------------------------------------- type: numeric (float) range: [0,1] units: 1 unique values: 2 missing .: 0/1,419 tabulation: Freq. Value 1,169 0 250 1 You will need to refer to the documentation to discover what else egen can do: type “help egen” in Stata to get a complete list of functions. Exercise 0 Generate, Replace, Recode &amp; Egen Open the gss.dta data. Generate a new variable that represents the squared value of age. Generate a new variable equal to “1” if income is greater than “19”. Create a new variable that counts the number of missing responses for each respondent. What is the maximum number of missing variables? By processing The bysort Command Sometimes, you’d like to create variables based on different categories of a single variable. For example, say you want to look at happiness based on whether an individual is male or female. The “bysort” prefix does just this: // tabulate happy separately for male and female bysort sex: tab happy // generate summary statistics using bysort bysort state: egen stateincome = mean(income) bysort degree: egen degreeincome = mean(income) bysort marital: egen marincomesd = sd(income) bysort sex: tab happy ------------------------------------------------------------------------------- -&gt; sex = Male GENERAL | HAPPINESS | Freq. Percent Cum. --------------+----------------------------------- VERY HAPPY | 189 30.39 30.39 PRETTY HAPPY | 350 56.27 86.66 NOT TOO HAPPY | 73 11.74 98.39 NA | 10 1.61 100.00 --------------+----------------------------------- Total | 622 100.00 ------------------------------------------------------------------------------- -&gt; sex = Female GENERAL | HAPPINESS | Freq. Percent Cum. --------------+----------------------------------- VERY HAPPY | 246 30.87 30.87 PRETTY HAPPY | 447 56.09 86.95 NOT TOO HAPPY | 84 10.54 97.49 DK | 1 0.13 97.62 NA | 19 2.38 100.00 --------------+----------------------------------- Total | 797 100.00 bysort state: egen stateincome = mean(income) variable state not found r(111); bysort degree: egen degreeincome = mean(income) bysort marital: egen marincomesd = sd(income) by prefix vs. by options Some commands won’t work with by prefix, but instead have a by option: // generate separate histograms for female and male hist nethrs, by(sex) Missing values You always need to consider how missing values are coded when recoding variables. Stata’s symbol for a missing value is . Stata interprets . as a large value Easy to make mistakes! To identify highly educated women, we might use the command: // generate and replace without considering missing values gen hi_ed=0 replace hi_ed=1 if wifeduc&gt;15 // What happens to our missing values? tab hi_ed, mi nola gen hi_ed=0 replace hi_ed=1 if wifeduc&gt;15 (944 real changes made) tab hi_ed, mi nola hi_ed | Freq. Percent Cum. ------------+----------------------------------- 0 | 475 33.47 33.47 1 | 944 66.53 100.00 ------------+----------------------------------- Total | 1,419 100.00 It looks like around 66% have higher education, but look closer: // gen hi_ed2, but don&#39;t set a value if wifeduc is missing gen hi_ed2 = 0 if wifeduc != . // only replace non-missing replace hi_ed2=1 if wifeduc &gt;15 &amp; wifeduc !=. //check to see that missingness is preserved tab hi_ed2, mi gen hi_ed2 = 0 if wifeduc != . (797 missing values generated) replace hi_ed2=1 if wifeduc &gt;15 &amp; wifeduc !=. (147 real changes made) | 797 56.17 100.00 ------------+----------------------------------- Total | 1,419 100.00 The correct value is 10%. Moral of the story? Be careful with missing values and remember that Stata considers missing values to be large! Bulk Conversion to Missing Values Often the data collection/generating procedure will have used some other value besides . to represent missing values. The mvdecode command will convert all these values to missing. For example: mvdecode _all, mv(999) mvdecode _all, mv(999) The “_all” command tells Stata to do this to all variables Use this command carefully! If you have any variables where “999” is a legitimate value, Stata is going to recode it to missing As an alternative, you could list var names separately rather than using “_all” Variable types Stata uses two main types of variables: String and Numeric. To be able to perform any mathematical operations, your variables need to be in a numeric format. Stata can store numbers with differing levels of precision, as described in the table below. type Minimum Maximum being 0 bytes byte -127 100 +/-1 1 int -32,767 32,740 +/-1 2 long -2,147,483,647 2,147,483,620 +/-1 4 float -1.70141173319*1038 1.70141173319*1038 +/-10-38 4 double -8.9884656743*10307 8.9884656743*10307 +/-10-323 8 Precision for float is 3.795x10-8. Precision for double is 1.414x10-16. Converting to &amp; from Strings Stata provides several ways to convert to and from strings. You can use tostring and destring to convert from one type to the other: // convert degree to a string tostring degree, gen(degree_s) // and back to a number destring degree_s, gen(degree_n) tostring degree, gen(degree_s) degree_s generated as str1 destring degree_s, gen(degree_n) degree_s has all characters numeric; degree_n generated as byte Use decode and encode to convert to/from variable labels: // convert degree to a descriptive string decode degree, gen(degree_s2) // and back to a number with labels encode degree_s2, gen(degree_n2) decode degree, gen(degree_s2) encode degree_s2, gen(degree_n2) Converting Strings to Date/Time Often date/time variables start out as strings – You’ll need to convert them to numbers using one of the conversion functions listed below. Format Meaning String-to-numeric conversion function %tc milliseconds clock(string, mask) %td days date(string, mask) %tw weeks weekly(string, mask) %tm months monthly(string, mask) %tq quarters quarterly(string, mask) %ty years yearly(string, mask) Date/time variables are stored as the number of units elapsed since 01jan1960 00:00:00.000. For example, the date function returns the number of days since that time, and the clock function returns the number of milliseconds since that time. // create string variable and convert to date gen date = &quot;November 9 2020&quot; gen date1 = date(date, &quot;MDY&quot;) list date1 in 1/5 gen date = &quot;November 9 2020&quot; gen date1 = date(date, &quot;MDY&quot;) list date1 in 1/5 +-------+ | date1 | |-------| 1. | 22228 | 2. | 22228 | 3. | 22228 | 4. | 22228 | 5. | 22228 | +-------+ Formatting Numbers as Dates Once you have converted the string to a number you can format it for display. You can simply accept the defaults used by your formatting string or provide details to customize it. // format so humans can read the date format date1 %d list date1 in 1/5 // format with detail format date1 %tdMonth_dd,_CCYY list date1 in 1/5 format date1 %d list date1 in 1/5 +-----------+ | date1 | |-----------| 1. | 09nov2020 | 2. | 09nov2020 | 3. | 09nov2020 | 4. | 09nov2020 | 5. | 09nov2020 | +-----------+ format date1 %tdMonth_dd,_CCYY list date1 in 1/5 +------------------+ | date1 | |------------------| 1. | November 9, 2020 | 2. | November 9, 2020 | 3. | November 9, 2020 | 4. | November 9, 2020 | 5. | November 9, 2020 | +------------------+ Exercise 1 Missing Values, String Conversion, &amp; by Processing Recode values “99” and “98” on the variable, “hrs1” as “missing.” Recode the marital variable into a “string” variable and then back into a numeric variable. Create a new variable that associates each individual with the average number of hours worked among individuals with matching educational degrees (see the last “by” example for inspiration). Merging, appending, &amp; joining Appending Datasets Sometimes you have observations in two different datasets, or you’d like to add observations to an existing dataset. In this case you can use the append command to add observations to the end of the observations in the master dataset. For example: clear // from the append help file webuse even list webuse odd list // Append even data to the end of the odd data append using &quot;http://www.stata-press.com/data/r14/even&quot; list clear clear webuse even (6th through 8th even numbers) list +---------------+ | number even | |---------------| 1. | 6 12 | 2. | 7 14 | 3. | 8 16 | +---------------+ webuse odd (First five odd numbers) list +--------------+ | number odd | |--------------| 1. | 1 1 | 2. | 2 3 | 3. | 3 5 | 4. | 4 7 | 5. | 5 9 | +--------------+ append using &quot;http://www.stata-press.com/data/r14/even&quot; list +---------------------+ | number odd even | |---------------------| 1. | 1 1 . | 2. | 2 3 . | 3. | 3 5 . | 4. | 4 7 . | 5. | 5 9 . | |---------------------| 6. | 6 . 12 | 7. | 7 . 14 | 8. | 8 . 16 | +---------------------+ clear To keep track of where observations came from, use the generate option as shown below: webuse odd append using &quot;http://www.stata-press.com/data/r14/even&quot;, generate(observesource) list clear webuse odd (First five odd numbers) ce) list +--------------------------------+ | number odd observ~e even | |--------------------------------| 1. | 1 1 0 . | 2. | 2 3 0 . | 3. | 3 5 0 . | 4. | 4 7 0 . | 5. | 5 9 0 . | |--------------------------------| 6. | 6 . 1 12 | 7. | 7 . 1 14 | 8. | 8 . 1 16 | +--------------------------------+ clear There is a “force” option will allow for data type mismatches, but again this is not recommended. Remember, append is for adding observations (i.e., rows) from a second data set. Merging Datasets You can merge variables from a second dataset to the dataset you’re currently working with. Current active dataset = master dataset Dataset you’d like to merge with master = using dataset There are different ways that you might be interested in merging data: Two datasets with same participant pool, one row per participant (1:1) A dataset with one participant per row with a dataset with multiple rows per participant (1:many or many:1) Before you begin: Identify the “ID” that you will use to merge your two datasets Determine which variables you’d like to merge In Stata &gt;= 11, data does NOT have to be sorted Variable types must match across datasets (there is a “force” option to get around this, but not recommended) // Adapted from the merge help page webuse autosize list webuse autoexpense list webuse autosize merge 1:1 make using &quot;http://www.stata-press.com/data/r14/autoexpense&quot; list clear // keep only the matches (AKA &quot;inner join&quot;) webuse autosize, clear merge 1:1 make using &quot;http://www.stata-press.com/data/r14/autoexpense&quot;, keep(match) nogen list clear webuse autosize (1978 Automobile Data) list +------------------------------------+ | make weight length | |------------------------------------| 1. | Toyota Celica 2,410 174 | 2. | BMW 320i 2,650 177 | 3. | Cad. Seville 4,290 204 | 4. | Pont. Grand Prix 3,210 201 | 5. | Datsun 210 2,020 165 | |------------------------------------| 6. | Plym. Arrow 3,260 170 | +------------------------------------+ webuse autoexpense (1978 Automobile Data) list +---------------------------------+ | make price mpg | |---------------------------------| 1. | Toyota Celica 5,899 18 | 2. | BMW 320i 9,735 25 | 3. | Cad. Seville 15,906 21 | 4. | Pont. Grand Prix 5,222 19 | 5. | Datsun 210 4,589 35 | +---------------------------------+ webuse autosize (1978 Automobile Data) merge 1:1 make using &quot;http://www.stata-press.com/data/r14/autoexpense&quot; Result # of obs. ----------------------------------------- not matched 1 from master 1 (_merge==1) from using 0 (_merge==2) matched 5 (_merge==3) ----------------------------------------- list +---------------------------------------------------------------------+ | make weight length price mpg _merge | |---------------------------------------------------------------------| 1. | BMW 320i 2,650 177 9,735 25 matched (3) | 2. | Cad. Seville 4,290 204 15,906 21 matched (3) | 3. | Datsun 210 2,020 165 4,589 35 matched (3) | 4. | Plym. Arrow 3,260 170 . . master only (1) | 5. | Pont. Grand Prix 3,210 201 5,222 19 matched (3) | |---------------------------------------------------------------------| 6. | Toyota Celica 2,410 174 5,899 18 matched (3) | +---------------------------------------------------------------------+ clear webuse autosize, clear (1978 Automobile Data) match) nogen Result # of obs. ----------------------------------------- not matched 0 matched 5 ----------------------------------------- list +---------------------------------------------------+ | make weight length price mpg | |---------------------------------------------------| 1. | BMW 320i 2,650 177 9,735 25 | 2. | Cad. Seville 4,290 204 15,906 21 | 3. | Datsun 210 2,020 165 4,589 35 | 4. | Pont. Grand Prix 3,210 201 5,222 19 | 5. | Toyota Celica 2,410 174 5,899 18 | +---------------------------------------------------+ clear Remember, merge is for adding variables (i.e., columns) from a second data set. Merge Options There are several options that provide more fine-grain control over what happens to non-id columns contained in both data sets. If you’ve carefully cleaned and prepared the data prior to merging this shouldn’t be an issue, but here are some details about how stata handles this situation. In standard merge, the master dataset is the authority and WON’T CHANGE If your master dataset has missing data and some of those values are not missing in your using dataset, specify “update” – this will fill in missing data in master If you want data from your using dataset to overwrite that in your master, specify “replace update” – this will replace master data with using data UNLESS the value is missing in the using dataset Many-to-many merges Stata allows you to specify merges like merge m:m id using newdata.dta, but I have never seen this do anything useful. To quote the official Stata manual: m:m specifies a many-to-many merge and is a bad idea. In an m:m merge, observations are matched within equal values of the key variable(s), with the first observation being matched to the first; the second, to the second; and so on. If the master and using have an unequal number of observations within the group, then the last observation of the shorter group is used repeatedly to match with subsequent observations of the longer group. Thus m:m merges are dependent on the current sort order—something which should never happen. Because m:m merges are such a bad idea, we are not going to show you an example. If you think that you need an m:m merge, then you probably need to work with your data so that you can use a 1:m or m:1 merge. Tips for this are given in Troubleshooting m:m merges below (emphasis added). If you are thinking about using merge m:m chances are good that you actually need joinby. Here is a quick example, modified from the joinby help page. clear webuse parent list webuse children list // Complete and utter nonsense! merge m:m family_id using http://www.stata-press.com/data/r14/parent // You want joinby instead clear webuse children joinby family_id using http://www.stata-press.com/data/r14/parent Remember, merge m:m is old and broken; do not use. Anytime you think you might want m:m you should use joinby instead. Creating summarized data sets Collapse Collapse will take master data and create a new dataset of summary statistics Useful in hierarchical linear modeling if you’d like to create aggregate, summary statistics Can generate group summary data for many descriptive stats Can also attach weights Before you collapse: Save your master dataset and then save it again under a new name (this will prevent collapse from writing over your original data_ Consider issues of missing data. Do you want Stata to use all possible observations? If not, the cw (casewise) option will make casewise deletions // Adapted from the collapse help page clear webuse college list // mean and sd by hospital collapse (mean) mean_gpa = gpa mean_hour = hour (sd) sd_gpa = gpa sd_hour = hour, by(year) list clear clear webuse college list +----------------------------+ | gpa hour year number | |----------------------------| 1. | 3.2 30 1 3 | 2. | 3.5 34 1 2 | 3. | 2.8 28 1 9 | 4. | 2.1 30 1 4 | 5. | 3.8 29 2 3 | |----------------------------| 6. | 2.5 30 2 4 | 7. | 2.9 35 2 5 | 8. | 3.7 30 3 4 | 9. | 2.2 35 3 2 | 10. | 3.3 33 3 3 | |----------------------------| 11. | 3.4 32 4 5 | 12. | 2.9 31 4 2 | +----------------------------+ our, by(year) list +--------------------------------------------------+ | year mean_gpa mean_h~r sd_gpa sd_hour | |--------------------------------------------------| 1. | 1 2.9 30.5 .6055301 2.516612 | 2. | 2 3.066667 31.33333 .6658328 3.21455 | 3. | 3 3.066667 32.66667 .7767453 2.516612 | 4. | 4 3.15 31.5 .3535534 .7071068 | +--------------------------------------------------+ clear You could also generate different statistics for multiple variables Exercise 2 Merge, Append, &amp; Collapse Open the gss2.dta dataset. This dataset contains only half of the variables that are in the complete gss dataset. Merge dataset gss1.dta with dataset gss2.dta. The identification variable is “id.” Open the gss.dta dataset and merge in data from the “marital.dta” dataset, which includes income information grouped by individuals’ marital status. The marital dataset contains collapsed data regarding average statistics of individuals based on their marital status. Open the gssAppend.dta dataset and Create a new dataset that combines the observations in gssAppend.dta with those in gssAddObserve.dta. Open the gss.dta dataset. Create a new dataset that summarizes mean and standard deviation of income based on individuals’ degree status (“degree”). In the process of creating this new dataset, rename your three new variables. Exercise Solutions Ex 0: prototype Open the gss.dta data. Generate a new variable that represents the squared value of age. use dataSets/gss.dta, clear gen age2 = age^2 Generate a new variable equal to “1” if income is greater than “19”. describe income label list income recode income (99=.) (98=.) gen highincome =0 if income != . replace highincome=1 if income&gt;19 sum highincome Create a new variable that counts the number of missing responses for each respondent. What is the maximum number of missing variables? egen nmissing = rowmiss(_all) sum nmissing Ex 1: prototype Recode values “99” and “98” on the variable, “hrs1” as “missing.” use dataSets/gss.dta, clear sum hrs1 recode hrs1 (99=.) (98=.) sum hrs1 Recode the marital variable into a “string” variable and then back into a numeric variable. tostring marital, gen(marstring) destring marstring, gen(mardstring) //compare with decode marital, gen(marital_s) encode marital_s, gen(marital_n) describe marital marstring mardstring marital_s marital_n sum marital marstring mardstring marital_s marital_n Create a new variable that associates each individual with the average number of hours worked among individuals with matching educational degrees (see the last “by” example for inspiration). bysort degree: egen hrsdegree = mean(hrs1) tab hrsdegree tab hrsdegree degree Ex 2: prototype Open the gss2.dta dataset. This dataset contains only half of the variables that are in the complete gss dataset. Merge dataset gss1.dta with dataset gss2.dta. The identification variable is “id.” use dataSets/gss2.dta, clear merge 1:1 id using dataSets/gss1.dta save gss3.dta, replace Open the gss.dta dataset and merge in data from the “marital.dta” dataset, which includes income information grouped by individuals’ marital status. The marital dataset contains collapsed data regarding average statistics of individuals based on their marital status. use dataSets/gss.dta, clear merge m:1 marital using dataSets/marital.dta, nogenerate replace update save gss4.dta, replace Open the gssAppend.dta dataset and Create a new dataset that combines the observations in gssAppend.dta with those in gssAddObserve.dta. use dataSets/gssAppend.dta, clear append using dataSets/gssAddObserve, generate(observe) Open the gss.dta dataset. Create a new dataset that summarizes mean and standard deviation of income based on individuals’ degree status (“degree”). In the process of creating this new dataset, rename your three new variables. use dataSets/gss.dta, clear save collapse2.dta, replace use collapse2.dta, clear collapse (mean) meaninc=income (sd) sdinc=income, by(marital) Wrap-up Feedback These workshops are a work-in-progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu Stata UCLA website: http://www.ats.ucla.edu/stat/Stata/ Stata website: http://www.stata.com/help.cgi?contents Email list: http://www.stata.com/statalist/ "],
["StataModGraph.html", "Stata Modeling &amp; Graphing Setup Fitting models Simple regression Multiple Regression Interactions Exporting &amp; saving results Graphing in Stata Univariate Graphics Bivariate Graphics Twoway Line Graphs Exporting Graphs Exercise Solutions Wrap-up", " Stata Modeling &amp; Graphing Topics Stata modeling Simple regression Multiple regression Interactions Exporting regression tables Testing model assumptions Stata graphing Univariate graphs Bivariate graphs Setup Software &amp; Materials Laptop users: you will need a copy of Stata installed on your machine. Harvard FAS affiliates can install a licensed version from http://downloads.fas.harvard.edu/download Download class materials at https://github.com/IQSS/dss-workshops/raw/master/Stata/StataModGraph.zip Extract materials from the zipped directory StataModGraph.zip (Right-click =&gt; Extract All on Windows, double-click on Mac) and move them to your desktop! Organization Please feel free to ask questions at any point if they are relevant to the current topic (or if you are lost!) Collaboration is encouraged - please introduce yourself to your neighbors! If you are using a laptop, you will need to adjust file paths accordingly Make comments in your Do-file - save on flash drive or email to yourself Goals This is an introduction to modeling and visualization in Stata Assumes basic knowledge of Stata Not appropriate for people already familiar with Stata If you are catching on before the rest of the class, experiment with command features described in help files Learning Objectives: Fit models in Stata Test modeling assumptions Plot basic graphs in Stata Plot two-way graphs Fitting models Today’s Dataset We have data on a variety of variables for all 50 states Population, density, energy use, voting tendencies, graduation rates, income, etc. We’re going to be predicting SAT scores Univariate Regression: SAT scores and Education Expenditures Does the amount of money spent on education affect the mean SAT score in a state? Dependent variable: csat Independent variable: expense Opening Files Look at bottom left hand corner of Stata screen This is the directory Stata is currently reading from Files are located in the StataStatistics folder on the Desktop Start by telling Stata where to look for these // change directory cd &quot;~/Desktop/Stata/StataStatGraph&quot; set more off cd &quot;~/Desktop/Stata/StataStatGraph&quot; /nfs/www/edu-harvard-iq-tutorials/Stata/StataStatGraph Use dir to see what is in the directory: dir cd dataSets dir cd .. dir total 8 drwxr-sr-x. 2 izahn tutorwww 4096 Oct 22 21:59 dataSets/ drwxr-sr-x. 3 izahn tutorwww 4096 Oct 22 21:59 images/ cd dataSets /nfs/www/edu-harvard-iq-tutorials/Stata/StataStatGraph/dataSets dir total 21008 -rwxr-xr-x. 1 izahn tutorwww 21103444 Oct 22 21:59 NatNeighCrimeStudy.dta* -rwxr-xr-x. 1 izahn tutorwww 8977 Oct 22 21:59 states.dta* -rwxr-xr-x. 1 izahn tutorwww 298191 Oct 22 21:59 TimePollPubSchools.dta* cd .. /nfs/www/edu-harvard-iq-tutorials/Stata/StataStatGraph Load the data // use the states data set use dataSets/states.dta use dataSets/states.dta (U.S. states data 1990-91) Simple regression Steps for running regression Examine descriptive statistics Look at relationship graphically and test correlation(s) Run and interpret regression Test regression assumptions Preliminaries We want to predict csat scores from expense First, let’s look at some descriptives // generate summary statistics for csat and expense sum csat expense sum csat expense Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- csat | 51 944.098 66.93497 832 1093 expense | 51 5235.961 1401.155 2960 9259 We want to predict csat scores from expense First, let’s look at some descriptives // look at codebok codebook csat expense codebook csat expense ------------------------------------------------------------------------------- csat Mean composite SAT score ------------------------------------------------------------------------------- type: numeric (int) range: [832,1093] units: 1 unique values: 45 missing .: 0/51 mean: 944.098 std. dev: 66.935 percentiles: 10% 25% 50% 75% 90% 874 886 926 997 1024 ------------------------------------------------------------------------------- expense Per pupil expenditures prim&amp;sec ------------------------------------------------------------------------------- type: numeric (int) range: [2960,9259] units: 1 unique values: 51 missing .: 0/51 mean: 5235.96 std. dev: 1401.16 percentiles: 10% 25% 50% 75% 90% 3782 4351 5000 5865 6738 Next, view relationship graphically Scatterplots work well for univariate relationships // graph expense by csat twoway scatter expense csat Next look at the correlation matrix // correlate csat and expense pwcorr csat expense, star(.05) pwcorr csat expense, star(.05) | csat expense -------------+------------------ csat | 1.0000 expense | -0.4663* 1.0000 Not very interesting with only one predictor SAT scores &amp; Education Expenditures regress csat expense regress csat expense Source | SS df MS Number of obs = 51 -------------+---------------------------------- F(1, 49) = 13.61 Model | 48708.3001 1 48708.3001 Prob &gt; F = 0.0006 Residual | 175306.21 49 3577.67775 R-squared = 0.2174 -------------+---------------------------------- Adj R-squared = 0.2015 Total | 224014.51 50 4480.2902 Root MSE = 59.814 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- expense | -.0222756 .0060371 -3.69 0.001 -.0344077 -.0101436 _cons | 1060.732 32.7009 32.44 0.000 995.0175 1126.447 ------------------------------------------------------------------------------ OLS assumptions Assumption 1: Specification is appropriate (i.e., no relevant omitted variables) Assumption 2: Homoscedasticity (The variance around the regression model is the same for all values of the predictor variable) Assumption 3: Errors are independent Assumption 4: Relationships are linear Assumption 5: Normal Distribution of errors (only needed for inference) Specification The model specification should be informed by theory - i.e., our substantive knowledge of the subject matter. It’s important to include all relevant predictors in the model, otherwise our estimates will be biased. Goodness of fit Homoscedasticity rvfplot rvfplot Normality A simple histogram of the residuals can be informative // graph the residual values of csat predict resid, residual histogram resid, normal predict resid, residual histogram resid, normal (bin=7, start=-131.81111, width=38.329487) Multiple Regression Just keep adding predictors Let’s try adding some predictors to the model of SAT scores income :: % students taking SATs percent :: % adults with HS diploma (high) Preliminaries As before, start with descriptive statistics and correlations // descriptive statistics and correlations sum income percent high pwcorr csat expense income percent high sum income percent high Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- income | 51 33.95657 6.423134 23.465 48.618 percent | 51 35.76471 26.19281 4 81 high | 51 76.26078 5.588741 64.3 86.6 pwcorr csat expense income percent high | csat expense income percent high -------------+--------------------------------------------- csat | 1.0000 expense | -0.4663 1.0000 income | -0.4713 0.6784 1.0000 percent | -0.8758 0.6509 0.6733 1.0000 high | 0.0858 0.3133 0.5099 0.1413 1.0000 regress csat on exense, income, percent, and high regress csat expense income percent high regress csat expense income percent high Source | SS df MS Number of obs = 51 -------------+---------------------------------- F(4, 46) = 51.86 Model | 183354.603 4 45838.6508 Prob &gt; F = 0.0000 Residual | 40659.9067 46 883.911016 R-squared = 0.8185 -------------+---------------------------------- Adj R-squared = 0.8027 Total | 224014.51 50 4480.2902 Root MSE = 29.731 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- expense | .0045604 .004384 1.04 0.304 -.0042641 .013385 income | .4437858 1.138947 0.39 0.699 -1.848795 2.736367 percent | -2.533084 .2454477 -10.32 0.000 -3.027145 -2.039024 high | 2.086599 .9246023 2.26 0.029 .2254712 3.947727 _cons | 836.6197 58.33238 14.34 0.000 719.2027 954.0366 ------------------------------------------------------------------------------ Exercise 0 Multiple Regression Open the datafile, states.dta. Select a few variables to use in a multiple regression of your own. Before running the regression, examine descriptive of the variables and generate a few scatterplots. Run your regression Examine the plausibility of the assumptions of normality and homogeneity Interactions What if we wanted to test an interaction between percent &amp; high? Option 1: generate product terms by hand // generate product of percent and high gen percenthigh = percent*high regress csat expense income percent high percenthigh gen percenthigh = percent*high regress csat expense income percent high percenthigh Source | SS df MS Number of obs = 51 -------------+---------------------------------- F(5, 45) = 46.11 Model | 187430.401 5 37486.0801 Prob &gt; F = 0.0000 Residual | 36584.1091 45 812.980201 R-squared = 0.8367 -------------+---------------------------------- Adj R-squared = 0.8185 Total | 224014.51 50 4480.2902 Root MSE = 28.513 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- expense | .0045575 .0042044 1.08 0.284 -.0039107 .0130256 income | .0887856 1.10374 0.08 0.936 -2.134261 2.311832 percent | -8.143002 2.516509 -3.24 0.002 -13.21151 -3.074493 high | .4240906 1.156545 0.37 0.716 -1.905311 2.753492 percenthigh | .0740926 .0330909 2.24 0.030 .0074441 .1407411 _cons | 972.525 82.5457 11.78 0.000 806.2695 1138.781 ------------------------------------------------------------------------------ What if we wanted to test an interaction between percent &amp; high? Option 2: Let Stata do your dirty work // use the # sign to represent interactions regress csat percent high c.percent#c.high // same as . regress csat c.percent##high regress csat percent high c.percent#c.high Source | SS df MS Number of obs = 51 -------------+---------------------------------- F(3, 47) = 77.39 Model | 186302.091 3 62100.6971 Prob &gt; F = 0.0000 Residual | 37712.4186 47 802.391885 R-squared = 0.8317 -------------+---------------------------------- Adj R-squared = 0.8209 Total | 224014.51 50 4480.2902 Root MSE = 28.327 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- percent | -8.15717 2.488388 -3.28 0.002 -13.16316 -3.151179 high | .6674578 1.082615 0.62 0.541 -1.510482 2.845398 | c.percent#| c.high | .0764271 .0324919 2.35 0.023 .0110619 .1417924 | _cons | 974.9354 81.98078 11.89 0.000 810.0113 1139.859 ------------------------------------------------------------------------------ Categorical Predictors For categorical variables, we first need to dummy code Use region as example Option 1: create dummy codes before fitting regression model // create region dummy codes using tab tab region, gen(region) //regress csat on region regress csat region1 region2 region3 tab region, gen(region) Geographica | l region | Freq. Percent Cum. ------------+----------------------------------- West | 13 26.00 26.00 N. East | 9 18.00 44.00 South | 16 32.00 76.00 Midwest | 12 24.00 100.00 ------------+----------------------------------- Total | 50 100.00 regress csat region1 region2 region3 Source | SS df MS Number of obs = 50 -------------+---------------------------------- F(3, 46) = 9.61 Model | 82049.4719 3 27349.824 Prob &gt; F = 0.0000 Residual | 130911.908 46 2845.91105 R-squared = 0.3853 -------------+---------------------------------- Adj R-squared = 0.3452 Total | 212961.38 49 4346.15061 Root MSE = 53.347 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- region1 | -63.77564 21.35592 -2.99 0.005 -106.7629 -20.7884 region2 | -120.5278 23.52385 -5.12 0.000 -167.8788 -73.17672 region3 | -80.08333 20.37225 -3.93 0.000 -121.0906 -39.07611 _cons | 1010.083 15.39998 65.59 0.000 979.0848 1041.082 ------------------------------------------------------------------------------ For categorical variables, we first need to dummy code Use region as example Option 2: Let Stata do it for you // regress csat on region using fvvarlist syntax // see help fvvarlist for details regress csat i.region regress csat i.region Source | SS df MS Number of obs = 50 -------------+---------------------------------- F(3, 46) = 9.61 Model | 82049.4719 3 27349.824 Prob &gt; F = 0.0000 Residual | 130911.908 46 2845.91105 R-squared = 0.3853 -------------+---------------------------------- Adj R-squared = 0.3452 Total | 212961.38 49 4346.15061 Root MSE = 53.347 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- region | N. East | -56.75214 23.13285 -2.45 0.018 -103.3161 -10.18813 South | -16.30769 19.91948 -0.82 0.417 -56.40353 23.78814 Midwest | 63.77564 21.35592 2.99 0.005 20.7884 106.7629 | _cons | 946.3077 14.79582 63.96 0.000 916.5253 976.0901 ------------------------------------------------------------------------------ Exercise 1 Regression, Categorical Predictors, &amp; Interactions Open the datafile, states.dta. Add on to the regression equation that you created in exercise 1 by generating an interaction term and testing the interaction. Try adding a categorical variable to your regression (remember, it will need to be dummy coded). You could use region or generate a new categorical variable from one of the continuous variables in the dataset. Exporting &amp; saving results Regression tables Usually when we’re running regression, we’ll be testing multiple models at a time Can be difficult to compare results Stata offers several user-friendly options for storing and viewing regression output from multiple models First, download the necessary packages: // install outreg2 package findit outreg2 Saving &amp; replaying You can store regression model results in Stata // fit two regression models and store the results regress csat expense income percent high estimates store Model1 regress csat expense income percent high i.region estimates store Model2 regress csat expense income percent high Source | SS df MS Number of obs = 51 -------------+---------------------------------- F(4, 46) = 51.86 Model | 183354.603 4 45838.6508 Prob &gt; F = 0.0000 Residual | 40659.9067 46 883.911016 R-squared = 0.8185 -------------+---------------------------------- Adj R-squared = 0.8027 Total | 224014.51 50 4480.2902 Root MSE = 29.731 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- expense | .0045604 .004384 1.04 0.304 -.0042641 .013385 income | .4437858 1.138947 0.39 0.699 -1.848795 2.736367 percent | -2.533084 .2454477 -10.32 0.000 -3.027145 -2.039024 high | 2.086599 .9246023 2.26 0.029 .2254712 3.947727 _cons | 836.6197 58.33238 14.34 0.000 719.2027 954.0366 ------------------------------------------------------------------------------ estimates store Model1 regress csat expense income percent high i.region Source | SS df MS Number of obs = 50 -------------+---------------------------------- F(7, 42) = 51.07 Model | 190570.293 7 27224.3275 Prob &gt; F = 0.0000 Residual | 22391.0874 42 533.121128 R-squared = 0.8949 -------------+---------------------------------- Adj R-squared = 0.8773 Total | 212961.38 49 4346.15061 Root MSE = 23.089 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- expense | -.004375 .0044603 -0.98 0.332 -.0133763 .0046263 income | 1.306164 .950279 1.37 0.177 -.6115765 3.223905 percent | -2.965514 .2496481 -11.88 0.000 -3.469325 -2.461704 high | 3.544804 1.075863 3.29 0.002 1.373625 5.715983 | region | N. East | 80.81334 15.4341 5.24 0.000 49.66607 111.9606 South | 33.61225 13.94521 2.41 0.020 5.469676 61.75483 Midwest | 32.15421 10.20145 3.15 0.003 11.56686 52.74157 | _cons | 724.8289 79.25065 9.15 0.000 564.8946 884.7631 ------------------------------------------------------------------------------ estimates store Model2 Stored models can be recalled // Display Model1 estimates replay Model1 estimates replay Model1 ------------------------------------------------------------------------------- Model Model1 ------------------------------------------------------------------------------- Source | SS df MS Number of obs = 51 -------------+---------------------------------- F(4, 46) = 51.86 Model | 183354.603 4 45838.6508 Prob &gt; F = 0.0000 Residual | 40659.9067 46 883.911016 R-squared = 0.8185 -------------+---------------------------------- Adj R-squared = 0.8027 Total | 224014.51 50 4480.2902 Root MSE = 29.731 ------------------------------------------------------------------------------ csat | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- expense | .0045604 .004384 1.04 0.304 -.0042641 .013385 income | .4437858 1.138947 0.39 0.699 -1.848795 2.736367 percent | -2.533084 .2454477 -10.32 0.000 -3.027145 -2.039024 high | 2.086599 .9246023 2.26 0.029 .2254712 3.947727 _cons | 836.6197 58.33238 14.34 0.000 719.2027 954.0366 ------------------------------------------------------------------------------ Stored models can be compared // Compare Model1 and Model2 coefficients estimates table Model1 Model2 estimates table Model1 Model2 ---------------------------------------- Variable | Model1 Model2 -------------+-------------------------- expense | .00456044 -.00437502 income | .44378583 1.3061642 percent | -2.5330843 -2.9655142 high | 2.0865991 3.5448038 | region | N. East | 80.813342 South | 33.612251 Midwest | 32.154215 | _cons | 836.61966 724.82886 ---------------------------------------- Exporting to Excel Avoid human error when transferring coefficients into tables Excel can be used to format publication-ready tables outreg2 [Model1 Model2] using csatprediction.xls, replace outreg2 [Model1 Model2] using csatprediction.xls, replace ~/ado/plus/o/outreg2.ado csatprediction.xls dir : seeout Graphing in Stata Graphing Strategies Keep it simple Labels, labels, labels!! Avoid cluttered graphs Every part of the graph should be meaningful Avoid: Shading Distracting colors Decoration Always know what you’re working with before you get started Recognize scale of data If you’re using multiple variables – how do their scales align? Before any graphing procedure review variables with codebook, sum, tab, etc. HELPFUL STATA HINT: If you want your command to go on multiple lines use /// at end of each line Terrible Graph Much Better Graph Univariate Graphics Our First Dataset Time Magazine Public School Poll Based on survey of 1,000 adults in U.S. Conducted in August 2010 Questions regarding feelings about parental involvement, teachers union, current potential for reform Open Stata and call up the datafile for today // Step 1: tell Stata where to find data: cd &quot;~/StataGraphics/dataSets&quot; // Step 2: call up our dataset: use TimePollPubSchools.dta Single Continuous Variables Example: Histograms Stata assumes you’re working with continuous data Very simple syntax: hist varname Put a comma after your varname and start adding options bin(#) : change the number of bars that the graph displays normal : overlay normal curve addlabels : add actual values to bars Histogram Options To change the numeric depiction of your data add these options after the comma Choose one: density fraction frequency percent Be sure to properly describe your histogram: title(insert name of graph) subtitle(insert subtitle of graph) note(insert note to appear at bottom of graph) caption(insert caption to appear below notes) Histogram Example hist F1, bin(10) percent title(TITLE) /// subtitle(SUBTITLE) caption(CAPTION) note(NOTES) Axis Titles &amp; Labels Axis title options (default is variable label): xtitle(insert x axis name) ytitle(insert y axis name) Don’t want axis titles? xtitle(\"\") ytitle(\"\") Add labels to X or Y axis: xlabel(insert x axis label) ylabel(insert y axis label) Tell Stata how to scale each axis xlabel(start#(increment)end#) xlabel(0(5)100) This would label x-axis from 0-100 in increments of 5 Axis Labels Example hist F1, bin(10) percent title(TITLE) subtitle(SUBTITLE) /// caption(CAPTION) note(NOTES) /// xtitle(Here&#39;s your x-axis title) /// ytitle(here&#39;s your y-axis title) Single Categorical Variables We can also use the hist command for bar graphs Simply specify “discrete” with options Stata will produce one bar for each level (i.e. category) of variable Use xlabel command to insert names of individual categories hist F4, title(Racial breakdown of Time Poll Sample) xtitle(Race) /// ytitle(Percent) xlabel(1 &quot;White&quot; 2 &quot;Black&quot; 3 &quot;Asian&quot; 4 &quot;Hispanic&quot; /// 5 &quot;Other&quot;) discrete percent addlabels Exercise 2 Histograms Bar Graphs Open the datafile, NatNeighCrimeStudy.dta. Create a histogram of the tract-level poverty rate (variable name: T_POVRTY). Insert the normal curve over the histogram Change the numeric representation on the Y-axis to “percent” Add appropriate titles to the overall graph and the x axis and y axis. Also, add a note that states the source of this data. Open the datafile, TimePollPubSchools.dta Create a histogram of the question, “What grade would you give your child’s school” (variable name: Q11). Be sure to tell Stata that this is a categorical variable. Format this graph so that the axes have proper titles and labels. Also, add an appropriate title to the overall graph that goes onto two lines. Add a note stating the source of the data. Bivariate Graphics Next Dataset: National Neighborhood Crime Study (NNCS) N=9,593 census tracts in 2000 Explore sources of variation in crime for communities in the United States Tract-level data: crime, social disorganization, disadvantage, socioeconomic inequality City-level data: labor market, socioeconomic inequality, population change The Twoway Family twoway is basic Stata command for all twoway graphs Use twoway anytime you want to make comparisons among variables Can be used to combine graphs (i.e., overlay one graph with another e.g., insert line of best fit over a scatter plot Some basic examples: use NatNeighCrimeStudy.dta twoway scatter T_PERCAP T_VIOLNT twoway dropline T_PERCAP T_VIOLNT twoway lfitci T_PERCAP T_VIOLNT Twoway &amp; the by Statement twoway scatter T_PERCAP T_VIOLNT, by(DIVISION) Twoway Title Options Same title options as with histogram title(insert name of graph) subtitle(insert subtitle of graph) note(insert note to appear at bottom of graph) caption(insert caption to appear below notes) Twoway Title Options Example twoway scatter T_PERCAP T_VIOLNT, /// title(Comparison of Per Capita Income /// and Violent Crime Rate at Tract level) /// xtitle(Violent Crime Rate) ytitle(Per Capita Income) /// note(Source: National Neighborhood Crime Study 2000) The title is a bit cramped–let’s fix that: twoway scatter T_PERCAP T_VIOLNT, /// title(&quot;Comparison of Per Capita Income&quot; /// &quot;and Violent Crime Rate at Tract level&quot;) /// xtitle(Violent Crime Rate) ytitle(Per Capita Income) /// note(Source: National Neighborhood Crime Study 2000) Twoway Symbol Options A variety of symbol shapes are available: use palette symbolpalette to seem them and msymbol() to set them Twoway Symbol Options twoway scatter T_PERCAP T_VIOLNT, /// title(&quot;Comparison of Per Capita Income&quot; /// &quot;and Violent Crime Rate at Tract level&quot;) /// xtitle(Violent Crime Rate) ytitle(Per Capita Income) /// note(Source: National Neighborhood Crime Study 2000) /// msymbol(Sh) mcolor(&quot;red&quot;) Overlaying Twoway Graphs Very simple to combine multiple graphs…just put each graph command in parentheses twoway (scatter var1 var2) (lfit var1 var2) Add individual options to each graph within the parentheses Add overall graph options as usual following the comma twoway (scatter var1 var2) (lfit var1 var2), options Overlaying Points &amp; Lines twoway (scatter T_PERCAP T_VIOLNT) /// (lfit T_PERCAP T_VIOLNT), /// title(&quot;Comparison of Per Capita Income&quot; /// &quot;and Violent Crime Rate at Tract level&quot;) /// xtitle(Violent Crime Rate) ytitle(Per Capita Income) /// note(Source: National Neighborhood Crime Study 2000) Overlaying Points &amp; Labels twoway (scatter T_PERCAP T_VIOLNT if T_VIOLNT==1976, /// mlabel(CITY)) (scatter T_PERCAP T_VIOLNT), /// title(&quot;Comparison of Per Capita Income&quot; /// &quot;and Violent Crime Rate at Tract level&quot;) /// xlabel(0(200)2400) note(Source: National Neighborhood /// Crime Study 2000) legend(off) Exercise 3 The TwoWay Family Open the datafile, NatNeighCrimeStudy.dta. Create a basic twoway scatterplot that compares the city unemployment rate (C_UNEMP) to the percent secondary sector low-wage jobs (C_SSLOW) Generate the same scatterplot, but this time, divide the plot by the dummy variable indicating whether the city is located in the south or not (C_SOUTH) Change the color of the symbol that you use in this scatter plot Change the type of symbol you use to a marker of your choice Notice in your scatterplot that is broken down by C_SOUTH that there is an outlier in the upper right hand corner of the “Not South” graph. Add the city name label to this marker. Review the options available under “help twowayoptions” and change one aspect of your graph using an option that we haven’t already reviewed Twoway Line Graphs Line graphs helpful for a variety of data Especially any type of time series data We’ll use data on US life expectancy from 1900-1999 webuse uslifeexp, clear webuse uslifeexp, clear twoway (line le_wm year, mcolor(&quot;red&quot;)) /// (line le_bm year, mcolor(&quot;green&quot;)) twoway (line (le_wfemale le_wmale le_bf le_bm) year, /// lpattern(dot solid dot solid)) Stata Graphing Lines palette linepalette Exporting Graphs From Stata, right click on image and select “save as” or try syntax: graph export myfig.esp, replace In Microsoft Word: insert -&gt; picture -&gt; from file Or, right click on graph in Stata and copy and paste into MS Word Exercise Solutions Ex 0: prototype ** Ex 1: prototype ** Ex 2: prototype ** Ex 3: prototype ** Wrap-up Feedback These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://dss.iq.harvard.edu/workshop-materials Data Science Services: https://dss.iq.harvard.edu/ Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research@hbs.edu Stata UCLA website: http://www.ats.ucla.edu/stat/Stata/ Stata website: http://www.stata.com/help.cgi?contents Email list: http://www.stata.com/statalist/ "]
]
